"""
ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
í¬ë¡¤ë§ â†’ ë°ì´í„° í–¥ìƒ â†’ ìœ„ì¹˜ì •ë³´ ì™„ì„±ë„ í™•ì¸
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.core.crawler import DiningCodeCrawler
from src.core.data_enhancement import DataEnhancer
import logging
import json
import pandas as pd
from datetime import datetime

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('test_full_pipeline.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

def test_full_pipeline():
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
    
    logger.info("=" * 80)
    logger.info("ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    logger.info("=" * 80)
    
    # 1ë‹¨ê³„: í¬ë¡¤ë§
    logger.info("\n[1ë‹¨ê³„] í¬ë¡¤ë§ ì‹œì‘")
    crawler = DiningCodeCrawler()
    
    try:
        # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì ì€ ìˆ˜ì˜ ê°€ê²Œë§Œ ìˆ˜ì§‘
        keyword = "ê°•ë‚¨ ë¬´í•œë¦¬í•„"
        rect = "37.4979,127.0276,37.5279,127.0576"  # ê°•ë‚¨ ì§€ì—­
        
        logger.info(f"ê²€ìƒ‰ í‚¤ì›Œë“œ: {keyword}")
        logger.info(f"ê²€ìƒ‰ ì˜ì—­: {rect}")
        
        # ê°€ê²Œ ëª©ë¡ ìˆ˜ì§‘
        stores = crawler.get_store_list(keyword, rect)
        logger.info(f"\ní¬ë¡¤ë§ ê²°ê³¼: {len(stores)}ê°œ ê°€ê²Œ ë°œê²¬")
        
        if not stores:
            logger.error("ê°€ê²Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            return
        
        # ìœ„ì¹˜ì •ë³´ ìƒíƒœ í™•ì¸ (í¬ë¡¤ë§ ì§í›„)
        stores_with_location_before = [s for s in stores if s.get('position_lat') and s.get('position_lng')]
        logger.info(f"í¬ë¡¤ë§ ì§í›„ ìœ„ì¹˜ì •ë³´ ìˆëŠ” ê°€ê²Œ: {len(stores_with_location_before)}ê°œ ({len(stores_with_location_before)/len(stores)*100:.1f}%)")
        
        # ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ (ì²˜ìŒ 5ê°œë§Œ)
        logger.info("\nìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
        detailed_stores = []
        for i, store in enumerate(stores[:5]):
            logger.info(f"ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘: {i+1}/5 - {store.get('name')}")
            detailed_store = crawler.get_store_detail(store)
            detailed_stores.append(detailed_store)
        
        # ë‚˜ë¨¸ì§€ëŠ” ê¸°ë³¸ ì •ë³´ë§Œ ì‚¬ìš©
        detailed_stores.extend(stores[5:])
        
        # ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ í›„ ìœ„ì¹˜ì •ë³´ ìƒíƒœ
        stores_with_location_detail = [s for s in detailed_stores if s.get('position_lat') and s.get('position_lng')]
        logger.info(f"\nìƒì„¸ ì •ë³´ ìˆ˜ì§‘ í›„ ìœ„ì¹˜ì •ë³´ ìˆëŠ” ê°€ê²Œ: {len(stores_with_location_detail)}ê°œ ({len(stores_with_location_detail)/len(detailed_stores)*100:.1f}%)")
        
        # 2ë‹¨ê³„: ë°ì´í„° í–¥ìƒ
        logger.info("\n[2ë‹¨ê³„] ë°ì´í„° í–¥ìƒ ì‹œì‘")
        data_enhancer = DataEnhancer()
        
        enhanced_stores, enhancement_stats = data_enhancer.enhance_stores_data(detailed_stores)
        
        logger.info(f"\në°ì´í„° í–¥ìƒ ì™„ë£Œ:")
        logger.info(f"- ì§€ì˜¤ì½”ë”© ì„±ê³µ: {enhancement_stats.geocoding_success}ê°œ")
        logger.info(f"- ê°€ê²© ì •ê·œí™”: {enhancement_stats.price_normalized}ê°œ")
        logger.info(f"- ì¹´í…Œê³ ë¦¬ ë§¤í•‘: {enhancement_stats.categories_mapped}ê°œ")
        logger.info(f"- ì¤‘ë³µ ì œê±°: {enhancement_stats.duplicates_removed}ê°œ")
        
        # 3ë‹¨ê³„: ìµœì¢… ê²°ê³¼ ë¶„ì„
        logger.info("\n[3ë‹¨ê³„] ìµœì¢… ê²°ê³¼ ë¶„ì„")
        analyze_results(enhanced_stores)
        
        # ê²°ê³¼ ì €ì¥
        save_results(enhanced_stores)
        
    except Exception as e:
        logger.error(f"í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        logger.error(traceback.format_exc())
        
    finally:
        crawler.close()
        logger.info("\ní…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

def analyze_results(stores):
    """ê²°ê³¼ ë¶„ì„ ë° í†µê³„ ì¶œë ¥"""
    total_count = len(stores)
    
    # ìœ„ì¹˜ì •ë³´ ë¶„ì„
    stores_with_location = [s for s in stores if s.get('position_lat') and s.get('position_lng')]
    location_count = len(stores_with_location)
    location_rate = location_count / total_count * 100 if total_count > 0 else 0
    
    # ì§€ì˜¤ì½”ë”© ì†ŒìŠ¤ ë¶„ì„
    source_stats = {
        'javascript': 0,
        'kakao': 0,
        'estimated': 0,
        'none': 0
    }
    
    for store in stores:
        if store.get('position_lat') and store.get('position_lng'):
            source = store.get('geocoding_source', 'javascript')
            if source in source_stats:
                source_stats[source] += 1
            else:
                source_stats['javascript'] += 1
        else:
            source_stats['none'] += 1
    
    # ë°ì´í„° ì™„ì„±ë„ ë¶„ì„
    completeness_stats = {
        'name': 0,
        'address': 0,
        'phone': 0,
        'coordinates': 0,
        'price': 0,
        'categories': 0,
        'images': 0,
        'menu': 0
    }
    
    for store in stores:
        if store.get('name'):
            completeness_stats['name'] += 1
        if store.get('address') or store.get('basic_address'):
            completeness_stats['address'] += 1
        if store.get('phone_number'):
            completeness_stats['phone'] += 1
        if store.get('position_lat') and store.get('position_lng'):
            completeness_stats['coordinates'] += 1
        if store.get('price') or store.get('normalized_price'):
            completeness_stats['price'] += 1
        if store.get('standard_categories'):
            completeness_stats['categories'] += 1
        if store.get('image_urls'):
            completeness_stats['images'] += 1
        if store.get('menu_items'):
            completeness_stats['menu'] += 1
    
    # ê²°ê³¼ ì¶œë ¥
    logger.info(f"\n{'='*60}")
    logger.info(f"ğŸ“Š ìµœì¢… í†µê³„")
    logger.info(f"{'='*60}")
    logger.info(f"ì´ ê°€ê²Œ ìˆ˜: {total_count}ê°œ")
    logger.info(f"\nğŸ“ ìœ„ì¹˜ì •ë³´ ì™„ì„±ë„: {location_count}/{total_count} ({location_rate:.1f}%)")
    
    logger.info(f"\nğŸ“Œ ìœ„ì¹˜ì •ë³´ ì¶œì²˜:")
    logger.info(f"  - JavaScript (ìƒì„¸í˜ì´ì§€): {source_stats['javascript']}ê°œ")
    logger.info(f"  - ì¹´ì¹´ì˜¤ API ì§€ì˜¤ì½”ë”©: {source_stats['kakao']}ê°œ")
    logger.info(f"  - ê·¼ì²˜ ê°€ê²Œ ê¸°ë°˜ ì¶”ì •: {source_stats['estimated']}ê°œ")
    logger.info(f"  - ìœ„ì¹˜ì •ë³´ ì—†ìŒ: {source_stats['none']}ê°œ")
    
    logger.info(f"\nğŸ“‹ ë°ì´í„° ì™„ì„±ë„:")
    for field, count in completeness_stats.items():
        rate = count / total_count * 100 if total_count > 0 else 0
        logger.info(f"  - {field}: {count}/{total_count} ({rate:.1f}%)")
    
    # ìƒìœ„ 5ê°œ ê°€ê²Œ ìƒ˜í”Œ ì¶œë ¥
    logger.info(f"\nğŸª ê°€ê²Œ ìƒ˜í”Œ (ìƒìœ„ 5ê°œ):")
    for i, store in enumerate(stores[:5]):
        logger.info(f"\n[{i+1}] {store.get('name')} ({store.get('branch', '')})")
        logger.info(f"  - ì£¼ì†Œ: {store.get('address') or store.get('basic_address')}")
        logger.info(f"  - ì¢Œí‘œ: ({store.get('position_lat')}, {store.get('position_lng')})")
        logger.info(f"  - ì¢Œí‘œ ì¶œì²˜: {store.get('geocoding_source', 'javascript')}")
        logger.info(f"  - ì „í™”: {store.get('phone_number')}")
        logger.info(f"  - ì¹´í…Œê³ ë¦¬: {store.get('standard_categories', [])}")

def save_results(stores):
    """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # JSONìœ¼ë¡œ ì €ì¥
    json_filename = f'test_results_{timestamp}.json'
    with open(json_filename, 'w', encoding='utf-8') as f:
        json.dump(stores, f, ensure_ascii=False, indent=2)
    logger.info(f"\nê²°ê³¼ê°€ {json_filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # CSVë¡œë„ ì €ì¥ (ì£¼ìš” í•„ë“œë§Œ)
    csv_data = []
    for store in stores:
        csv_data.append({
            'name': store.get('name'),
            'branch': store.get('branch'),
            'address': store.get('address') or store.get('basic_address'),
            'latitude': store.get('position_lat'),
            'longitude': store.get('position_lng'),
            'geocoding_source': store.get('geocoding_source', 'javascript'),
            'phone': store.get('phone_number'),
            'categories': ', '.join(store.get('standard_categories', []))
        })
    
    csv_filename = f'test_results_{timestamp}.csv'
    df = pd.DataFrame(csv_data)
    df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
    logger.info(f"CSV ê²°ê³¼ê°€ {csv_filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    test_full_pipeline() 