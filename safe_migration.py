#!/usr/bin/env python3
"""
ì•ˆì „í•œ ë§ˆì´ê·¸ë ˆì´ì…˜: í¬ë¡¤ë§ DB â†’ ë©”ì¸ í”„ë¡œì íŠ¸ DB
í¬ë¡¤ë§ DB ìŠ¤í‚¤ë§ˆ ë³€ê²½ ì—†ì´ ë©”ì¸ í”„ë¡œì íŠ¸ ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ë°ì´í„° ë³€í™˜
"""
import psycopg2
import psycopg2.extras
import logging
import json
import re
import os
from datetime import datetime
from typing import List, Dict, Optional, Union
from urllib.parse import urlparse

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('safe_migration.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class SafeMigration:
    """ì•ˆì „í•œ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ í´ë˜ìŠ¤"""
    
    def __init__(self, main_project_db_url: str = None, crawler_db_url: str = None):
        # í¬ë¡¤ë§ DB ì—°ê²° (ê¸°ì¡´ ê·¸ëŒ€ë¡œ)
        self.crawler_db_url = crawler_db_url or 'postgresql://postgres:12345@localhost:5432/refill_spot'
        
        # ë©”ì¸ í”„ë¡œì íŠ¸ DB ì—°ê²° (Supabase ë˜ëŠ” ë³„ë„ DB)
        self.main_project_db_url = os.getenv('PROJECT_DATABASE_URL')
        
        # í¬ë¡¤ë§ DB ì—°ê²°
        self.crawler_conn = self._create_connection(self.crawler_db_url)
        logger.info("í¬ë¡¤ë§ DB ì—°ê²° ì™„ë£Œ")
        
        # ë©”ì¸ í”„ë¡œì íŠ¸ ìŠ¤í‚¤ë§ˆ ì •ì˜ (ë³€ê²½í•˜ì§€ ì•Šì„ ì›ë³¸ ìŠ¤í‚¤ë§ˆ)
        self.main_schema = {
            'required_fields': ['name', 'address', 'position_lat', 'position_lng', 'position_x', 'position_y'],
            'optional_fields': ['description', 'naver_rating', 'kakao_rating', 'open_hours', 'price', 'refill_items', 'image_urls'],
            'data_types': {
                'name': 'text',
                'address': 'text', 
                'description': 'text',
                'position_lat': 'double precision',
                'position_lng': 'double precision',
                'position_x': 'double precision',
                'position_y': 'double precision',
                'naver_rating': 'double precision',
                'kakao_rating': 'double precision',
                'open_hours': 'text',
                'price': 'text',
                'refill_items': 'text[]',
                'image_urls': 'text[]'
            }
        }
    
    def _create_connection(self, db_url: str):
        """DB ì—°ê²° ìƒì„±"""
        try:
            parsed = urlparse(db_url)
            config = {
                'host': parsed.hostname,
                'port': parsed.port or 5432,
                'database': parsed.path[1:] if parsed.path else 'postgres',
                'user': parsed.username,
                'password': parsed.password
            }
            return psycopg2.connect(**config)
        except Exception as e:
            logger.error(f"DB ì—°ê²° ì‹¤íŒ¨ ({db_url}): {e}")
            raise
    
    def get_crawler_stores(self, limit: Optional[int] = None) -> List[Dict]:
        """í¬ë¡¤ë§ DBì—ì„œ ë°ì´í„° ì¡°íšŒ (ê¸°ì¡´ í¬ë¡¤ë§ ìŠ¤í‚¤ë§ˆ ê·¸ëŒ€ë¡œ ì‚¬ìš©)"""
        try:
            cursor = self.crawler_conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
            
            # í¬ë¡¤ë§ DBì˜ ì‹¤ì œ ìŠ¤í‚¤ë§ˆì— ë§ëŠ” ì¿¼ë¦¬
            query = """
                SELECT 
                    s.*
                FROM stores s
                WHERE s.name IS NOT NULL 
                AND s.name != ''
                ORDER BY s.created_at DESC
            """
            
            if limit:
                query += f" LIMIT {limit}"
                
            cursor.execute(query)
            stores = cursor.fetchall()
            
            logger.info(f"í¬ë¡¤ë§ DBì—ì„œ {len(stores)}ê°œ ê°€ê²Œ ì¡°íšŒ ì™„ë£Œ")
            return [dict(store) for store in stores]
            
        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
            raise
        finally:
            cursor.close()
    
    def convert_to_main_schema(self, crawler_store: Dict) -> Dict:
        """í¬ë¡¤ë§ ë°ì´í„°ë¥¼ ë©”ì¸ í”„ë¡œì íŠ¸ ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ì•ˆì „í•˜ê²Œ ë³€í™˜"""
        try:
            converted = {}
            
            # 1. í•„ìˆ˜ í•„ë“œ ì²˜ë¦¬ (ë©”ì¸ í”„ë¡œì íŠ¸ NOT NULL í•„ë“œë“¤)
            converted['name'] = self._safe_text(crawler_store.get('name'), 'ë¬´í•œë¦¬í•„ ê°€ê²Œ')
            converted['address'] = self._safe_text(crawler_store.get('address'), 'ì£¼ì†Œ ì •ë³´ ì—†ìŒ')
            
            # 2. ìœ„ì¹˜ ì •ë³´ ì²˜ë¦¬ (ë©”ì¸ í”„ë¡œì íŠ¸ NOT NULL)
            converted['position_lat'] = self._safe_coordinate(crawler_store.get('position_lat'), 37.5665)  # ì„œìš¸ì‹œì²­
            converted['position_lng'] = self._safe_coordinate(crawler_store.get('position_lng'), 126.9780)
            converted['position_x'] = self._safe_coordinate(crawler_store.get('position_x'), converted['position_lng'])
            converted['position_y'] = self._safe_coordinate(crawler_store.get('position_y'), converted['position_lat'])
            
            # 3. ì„ íƒì  í•„ë“œ ì²˜ë¦¬
            converted['description'] = self._generate_description(crawler_store)
            converted['naver_rating'] = self._safe_float(crawler_store.get('naver_rating'))
            converted['kakao_rating'] = self._safe_float(crawler_store.get('kakao_rating'))
            converted['open_hours'] = self._process_open_hours(crawler_store)
            converted['price'] = self._process_price(crawler_store)
            converted['refill_items'] = self._process_refill_items(crawler_store)
            converted['image_urls'] = self._process_image_urls(crawler_store)
            
            # 4. ë°ì´í„° ê²€ì¦
            self._validate_converted_data(converted)
            
            return converted
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ë³€í™˜ ì‹¤íŒ¨ ({crawler_store.get('name', 'Unknown')}): {e}")
            raise
    
    def _safe_text(self, value, default: str = '') -> str:
        """ì•ˆì „í•œ í…ìŠ¤íŠ¸ ë³€í™˜"""
        if value is None:
            return default
        text = str(value).strip()
        return text if text else default
    
    def _safe_coordinate(self, value, default: float) -> float:
        """ì•ˆì „í•œ ì¢Œí‘œ ë³€í™˜"""
        if value is None:
            return default
        try:
            coord = float(value)
            # ì„œìš¸ì‹œ ë²”ìœ„ ê²€ì¦
            if 37.4 <= coord <= 37.7 or 126.8 <= coord <= 127.2:
                return coord
            return default
        except (ValueError, TypeError):
            return default
    
    def _safe_float(self, value) -> Optional[float]:
        """ì•ˆì „í•œ float ë³€í™˜"""
        if value is None:
            return None
        try:
            return float(value)
        except (ValueError, TypeError):
            return None
    
    def _generate_description(self, store: Dict) -> Optional[str]:
        """ì„¤ëª… ìƒì„± (í¬ë¡¤ë§ ë°ì´í„°ì˜ í’ë¶€í•œ ì •ë³´ë¥¼ ìš”ì•½)"""
        parts = []
        
        # ê¸°ë³¸ ì„¤ëª…
        if store.get('description'):
            parts.append(str(store['description']))
        
        # ë¬´í•œë¦¬í•„ ì •ë³´
        if store.get('refill_type'):
            parts.append(f"{store['refill_type']} ì „ë¬¸ì ")
        
        # ë¶„ìœ„ê¸° ì •ë³´
        if store.get('atmosphere'):
            parts.append(f"ë¶„ìœ„ê¸°: {store['atmosphere']}")
        
        # í‚¤ì›Œë“œ ì •ë³´
        if store.get('keywords') and isinstance(store['keywords'], list):
            keywords = [str(k) for k in store['keywords'][:3]]  # ìƒìœ„ 3ê°œë§Œ
            if keywords:
                parts.append(f"íŠ¹ì§•: {', '.join(keywords)}")
        
        result = ' | '.join(filter(None, parts))
        return result[:500] if result else None
    
    def _process_open_hours(self, store: Dict) -> Optional[str]:
        """ì˜ì—…ì‹œê°„ ì •ë³´ í†µí•©"""
        hours_info = []
        
        if store.get('open_hours'):
            hours_info.append(str(store['open_hours']))
        elif store.get('open_hours_raw'):
            hours_info.append(str(store['open_hours_raw']))
        
        if store.get('break_time'):
            hours_info.append(f"ë¸Œë ˆì´í¬íƒ€ì„: {store['break_time']}")
        
        if store.get('holiday'):
            hours_info.append(f"íœ´ë¬´: {store['holiday']}")
        
        return ' / '.join(hours_info) if hours_info else None
    
    def _process_price(self, store: Dict) -> Optional[str]:
        """ê°€ê²© ì •ë³´ ì²˜ë¦¬ (ë©”ì¸ í”„ë¡œì íŠ¸ëŠ” text íƒ€ì…)"""
        # í¬ë¡¤ë§ DBì˜ ë‹¤ì–‘í•œ ê°€ê²© í•„ë“œì—ì„œ ì •ë³´ ìˆ˜ì§‘
        price_sources = [
            store.get('price'),
            store.get('average_price'),
            store.get('price_range')
        ]
        
        for price in price_sources:
            if price:
                return str(price).strip()
        
        # price_details ë°°ì—´ì—ì„œ ì¶”ì¶œ
        if store.get('price_details') and isinstance(store['price_details'], list):
            details = [str(p) for p in store['price_details'] if p]
            if details:
                return ', '.join(details[:3])  # ìƒìœ„ 3ê°œë§Œ
        
        return None
    
    def _process_refill_items(self, store: Dict) -> List[str]:
        """ë¬´í•œë¦¬í•„ ì•„ì´í…œ ì²˜ë¦¬"""
        items = set()
        
        # ê¸°ì¡´ refill_items í•„ë“œ
        if store.get('refill_items'):
            if isinstance(store['refill_items'], list):
                items.update([str(item) for item in store['refill_items'] if item])
            elif isinstance(store['refill_items'], str):
                items.add(store['refill_items'])
        
        # refill_typeì—ì„œ ì¶”ì¶œ
        if store.get('refill_type'):
            refill_type = str(store['refill_type'])
            # "ì†Œê³ ê¸° ë¬´í•œë¦¬í•„" -> "ì†Œê³ ê¸°" ì¶”ì¶œ
            match = re.search(r'(.+?)\s*ë¬´í•œë¦¬í•„', refill_type)
            if match:
                item = match.group(1).strip()
                if item:
                    items.add(item)
        
        # ì •ë¦¬ ë° í•„í„°ë§
        filtered_items = []
        for item in items:
            if item and len(str(item).strip()) > 0:
                filtered_items.append(str(item).strip())
        
        return filtered_items[:10]  # ìµœëŒ€ 10ê°œë¡œ ì œí•œ
    
    def _process_image_urls(self, store: Dict) -> List[str]:
        """ì´ë¯¸ì§€ URL ì²˜ë¦¬"""
        urls = set()
        
        # ë‹¤ì–‘í•œ ì´ë¯¸ì§€ í•„ë“œì—ì„œ ìˆ˜ì§‘
        image_fields = ['main_image', 'image_urls', 'menu_images', 'interior_images']
        
        for field in image_fields:
            value = store.get(field)
            if value:
                if isinstance(value, list):
                    urls.update([str(url) for url in value if url])
                elif isinstance(value, str):
                    urls.add(value)
        
        # URL ê²€ì¦ ë° í•„í„°ë§
        valid_urls = []
        for url in urls:
            if url and str(url).startswith(('http://', 'https://')):
                valid_urls.append(str(url))
        
        return valid_urls[:5]  # ìµœëŒ€ 5ê°œë¡œ ì œí•œ
    
    def _validate_converted_data(self, data: Dict):
        """ë³€í™˜ëœ ë°ì´í„° ê²€ì¦"""
        # í•„ìˆ˜ í•„ë“œ ê²€ì¦
        for field in self.main_schema['required_fields']:
            if field not in data or data[field] is None:
                raise ValueError(f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}")
        
        # ìœ„ì¹˜ ì¢Œí‘œ ìœ íš¨ì„± ê²€ì¦
        lat, lng = data['position_lat'], data['position_lng']
        if not (37.0 <= lat <= 38.0 and 126.0 <= lng <= 128.0):
            logger.warning(f"ì¢Œí‘œ ë²”ìœ„ ì´ìƒ: lat={lat}, lng={lng}")
    
    def generate_migration_sql(self, limit: Optional[int] = None) -> List[str]:
        """ë©”ì¸ í”„ë¡œì íŠ¸ìš© ë§ˆì´ê·¸ë ˆì´ì…˜ SQL ìƒì„±"""
        try:
            stores = self.get_crawler_stores(limit)
            sql_statements = []
            
            logger.info(f"ì´ {len(stores)}ê°œ ê°€ê²Œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘")
            
            for i, store in enumerate(stores, 1):
                try:
                    # ì•ˆì „í•œ ë°ì´í„° ë³€í™˜
                    converted = self.convert_to_main_schema(store)
                    
                    # SQL ìƒì„± (ë©”ì¸ í”„ë¡œì íŠ¸ ìŠ¤í‚¤ë§ˆì— ë§ê²Œ)
                    refill_items_sql = self._array_to_sql(converted['refill_items'])
                    image_urls_sql = self._array_to_sql(converted['image_urls'])
                    
                    sql = f"""
INSERT INTO stores (
    name, address, description,
    position_lat, position_lng, position_x, position_y,
    naver_rating, kakao_rating, open_hours, price,
    refill_items, image_urls
) VALUES (
    {self._escape_sql_string(converted['name'])},
    {self._escape_sql_string(converted['address'])},
    {self._escape_sql_string(converted['description'])},
    {converted['position_lat']},
    {converted['position_lng']},
    {converted['position_x']},
    {converted['position_y']},
    {converted['naver_rating'] if converted['naver_rating'] else 'NULL'},
    {converted['kakao_rating'] if converted['kakao_rating'] else 'NULL'},
    {self._escape_sql_string(converted['open_hours'])},
    {self._escape_sql_string(converted['price'])},
    {refill_items_sql},
    {image_urls_sql}
);"""
                    
                    sql_statements.append(sql.strip())
                    
                    if i % 10 == 0:
                        logger.info(f"ì§„í–‰ë¥ : {i}/{len(stores)} ({i/len(stores)*100:.1f}%)")
                    
                except Exception as e:
                    logger.error(f"ê°€ê²Œ '{store.get('name', 'Unknown')}' ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {e}")
                    continue
            
            logger.info(f"ë§ˆì´ê·¸ë ˆì´ì…˜ SQL ìƒì„± ì™„ë£Œ: {len(sql_statements)}ê°œ")
            return sql_statements
            
        except Exception as e:
            logger.error(f"ë§ˆì´ê·¸ë ˆì´ì…˜ SQL ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    def _escape_sql_string(self, value) -> str:
        """SQL ë¬¸ìì—´ ì´ìŠ¤ì¼€ì´í”„"""
        if value is None:
            return 'NULL'
        return f"'{str(value).replace(chr(39), chr(39)*2)}'"  # ì‘ì€ë”°ì˜´í‘œ ì´ìŠ¤ì¼€ì´í”„
    
    def _array_to_sql(self, arr: List[str]) -> str:
        """ë°°ì—´ì„ SQL í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        if not arr:
            return 'ARRAY[]::text[]'
        escaped_items = [f"'{item.replace(chr(39), chr(39)*2)}'" for item in arr]
        return f"ARRAY[{','.join(escaped_items)}]"
    
    def close(self):
        """ì—°ê²° ì¢…ë£Œ"""
        if self.crawler_conn:
            self.crawler_conn.close()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”„ ì•ˆì „í•œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘")
    print("=" * 50)
    
    migration = SafeMigration()
    
    try:
        # 1. ë¯¸ë¦¬ë³´ê¸°
        print("ğŸ‘€ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°...")
        stores = migration.get_crawler_stores(limit=3)
        
        if not stores:
            print("âŒ í¬ë¡¤ë§ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"ğŸ“Š ì´ {len(stores)}ê°œ ìƒ˜í”Œ ë°ì´í„° í™•ì¸")
        for i, store in enumerate(stores, 1):
            converted = migration.convert_to_main_schema(store)
            print(f"{i}. {converted['name']}")
            print(f"   ì£¼ì†Œ: {converted['address']}")
            print(f"   ë¦¬í•„ì•„ì´í…œ: {', '.join(converted['refill_items'][:3])}")
            print()
        
        # 2. ì „ì²´ ë§ˆì´ê·¸ë ˆì´ì…˜ SQL ìƒì„±
        print("ğŸ“ ë§ˆì´ê·¸ë ˆì´ì…˜ SQL ìƒì„± ì¤‘...")
        sql_statements = migration.generate_migration_sql(limit=100)  # í…ŒìŠ¤íŠ¸ìš© 100ê°œ
        
        # 3. SQL íŒŒì¼ ì €ì¥
        output_file = "safe_migration.sql"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("-- ì•ˆì „í•œ ë§ˆì´ê·¸ë ˆì´ì…˜ SQL\n")
            f.write(f"-- ìƒì„±ì¼: {datetime.now()}\n")
            f.write("-- í¬ë¡¤ë§ DB â†’ ë©”ì¸ í”„ë¡œì íŠ¸ DB\n\n")
            f.write("\n\n".join(sql_statements))
        
        print(f"âœ… ë§ˆì´ê·¸ë ˆì´ì…˜ SQL ìƒì„± ì™„ë£Œ: {output_file}")
        print(f"ğŸ“„ ì´ {len(sql_statements)}ê°œ ê°€ê²Œ ë°ì´í„° ì¤€ë¹„ë¨")
        
        print("\nğŸ”§ ë‹¤ìŒ ë‹¨ê³„:")
        print("1. safe_migration.sql íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”")
        print("2. ë©”ì¸ í”„ë¡œì íŠ¸ DBì—ì„œ ì‹¤í–‰í•˜ì„¸ìš”")
        print("3. ë°ì´í„° ë¬´ê²°ì„±ì„ ê²€ì¦í•˜ì„¸ìš”")
        
    except Exception as e:
        print(f"âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {e}")
        logger.error(f"ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {e}")
    finally:
        migration.close()
        print("\nğŸ”š ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ")

if __name__ == "__main__":
    main()