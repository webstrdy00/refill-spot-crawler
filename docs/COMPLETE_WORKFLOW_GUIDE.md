# Refill Spot í¬ë¡¤ë§ë¶€í„° ë§ˆì´ê·¸ë ˆì´ì…˜ê¹Œì§€ ì™„ì „ ê°€ì´ë“œ

## ğŸ“– ëª©ì°¨
1. [í”„ë¡œì íŠ¸ êµ¬ì¡°](#í”„ë¡œì íŠ¸-êµ¬ì¡°)
2. [ì´ˆê¸° ì„¤ì •](#ì´ˆê¸°-ì„¤ì •)
3. [ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •](#ë°ì´í„°ë² ì´ìŠ¤-ì„¤ì •)
4. [í¬ë¡¤ë§ ì‹¤í–‰](#í¬ë¡¤ë§-ì‹¤í–‰)
5. [ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜](#ë°ì´í„°-ë§ˆì´ê·¸ë ˆì´ì…˜)
6. [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)

## ğŸ—ï¸ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
refill-spot-crawler/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ docker-compose.yml     # Docker ì„œë¹„ìŠ¤ ì„¤ì •
â”‚   â”œâ”€â”€ init.sql              # PostgreSQL ì´ˆê¸° ìŠ¤í‚¤ë§ˆ
â”‚   â”œâ”€â”€ supabase_schema.sql   # Supabase ìŠ¤í‚¤ë§ˆ
â”‚   â””â”€â”€ supabase_initial_data.sql
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/                 # í•µì‹¬ ê¸°ëŠ¥
â”‚   â”‚   â”œâ”€â”€ database.py       # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
â”‚   â”‚   â”œâ”€â”€ optimized_database.py
â”‚   â”‚   â””â”€â”€ data_enhancement.py
â”‚   â”œâ”€â”€ utils/                # ìœ í‹¸ë¦¬í‹°
â”‚   â”‚   â”œâ”€â”€ main.py          # ë©”ì¸ ì‹¤í–‰ íŒŒì¼
â”‚   â”‚   â”œâ”€â”€ parallel_crawler.py
â”‚   â”‚   â”œâ”€â”€ seoul_scheduler.py
â”‚   â”‚   â”œâ”€â”€ supabase_migration.py
â”‚   â”‚   â””â”€â”€ data_migration.py
â”‚   â”œâ”€â”€ automation/           # ìë™í™” ì‹œìŠ¤í…œ
â”‚   â””â”€â”€ tests/               # í…ŒìŠ¤íŠ¸ íŒŒì¼
â”œâ”€â”€ docs/                    # ë¬¸ì„œ
â””â”€â”€ run_crawler.py          # ê°„ë‹¨ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
```

## ğŸš€ ì´ˆê¸° ì„¤ì •

### 1. ê°€ìƒí™˜ê²½ ì„¤ì • (Windows)
```bash
# ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”
setup_venv.bat

# ë˜ëŠ” ìˆ˜ë™ìœ¼ë¡œ:
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
```

### 2. Chrome WebDriver ì„¤ì¹˜
- Chrome ë¸Œë¼ìš°ì € ìµœì‹  ë²„ì „ ì„¤ì¹˜
- ChromeDriverëŠ” ìë™ìœ¼ë¡œ ê´€ë¦¬ë¨ (selenium-manager)

## ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •

### 1. Docker ì»¨í…Œì´ë„ˆ ì‹œì‘
```bash
cd config
docker-compose up -d
```

### 2. ì»¨í…Œì´ë„ˆ ìƒíƒœ í™•ì¸
```bash
docker-compose ps
```

ì˜ˆìƒ ì¶œë ¥:
```
NAME                  IMAGE                    STATUS
refill_spot_db        postgis/postgis:15-3.3   Up (healthy)
refill_spot_pgadmin   dpage/pgadmin4:latest    Up
refill_spot_redis     redis:7-alpine           Up
```

### 3. ë°ì´í„°ë² ì´ìŠ¤ ì ‘ì† ì •ë³´
- **Host**: localhost
- **Port**: 5432
- **Database**: refill_spot
- **Username**: postgres
- **Password**: refillspot123

### 4. PgAdmin ì ‘ì† (ì˜µì…˜)
- URL: http://localhost:5050
- Email: admin@refillspot.com
- Password: admin123

### 5. í…Œì´ë¸” ìƒì„± í™•ì¸ ë° ìˆ˜ì •
ê¸°ì¡´ ì»¨í…Œì´ë„ˆì—ì„œ ëˆ„ë½ëœ í…Œì´ë¸”ì´ ìˆì„ ê²½ìš°:

```bash
# í…Œì´ë¸” ëª©ë¡ í™•ì¸
docker exec -i refill_spot_db psql -U postgres -d refill_spot -c "\dt"

# ëˆ„ë½ëœ í…Œì´ë¸” ìˆ˜ë™ ìƒì„±
docker exec -i refill_spot_db psql -U postgres -d refill_spot -c "
CREATE TABLE IF NOT EXISTS store_categories (
  store_id INTEGER REFERENCES stores(id) ON DELETE CASCADE,
  category_id INTEGER REFERENCES categories(id) ON DELETE CASCADE,
  PRIMARY KEY (store_id, category_id)
);

CREATE TABLE IF NOT EXISTS crawling_logs (
  id SERIAL PRIMARY KEY,
  keyword VARCHAR(100),
  rect_area VARCHAR(200),
  stores_found INTEGER DEFAULT 0,
  stores_processed INTEGER DEFAULT 0,
  errors INTEGER DEFAULT 0,
  status VARCHAR(20) DEFAULT 'running',
  error_message TEXT,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  completed_at TIMESTAMP
);"
```

## ğŸ•·ï¸ í¬ë¡¤ë§ ì‹¤í–‰

### 1. ë‹¨ê³„ë³„ í¬ë¡¤ë§ ì‹¤í–‰

#### 1ë‹¨ê³„: í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§
```bash
python src/utils/main.py stage1
```

#### 2ë‹¨ê³„: ê°•ë‚¨ ì§€ì—­ í¬ë¡¤ë§
```bash
python src/utils/main.py stage2
```

#### 3ë‹¨ê³„: ì„œìš¸ ì£¼ìš” ì§€ì—­ í¬ë¡¤ë§
```bash
python src/utils/main.py stage3
```

#### 4ë‹¨ê³„: ì„œìš¸ ì™„ì „ ì»¤ë²„ë¦¬ì§€
```bash
python src/utils/main.py stage4
```

#### 5ë‹¨ê³„: ì „êµ­ í™•ì¥ (ê³ ê¸‰)
```bash
python src/utils/main.py stage5
```

### 2. ê°„ë‹¨ ì‹¤í–‰ ë°©ë²•
```bash
python run_crawler.py
```

### 3. í¬ë¡¤ë§ ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§

#### ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í™•ì¸:
```sql
-- ì´ ìˆ˜ì§‘ëœ ê°€ê²Œ ìˆ˜
SELECT COUNT(*) FROM stores;

-- ì§€ì—­ë³„ í†µê³„
SELECT 
  CASE 
    WHEN address LIKE '%ê°•ë‚¨%' THEN 'ê°•ë‚¨'
    WHEN address LIKE '%í™ëŒ€%' OR address LIKE '%ë§ˆí¬%' THEN 'í™ëŒ€/ë§ˆí¬'
    WHEN address LIKE '%ê°•ë¶%' THEN 'ê°•ë¶'
    ELSE 'ê¸°íƒ€'
  END as region,
  COUNT(*) as store_count
FROM stores 
GROUP BY region;

-- í¬ë¡¤ë§ ë¡œê·¸ í™•ì¸
SELECT * FROM crawling_logs ORDER BY created_at DESC LIMIT 10;
```

#### íŒŒì´ì¬ ìŠ¤í¬ë¦½íŠ¸ë¡œ í™•ì¸:
```bash
python src/tests/check_db.py
```

### 4. í¬ë¡¤ë§ ê²°ê³¼ ê²€ì¦
```bash
# ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
python src/tests/stage5_mini_test.py

# ìƒì„¸ ë°ì´í„° ê²€ì¦
python src/utils/data_validator.py
```

## ğŸ“¦ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜

### 1. Supabaseë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜

#### Supabase í”„ë¡œì íŠ¸ ì„¤ì •:
1. [Supabase](https://supabase.com) ê³„ì • ìƒì„±
2. ìƒˆ í”„ë¡œì íŠ¸ ìƒì„±
3. API URLê³¼ anon key í™•ì¸

#### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •:
```bash
# .env íŒŒì¼ ìƒì„±
SUPABASE_URL=your_supabase_url
SUPABASE_KEY=your_supabase_anon_key
```

#### ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰:
```bash
# ìŠ¤í‚¤ë§ˆ ìƒì„±
python src/utils/supabase_migration.py

# ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜
python src/utils/data_migration.py
```

### 2. ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦
```bash
# ë§ˆì´ê·¸ë ˆì´ì…˜ ê²°ê³¼ í™•ì¸
python src/tests/test_connection.py

# ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬
python migrate_to_supabase.py
```

## ğŸ”§ ë¬¸ì œ í•´ê²°

### ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œë“¤

#### 1. "store_categories í…Œì´ë¸” ëˆ„ë½" ì˜¤ë¥˜
**í•´ê²°ì±…:**
```bash
docker exec -i refill_spot_db psql -U postgres -d refill_spot -c "
CREATE TABLE IF NOT EXISTS store_categories (
  store_id INTEGER REFERENCES stores(id) ON DELETE CASCADE,
  category_id INTEGER REFERENCES categories(id) ON DELETE CASCADE,
  PRIMARY KEY (store_id, category_id)
);"
```

#### 2. Docker ì»¨í…Œì´ë„ˆ ì—°ê²° ì‹¤íŒ¨
**í•´ê²°ì±…:**
```bash
# ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘
docker-compose down
docker-compose up -d

# ë¡œê·¸ í™•ì¸
docker-compose logs postgres
```

#### 3. ChromeDriver ì˜¤ë¥˜
**í•´ê²°ì±…:**
```bash
# Chrome ë¸Œë¼ìš°ì € ì—…ë°ì´íŠ¸
# ë˜ëŠ” ìˆ˜ë™ ChromeDriver ì„¤ì¹˜
```

#### 4. ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜
**í•´ê²°ì±…:**
```python
# src/utils/main.py ì—ì„œ ë°°ì¹˜ í¬ê¸° ì¡°ì •
BATCH_SIZE = 50  # ê¸°ë³¸ê°’ì—ì„œ ì¤„ì´ê¸°
```

#### 5. í¬ë¡¤ë§ ì†ë„ ì €í•˜
**í•´ê²°ì±…:**
```python
# ë™ì‹œ ì‹¤í–‰ í”„ë¡œì„¸ìŠ¤ ìˆ˜ ì¡°ì •
MAX_WORKERS = 3  # CPU ì½”ì–´ ìˆ˜ì— ë§ê²Œ ì¡°ì •
```

### ì„±ëŠ¥ ìµœì í™”

#### 1. ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ìŠ¤ í™•ì¸
```sql
-- ì¸ë±ìŠ¤ ëª©ë¡ í™•ì¸
SELECT indexname, tablename FROM pg_indexes WHERE schemaname = 'public';
```

#### 2. í¬ë¡¤ë§ ë³‘ë ¬ ì²˜ë¦¬ ì¡°ì •
```python
# src/utils/parallel_crawler.py
# CPU ì„±ëŠ¥ì— ë§ê²Œ ì›Œì»¤ ìˆ˜ ì¡°ì •
workers = min(4, cpu_count())
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ìœ ì§€ë³´ìˆ˜

### 1. ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
```bash
# ì „ì²´ ì‹œìŠ¤í…œ ìƒíƒœ
python src/tests/stage6_test.py

# ë°ì´í„°ë² ì´ìŠ¤ í†µê³„
python src/tests/check_db_tables.py
```

### 2. ìë™í™” ìŠ¤ì¼€ì¤„ë§
```bash
# ì •ê¸° í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ ì„¤ì •
python src/utils/seoul_scheduler.py
```

### 3. ë¡œê·¸ ëª¨ë‹ˆí„°ë§
```bash
# í¬ë¡¤ë§ ë¡œê·¸ í™•ì¸
tail -f crawling.log

# ë°ì´í„°ë² ì´ìŠ¤ ë¡œê·¸ í™•ì¸
docker logs refill_spot_db
```

## ğŸ¯ ì¶”ì²œ ì›Œí¬í”Œë¡œìš°

### ì´ˆê¸° ì„¤ì • (í•œ ë²ˆë§Œ):
1. ê°€ìƒí™˜ê²½ ì„¤ì • â†’ Docker ì‹¤í–‰ â†’ ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸
2. í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§ (stage1) ì‹¤í–‰
3. ê²°ê³¼ í™•ì¸ í›„ ë‹¨ê³„ë³„ í¬ë¡¤ë§ ì§„í–‰

### ì •ê¸° ì—…ë°ì´íŠ¸:
1. stage4 ì‹¤í–‰ (ì„œìš¸ ì „ì²´ ì—…ë°ì´íŠ¸)
2. ë°ì´í„° í’ˆì§ˆ ê²€ì¦
3. Supabase ë§ˆì´ê·¸ë ˆì´ì…˜
4. ê²°ê³¼ ëª¨ë‹ˆí„°ë§

### ë¬¸ì œ ë°œìƒ ì‹œ:
1. ë¡œê·¸ í™•ì¸ â†’ ì›ì¸ íŒŒì•…
2. í•´ë‹¹ êµ¬ê°„ ì¬í¬ë¡¤ë§
3. ë°ì´í„° ê²€ì¦ í›„ ë§ˆì´ê·¸ë ˆì´ì…˜

ì´ ê°€ì´ë“œë¥¼ ë”°ë¼í•˜ë©´ í¬ë¡¤ë§ë¶€í„° ë§ˆì´ê·¸ë ˆì´ì…˜ê¹Œì§€ ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì•ˆì •ì ìœ¼ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.