"""
5ë‹¨ê³„ ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ ì†Œê·œëª¨ ì‹¤ì œ í…ŒìŠ¤íŠ¸
2-3ê°œ êµ¬ë§Œ ëŒ€ìƒìœ¼ë¡œ ì‹¤ì œ ë³‘ë ¬ í¬ë¡¤ë§ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import logging
import time
import multiprocessing as mp
from typing import Dict, List

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(process)d - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# í…ŒìŠ¤íŠ¸ìš© ì†Œê·œëª¨ ì§€ì—­ ì„¤ì •
TEST_DISTRICTS = {
    "ê°•ë‚¨êµ¬": {
        "keywords": ["ê°•ë‚¨êµ¬ ë¬´í•œë¦¬í•„", "ê°•ë‚¨ ê³ ê¸°ë¬´í•œë¦¬í•„", "ê°•ë‚¨ì—­ ë¬´í•œë¦¬í•„"],
        "rect": "37.4979,127.0276,37.5279,127.0576"
    },
    "ì„œì´ˆêµ¬": {
        "keywords": ["ì„œì´ˆêµ¬ ë¬´í•œë¦¬í•„", "ì„œì´ˆ ê³ ê¸°ë¬´í•œë¦¬í•„", "êµëŒ€ì—­ ë¬´í•œë¦¬í•„"], 
        "rect": "37.4833,127.0322,37.5133,127.0622"
    }
}

def mini_crawl_district_worker(district_name: str, district_info: Dict) -> Dict:
    """ì†Œê·œëª¨ êµ¬ í¬ë¡¤ë§ ì›Œì»¤"""
    start_time = time.time()
    process_id = mp.current_process().pid
    
    logger.info(f"[PID:{process_id}] {district_name} í¬ë¡¤ë§ ì‹œì‘")
    
    try:
        from crawler import DiningCodeCrawler
        from data_enhancement import DataEnhancer
        from optimized_database import OptimizedDatabaseManager
        
        # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        crawler = DiningCodeCrawler()
        enhancer = DataEnhancer()
        db_manager = OptimizedDatabaseManager()
        
        all_stores = []
        keywords = district_info.get('keywords', [])
        rect = district_info.get('rect', '')
        
        # í‚¤ì›Œë“œë³„ í¬ë¡¤ë§ (ì œí•œì ìœ¼ë¡œ)
        for keyword in keywords[:1]:  # ì²« ë²ˆì§¸ í‚¤ì›Œë“œë§Œ í…ŒìŠ¤íŠ¸
            logger.info(f"[PID:{process_id}] {district_name} - í‚¤ì›Œë“œ: {keyword}")
            
            stores = crawler.get_store_list(keyword, rect)
            
            if stores:
                # ìƒìœ„ 3ê°œë§Œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ (í…ŒìŠ¤íŠ¸ìš©)
                detailed_stores = []
                for store in stores[:3]:
                    try:
                        detailed_store = crawler.get_store_detail(store)
                        if detailed_store:
                            detailed_stores.append(detailed_store)
                    except Exception as e:
                        logger.warning(f"[PID:{process_id}] ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                        continue
                
                all_stores.extend(detailed_stores)
                logger.info(f"[PID:{process_id}] {district_name} - {len(detailed_stores)}ê°œ ìƒì„¸ì •ë³´ ìˆ˜ì§‘")
        
        # ë°ì´í„° ê°•í™”
        if all_stores:
            enhanced_stores, enhancement_stats = enhancer.enhance_stores_data(all_stores)
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ (ê³ ì„±ëŠ¥ ì‚½ì… ì‚¬ìš©)
            if enhanced_stores:
                store_ids = db_manager.insert_stores_high_performance(enhanced_stores)
                logger.info(f"[PID:{process_id}] {district_name}: {len(store_ids)}ê°œ ê°€ê²Œ ì €ì¥ ì™„ë£Œ")
        
        processing_time = time.time() - start_time
        
        result = {
            'district_name': district_name,
            'success': True,
            'stores_found': len(all_stores),
            'stores_processed': len(enhanced_stores) if 'enhanced_stores' in locals() else 0,
            'processing_time': processing_time,
            'process_id': process_id
        }
        
        logger.info(f"[PID:{process_id}] {district_name} ì™„ë£Œ: "
                   f"{result['stores_processed']}ê°œ ì²˜ë¦¬, {processing_time:.1f}ì´ˆ ì†Œìš”")
        
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        crawler.close()
        db_manager.close()
        
        return result
        
    except Exception as e:
        logger.error(f"[PID:{process_id}] {district_name} ì‹¤íŒ¨: {e}")
        return {
            'district_name': district_name,
            'success': False,
            'error': str(e),
            'processing_time': time.time() - start_time,
            'process_id': process_id
        }

def run_mini_parallel_test():
    """ì†Œê·œëª¨ ë³‘ë ¬ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    logger.info("ğŸ§ª 5ë‹¨ê³„ ì‹œìŠ¤í…œ ì†Œê·œëª¨ ë³‘ë ¬ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    start_time = time.time()
    
    # ë©€í‹°í”„ë¡œì„¸ì‹± ì„¤ì •
    mp.set_start_method('spawn', force=True)
    
    # ë³‘ë ¬ ì‹¤í–‰
    with mp.Pool(processes=2) as pool:
        # ì‘ì—… ìƒì„±
        tasks = [(name, info) for name, info in TEST_DISTRICTS.items()]
        
        # ë³‘ë ¬ ì‹¤í–‰
        results = pool.starmap(mini_crawl_district_worker, tasks)
    
    total_time = time.time() - start_time
    
    # ê²°ê³¼ ë¶„ì„
    successful_results = [r for r in results if r.get('success', False)]
    failed_results = [r for r in results if not r.get('success', False)]
    
    total_stores_processed = sum(r.get('stores_processed', 0) for r in successful_results)
    total_stores_found = sum(r.get('stores_found', 0) for r in successful_results)
    
    # ì„±ëŠ¥ ê³„ì‚°
    stores_per_hour = (total_stores_processed / total_time * 3600) if total_time > 0 else 0
    
    # ê²°ê³¼ ë¦¬í¬íŠ¸
    logger.info("="*60)
    logger.info("ğŸ“Š ì†Œê·œëª¨ ë³‘ë ¬ í…ŒìŠ¤íŠ¸ ê²°ê³¼")
    logger.info("="*60)
    logger.info(f"í…ŒìŠ¤íŠ¸ ì§€ì—­: {len(TEST_DISTRICTS)}ê°œ êµ¬")
    logger.info(f"ì„±ê³µ: {len(successful_results)}ê°œ, ì‹¤íŒ¨: {len(failed_results)}ê°œ")
    logger.info(f"ì´ ë°œê²¬ ê°€ê²Œ: {total_stores_found}ê°œ")
    logger.info(f"ì´ ì²˜ë¦¬ ê°€ê²Œ: {total_stores_processed}ê°œ")
    logger.info(f"ì´ ì†Œìš”ì‹œê°„: {total_time:.1f}ì´ˆ")
    logger.info(f"ì²˜ë¦¬ ì†ë„: {stores_per_hour:.0f}ê°œ/ì‹œê°„")
    
    # ê°œë³„ ê²°ê³¼
    for result in results:
        status = "âœ…" if result.get('success') else "âŒ"
        district = result['district_name']
        time_taken = result.get('processing_time', 0)
        processed = result.get('stores_processed', 0)
        logger.info(f"{status} {district}: {processed}ê°œ ì²˜ë¦¬, {time_taken:.1f}ì´ˆ")
    
    # ì„±ëŠ¥ í‰ê°€
    if stores_per_hour >= 100:
        logger.info("ğŸ¯ ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„±! (100ê°œ/ì‹œê°„ ì´ìƒ)")
    elif stores_per_hour >= 50:
        logger.info("âš¡ ì–‘í˜¸í•œ ì„±ëŠ¥ (50ê°œ/ì‹œê°„ ì´ìƒ)")
    else:
        logger.info("ğŸ”§ ì„±ëŠ¥ ê°œì„  í•„ìš” (50ê°œ/ì‹œê°„ ë¯¸ë§Œ)")
    
    logger.info("="*60)
    
    return {
        'total_time': total_time,
        'stores_per_hour': stores_per_hour,
        'success_rate': len(successful_results) / len(results) * 100,
        'total_processed': total_stores_processed,
        'results': results
    }

if __name__ == "__main__":
    result = run_mini_parallel_test()
    
    print(f"\nğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print(f"ì²˜ë¦¬ ì†ë„: {result['stores_per_hour']:.0f}ê°œ/ì‹œê°„")
    print(f"ì„±ê³µë¥ : {result['success_rate']:.1f}%")
    print(f"ì´ ì²˜ë¦¬: {result['total_processed']}ê°œ ê°€ê²Œ") 