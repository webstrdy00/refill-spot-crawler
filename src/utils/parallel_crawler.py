"""
5ë‹¨ê³„: ë³‘ë ¬ í¬ë¡¤ë§ ì‹œìŠ¤í…œ
ë©€í‹°í”„ë¡œì„¸ì‹±ì„ í™œìš©í•œ ê³ ì„±ëŠ¥ í¬ë¡¤ë§
"""

import logging
import time
import multiprocessing as mp
from multiprocessing import Pool, Manager, Queue
from concurrent.futures import ProcessPoolExecutor, as_completed
from typing import List, Dict, Tuple, Optional
from datetime import datetime
import psutil
import os
import signal
import sys
from dataclasses import dataclass, asdict
import json
from pathlib import Path

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ import
from ..core.database import DatabaseManager
from ..core.crawler import DiningCodeCrawler
from ..core.caching_system import CacheManager
from .seoul_districts import SEOUL_DISTRICTS

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('parallel_crawler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class CrawlingTask:
    """í¬ë¡¤ë§ ì‘ì—… ë‹¨ìœ„"""
    district_name: str
    district_info: Dict
    task_id: str
    priority: int = 1
    retry_count: int = 0
    max_retries: int = 3
    
@dataclass
class CrawlingResult:
    """í¬ë¡¤ë§ ê²°ê³¼"""
    task_id: str
    district_name: str
    success: bool
    stores_found: int
    stores_processed: int
    processing_time: float
    error_message: Optional[str] = None
    memory_usage_mb: float = 0.0

class PerformanceMonitor:
    """ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self):
        self.redis_client = None
        self.setup_redis()
        
    def setup_redis(self):
        """Redis ì—°ê²° ì„¤ì •"""
        try:
            self.redis_client = redis.Redis(
                host='localhost', 
                port=6379, 
                db=0, 
                decode_responses=True
            )
            self.redis_client.ping()
            logger.info("Redis ì—°ê²° ì„±ê³µ")
        except Exception as e:
            logger.warning(f"Redis ì—°ê²° ì‹¤íŒ¨: {e}. ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ë§Œ ì‚¬ìš©")
            self.redis_client = None
    
    def log_performance(self, process_id: int, district: str, metrics: Dict):
        """ì„±ëŠ¥ ì§€í‘œ ë¡œê¹…"""
        timestamp = datetime.now().isoformat()
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
        process = psutil.Process(process_id)
        memory_mb = process.memory_info().rss / 1024 / 1024
        
        performance_data = {
            'timestamp': timestamp,
            'process_id': process_id,
            'district': district,
            'memory_mb': memory_mb,
            'cpu_percent': process.cpu_percent(),
            **metrics
        }
        
        # Redisì— ì €ì¥ (ê°€ëŠ¥í•œ ê²½ìš°)
        if self.redis_client:
            try:
                key = f"performance:{process_id}:{timestamp}"
                self.redis_client.setex(key, 3600, json.dumps(performance_data))
            except Exception as e:
                logger.warning(f"Redis ì„±ëŠ¥ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
        
        # ë¡œê·¸ì—ë„ ê¸°ë¡
        logger.info(f"ì„±ëŠ¥ ì§€í‘œ - PID:{process_id}, êµ¬:{district}, "
                   f"ë©”ëª¨ë¦¬:{memory_mb:.1f}MB, CPU:{performance_data.get('cpu_percent', 0):.1f}%")
        
        return performance_data
    
    def get_system_status(self) -> Dict:
        """ì‹œìŠ¤í…œ ì „ì²´ ìƒíƒœ ì¡°íšŒ"""
        return {
            'total_memory_gb': psutil.virtual_memory().total / 1024 / 1024 / 1024,
            'available_memory_gb': psutil.virtual_memory().available / 1024 / 1024 / 1024,
            'memory_percent': psutil.virtual_memory().percent,
            'cpu_count': psutil.cpu_count(),
            'cpu_percent': psutil.cpu_percent(interval=1),
            'timestamp': datetime.now().isoformat()
        }

class AdaptiveConcurrencyController:
    """ì ì‘í˜• ë™ì‹œì„± ì œì–´"""
    
    def __init__(self, initial_workers: int = 4):
        self.current_workers = initial_workers
        self.min_workers = 2
        self.max_workers = min(8, mp.cpu_count())
        self.performance_history = []
        self.adjustment_threshold = 5  # 5ë²ˆì˜ ì¸¡ì • í›„ ì¡°ì •
        
    def should_adjust_workers(self, performance_metrics: List[Dict]) -> Optional[int]:
        """ì›Œì»¤ ìˆ˜ ì¡°ì • í•„ìš”ì„± íŒë‹¨"""
        if len(performance_metrics) < self.adjustment_threshold:
            return None
            
        recent_metrics = performance_metrics[-self.adjustment_threshold:]
        
        # í‰ê·  ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        avg_memory = sum(m.get('memory_mb', 0) for m in recent_metrics) / len(recent_metrics)
        avg_processing_time = sum(m.get('processing_time', 0) for m in recent_metrics) / len(recent_metrics)
        avg_success_rate = sum(m.get('success_rate', 100) for m in recent_metrics) / len(recent_metrics)
        
        # ì¡°ì • ë¡œì§
        if avg_memory > 1500:  # 1.5GB ì´ìƒ ì‚¬ìš© ì‹œ ì›Œì»¤ ê°ì†Œ
            new_workers = max(self.min_workers, self.current_workers - 1)
            logger.info(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ({avg_memory:.1f}MB). ì›Œì»¤ ìˆ˜ ê°ì†Œ: {self.current_workers} -> {new_workers}")
            return new_workers
            
        elif avg_success_rate < 80:  # ì„±ê³µë¥  80% ë¯¸ë§Œ ì‹œ ì›Œì»¤ ê°ì†Œ
            new_workers = max(self.min_workers, self.current_workers - 1)
            logger.info(f"ì„±ê³µë¥  ë‚®ìŒ({avg_success_rate:.1f}%). ì›Œì»¤ ìˆ˜ ê°ì†Œ: {self.current_workers} -> {new_workers}")
            return new_workers
            
        elif avg_memory < 800 and avg_success_rate > 95 and avg_processing_time < 300:  # ì—¬ìœ  ìˆì„ ë•Œ ì›Œì»¤ ì¦ê°€
            new_workers = min(self.max_workers, self.current_workers + 1)
            logger.info(f"ì„±ëŠ¥ ì—¬ìœ  ìˆìŒ. ì›Œì»¤ ìˆ˜ ì¦ê°€: {self.current_workers} -> {new_workers}")
            return new_workers
            
        return None
    
    def update_workers(self, new_count: int):
        """ì›Œì»¤ ìˆ˜ ì—…ë°ì´íŠ¸"""
        self.current_workers = new_count

def crawl_district_worker(task: CrawlingTask) -> CrawlingResult:
    """ê°œë³„ êµ¬ í¬ë¡¤ë§ ì›Œì»¤ í•¨ìˆ˜"""
    start_time = time.time()
    process_id = os.getpid()
    
    logger.info(f"[PID:{process_id}] {task.district_name} í¬ë¡¤ë§ ì‹œì‘")
    
    crawler = None
    db_manager = None
    enhancer = None
    
    try:
        # ì„±ëŠ¥ ëª¨ë‹ˆí„° ì´ˆê¸°í™”
        monitor = PerformanceMonitor()
        
        # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        crawler = DiningCodeCrawler()
        db_manager = DatabaseManager()
        enhancer = DataEnhancer()
        
        district_info = task.district_info
        all_stores = []
        
        # êµ¬ë³„ í‚¤ì›Œë“œë¡œ í¬ë¡¤ë§
        keywords = district_info.get('keywords', [])
        rect = district_info.get('rect', '')
        
        # keywordsê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ í‚¤ì›Œë“œ ìƒì„±
        if not keywords:
            district_name = task.district_name
            keywords = [
                f"{district_name} ë¬´í•œë¦¬í•„",
                f"{district_name} ê³ ê¸°ë¬´í•œë¦¬í•„", 
                f"{district_name} ë¬´í•œë¦¬í•„ ë§›ì§‘"
            ]
        
        for keyword in keywords:
            try:
                logger.info(f"[PID:{process_id}] {task.district_name} - í‚¤ì›Œë“œ: {keyword}")
                
                # ê°€ê²Œ ëª©ë¡ ìˆ˜ì§‘
                stores = crawler.get_store_list(keyword, rect)
                
                if stores:
                    # ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ (ë°°ì¹˜ ì²˜ë¦¬)
                    detailed_stores = []
                    for store in stores:
                        try:
                            detailed_store = crawler.get_store_detail(store)
                            if detailed_store:
                                detailed_stores.append(detailed_store)
                        except Exception as e:
                            logger.warning(f"[PID:{process_id}] ê°€ê²Œ ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                            continue
                    
                    all_stores.extend(detailed_stores)
                    
                    # ì„±ëŠ¥ ì§€í‘œ ê¸°ë¡
                    monitor.log_performance(process_id, task.district_name, {
                        'keyword': keyword,
                        'stores_found': len(stores),
                        'stores_detailed': len(detailed_stores),
                        'processing_time': time.time() - start_time
                    })
                
                # í‚¤ì›Œë“œ ê°„ ì§€ì—°
                time.sleep(2)
                
            except Exception as e:
                logger.error(f"[PID:{process_id}] í‚¤ì›Œë“œ '{keyword}' ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        # ì¤‘ë³µ ì œê±°
        unique_stores = remove_duplicates(all_stores)
        
        # ë°ì´í„° ê³ ë„í™”
        enhanced_stores, enhancement_stats = enhancer.enhance_stores_data(unique_stores)
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        if enhanced_stores:
            store_ids = db_manager.insert_stores_batch(enhanced_stores)
            logger.info(f"[PID:{process_id}] {task.district_name}: {len(store_ids)}ê°œ ê°€ê²Œ ì €ì¥ ì™„ë£Œ")
        
        processing_time = time.time() - start_time
        
        # ìµœì¢… ì„±ëŠ¥ ì§€í‘œ
        final_memory = psutil.Process(process_id).memory_info().rss / 1024 / 1024
        
        result = CrawlingResult(
            task_id=task.task_id,
            district_name=task.district_name,
            success=True,
            stores_found=len(all_stores),
            stores_processed=len(enhanced_stores) if enhanced_stores else 0,
            processing_time=processing_time,
            memory_usage_mb=final_memory
        )
        
        logger.info(f"[PID:{process_id}] {task.district_name} ì™„ë£Œ: "
                   f"{result.stores_processed}ê°œ ì²˜ë¦¬, {processing_time:.1f}ì´ˆ ì†Œìš”")
        
        return result
        
    except Exception as e:
        error_msg = f"êµ¬ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}\n{traceback.format_exc()}"
        logger.error(f"[PID:{process_id}] {task.district_name} ì‹¤íŒ¨: {error_msg}")
        
        return CrawlingResult(
            task_id=task.task_id,
            district_name=task.district_name,
            success=False,
            stores_found=0,
            stores_processed=0,
            processing_time=time.time() - start_time,
            error_message=error_msg
        )
        
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        if crawler:
            try:
                crawler.close()
            except:
                pass
        if db_manager:
            try:
                db_manager.close()
            except:
                pass

def remove_duplicates(stores: List[Dict]) -> List[Dict]:
    """ì¤‘ë³µ ê°€ê²Œ ì œê±° (ê°„ë‹¨í•œ ë²„ì „)"""
    seen = set()
    unique_stores = []
    
    for store in stores:
        # diningcode_place_id ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
        place_id = store.get('diningcode_place_id')
        if place_id and place_id not in seen:
            seen.add(place_id)
            unique_stores.append(store)
    
    logger.info(f"ì¤‘ë³µ ì œê±°: {len(stores)} -> {len(unique_stores)}")
    return unique_stores

class ParallelCrawlingManager:
    """ë³‘ë ¬ í¬ë¡¤ë§ ê´€ë¦¬ì"""
    
    def __init__(self, max_workers: int = 4):
        self.max_workers = max_workers
        self.monitor = PerformanceMonitor()
        self.concurrency_controller = AdaptiveConcurrencyController(max_workers)
        self.results = []
        self.failed_tasks = []
        
    def create_crawling_tasks(self) -> List[CrawlingTask]:
        """í¬ë¡¤ë§ ì‘ì—… ìƒì„±"""
        tasks = []
        
        for i, (district_name, district_info) in enumerate(SEOUL_DISTRICTS.items()):
            task = CrawlingTask(
                district_name=district_name,
                district_info=district_info,
                task_id=f"task_{i:02d}_{district_name}",
                priority=1
            )
            tasks.append(task)
        
        logger.info(f"ì´ {len(tasks)}ê°œ í¬ë¡¤ë§ ì‘ì—… ìƒì„±")
        return tasks
    
    def run_parallel_crawling(self) -> Dict:
        """ë³‘ë ¬ í¬ë¡¤ë§ ì‹¤í–‰"""
        start_time = time.time()
        tasks = self.create_crawling_tasks()
        
        logger.info(f"=== ë³‘ë ¬ í¬ë¡¤ë§ ì‹œì‘: {len(tasks)}ê°œ êµ¬, {self.max_workers}ê°œ ì›Œì»¤ ===")
        
        # ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
        system_status = self.monitor.get_system_status()
        logger.info(f"ì‹œìŠ¤í…œ ìƒíƒœ: ë©”ëª¨ë¦¬ {system_status['available_memory_gb']:.1f}GB ì‚¬ìš© ê°€ëŠ¥, "
                   f"CPU {system_status['cpu_count']}ì½”ì–´")
        
        completed_tasks = 0
        total_stores_found = 0
        total_stores_processed = 0
        
        # ProcessPoolExecutorë¡œ ë³‘ë ¬ ì‹¤í–‰
        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            # ì‘ì—… ì œì¶œ
            future_to_task = {executor.submit(crawl_district_worker, task): task for task in tasks}
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for future in as_completed(future_to_task):
                task = future_to_task[future]
                
                try:
                    result = future.result()
                    self.results.append(result)
                    
                    if result.success:
                        completed_tasks += 1
                        total_stores_found += result.stores_found
                        total_stores_processed += result.stores_processed
                        
                        logger.info(f"âœ… {result.district_name} ì™„ë£Œ "
                                   f"({completed_tasks}/{len(tasks)}) - "
                                   f"{result.stores_processed}ê°œ ì²˜ë¦¬")
                    else:
                        self.failed_tasks.append(task)
                        logger.error(f"âŒ {result.district_name} ì‹¤íŒ¨: {result.error_message}")
                        
                except Exception as e:
                    logger.error(f"ì‘ì—… ê²°ê³¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    self.failed_tasks.append(task)
        
        total_time = time.time() - start_time
        
        # ìµœì¢… ê²°ê³¼ ìš”ì•½
        summary = {
            'total_tasks': len(tasks),
            'completed_tasks': completed_tasks,
            'failed_tasks': len(self.failed_tasks),
            'success_rate': completed_tasks / len(tasks) * 100,
            'total_stores_found': total_stores_found,
            'total_stores_processed': total_stores_processed,
            'total_time_minutes': total_time / 60,
            'average_time_per_district': total_time / len(tasks),
            'stores_per_hour': total_stores_processed / (total_time / 3600) if total_time > 0 else 0
        }
        
        logger.info("=== ë³‘ë ¬ í¬ë¡¤ë§ ì™„ë£Œ ===")
        logger.info(f"ì„±ê³µë¥ : {summary['success_rate']:.1f}% ({completed_tasks}/{len(tasks)})")
        logger.info(f"ì´ ì²˜ë¦¬: {total_stores_processed}ê°œ ê°€ê²Œ")
        logger.info(f"ì†Œìš”ì‹œê°„: {summary['total_time_minutes']:.1f}ë¶„")
        logger.info(f"ì²˜ë¦¬ì†ë„: {summary['stores_per_hour']:.0f}ê°œ/ì‹œê°„")
        
        return summary
    
    def retry_failed_tasks(self) -> Dict:
        """ì‹¤íŒ¨í•œ ì‘ì—… ì¬ì‹œë„"""
        if not self.failed_tasks:
            logger.info("ì¬ì‹œë„í•  ì‹¤íŒ¨ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        logger.info(f"ì‹¤íŒ¨í•œ {len(self.failed_tasks)}ê°œ ì‘ì—… ì¬ì‹œë„ ì‹œì‘")
        
        retry_results = []
        for task in self.failed_tasks:
            if task.retry_count < task.max_retries:
                task.retry_count += 1
                logger.info(f"{task.district_name} ì¬ì‹œë„ ({task.retry_count}/{task.max_retries})")
                
                try:
                    result = crawl_district_worker(task)
                    retry_results.append(result)
                    
                    if result.success:
                        logger.info(f"âœ… {task.district_name} ì¬ì‹œë„ ì„±ê³µ")
                    else:
                        logger.error(f"âŒ {task.district_name} ì¬ì‹œë„ ì‹¤íŒ¨")
                        
                except Exception as e:
                    logger.error(f"{task.district_name} ì¬ì‹œë„ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return {
            'retry_attempts': len(retry_results),
            'retry_successes': sum(1 for r in retry_results if r.success),
            'retry_results': retry_results
        }

def run_stage5_parallel_crawling():
    """5ë‹¨ê³„ ë³‘ë ¬ í¬ë¡¤ë§ ì‹¤í–‰"""
    logger.info("ğŸš€ 5ë‹¨ê³„ ë³‘ë ¬ í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì‹œì‘")
    
    # ìµœì  ì›Œì»¤ ìˆ˜ ê²°ì • (CPU ì½”ì–´ ìˆ˜ ê¸°ë°˜)
    cpu_count = mp.cpu_count()
    optimal_workers = min(8, max(4, cpu_count - 1))  # ìµœì†Œ 4ê°œ, ìµœëŒ€ 8ê°œ
    
    logger.info(f"ì‹œìŠ¤í…œ ì •ë³´: CPU {cpu_count}ì½”ì–´, ì›Œì»¤ {optimal_workers}ê°œ ì‚¬ìš©")
    
    # ë³‘ë ¬ í¬ë¡¤ë§ ë§¤ë‹ˆì € ìƒì„±
    manager = ParallelCrawlingManager(max_workers=optimal_workers)
    
    try:
        # ë©”ì¸ í¬ë¡¤ë§ ì‹¤í–‰
        summary = manager.run_parallel_crawling()
        
        # ì‹¤íŒ¨í•œ ì‘ì—… ì¬ì‹œë„
        if manager.failed_tasks:
            retry_summary = manager.retry_failed_tasks()
            summary.update(retry_summary)
        
        # ìµœì¢… ì„±ê³¼ ë¦¬í¬íŠ¸
        logger.info("ğŸ¯ 5ë‹¨ê³„ ë³‘ë ¬ í¬ë¡¤ë§ ì„±ê³¼ ë¦¬í¬íŠ¸")
        logger.info(f"â€¢ ì²˜ë¦¬ ì†ë„: {summary['stores_per_hour']:.0f}ê°œ/ì‹œê°„ (ëª©í‘œ: 500-1,000ê°œ/ì‹œê°„)")
        logger.info(f"â€¢ ì´ ì†Œìš”ì‹œê°„: {summary['total_time_minutes']:.1f}ë¶„")
        logger.info(f"â€¢ ì„±ê³µë¥ : {summary['success_rate']:.1f}%")
        logger.info(f"â€¢ ì´ ì²˜ë¦¬ ê°€ê²Œ: {summary['total_stores_processed']}ê°œ")
        
        # ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€ í™•ì¸
        if summary['stores_per_hour'] >= 500:
            logger.info("âœ… 5ë‹¨ê³„ ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„±!")
        else:
            logger.warning("âš ï¸ ì„±ëŠ¥ ëª©í‘œ ë¯¸ë‹¬ì„±. ì¶”ê°€ ìµœì í™” í•„ìš”")
        
        return summary
        
    except KeyboardInterrupt:
        logger.info("ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        return {}
    except Exception as e:
        logger.error(f"ë³‘ë ¬ í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return {}

if __name__ == "__main__":
    # ë©€í‹°í”„ë¡œì„¸ì‹± ì‹œì‘ì  ì„¤ì • (Windows í˜¸í™˜ì„±)
    mp.set_start_method('spawn', force=True)
    
    # ë³‘ë ¬ í¬ë¡¤ë§ ì‹¤í–‰
    result = run_stage5_parallel_crawling()
    
    if result:
        print(f"\nğŸ‰ ë³‘ë ¬ í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"ì²˜ë¦¬ ì†ë„: {result.get('stores_per_hour', 0):.0f}ê°œ/ì‹œê°„")
        print(f"ì´ ì†Œìš”ì‹œê°„: {result.get('total_time_minutes', 0):.1f}ë¶„") 