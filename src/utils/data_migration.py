"""
í¬ë¡¤ë§ ë°ì´í„°ë¥¼ í”„ë¡œì íŠ¸ DBë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""
import psycopg2
import psycopg2.extras
import logging
from typing import List, Dict, Optional, Union
import json
import re
import os
from urllib.parse import urlparse
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataMigration:
    def __init__(self, crawler_db_config: Union[Dict, str] = None, project_db_config: Union[Dict, str] = None):
        """
        Args:
            crawler_db_config: í¬ë¡¤ëŸ¬ DB ì—°ê²° ì •ë³´ (dict ë˜ëŠ” DATABASE_URL ë¬¸ìì—´)
            project_db_config: í”„ë¡œì íŠ¸ DB ì—°ê²° ì •ë³´ (dict ë˜ëŠ” DATABASE_URL ë¬¸ìì—´)
        """
        # í¬ë¡¤ëŸ¬ DB ì—°ê²° ì„¤ì • (ê¸°ì¡´ DATABASE_URL ì‚¬ìš©)
        if crawler_db_config is None:
            crawler_db_config = os.getenv('DATABASE_URL', 
                                         'postgresql://postgres:12345@localhost:5432/refill_spot_crawler')
        
        # í”„ë¡œì íŠ¸ DB ì—°ê²° ì„¤ì • (ìƒˆë¡œìš´ PROJECT_DATABASE_URL ì‚¬ìš©)
        if project_db_config is None:
            project_db_config = os.getenv('PROJECT_DATABASE_URL',
                                         'postgresql://postgres:your_password@localhost:5432/refill_spot')
        
        # ì—°ê²° ì •ë³´ íŒŒì‹± ë° ì—°ê²°
        self.crawler_conn = self._create_connection(crawler_db_config, "í¬ë¡¤ëŸ¬ DB")
        self.project_conn = self._create_connection(project_db_config, "í”„ë¡œì íŠ¸ DB")
    
    def _create_connection(self, db_config: Union[Dict, str], db_name: str):
        """DB ì—°ê²° ìƒì„±"""
        try:
            if isinstance(db_config, str):
                # DATABASE_URL í˜•ì‹ íŒŒì‹±
                parsed = urlparse(db_config)
                config = {
                    'host': parsed.hostname,
                    'port': parsed.port or 5432,
                    'database': parsed.path[1:] if parsed.path else 'postgres',
                    'user': parsed.username,
                    'password': parsed.password
                }
                logger.info(f"{db_name} ì—°ê²°: {config['user']}@{config['host']}:{config['port']}/{config['database']}")
            else:
                # Dictionary í˜•ì‹
                config = db_config
                logger.info(f"{db_name} ì—°ê²°: {config.get('user')}@{config.get('host')}:{config.get('port')}/{config.get('database')}")
            
            return psycopg2.connect(**config)
            
        except Exception as e:
            logger.error(f"{db_name} ì—°ê²° ì‹¤íŒ¨: {e}")
            raise
        
    def migrate_stores(self, limit: Optional[int] = None):
        """í¬ë¡¤ëŸ¬ DBì—ì„œ í”„ë¡œì íŠ¸ DBë¡œ ê°€ê²Œ ì •ë³´ ë§ˆì´ê·¸ë ˆì´ì…˜"""
        try:
            # 1. í¬ë¡¤ëŸ¬ DBì—ì„œ ë°ì´í„° ì¡°íšŒ
            crawler_cursor = self.crawler_conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
            
            query = """
                SELECT 
                    s.*,
                    array_agg(DISTINCT c.name) as category_names
                FROM stores s
                LEFT JOIN store_categories sc ON s.id = sc.store_id
                LEFT JOIN categories c ON sc.category_id = c.id
                WHERE s.status = 'ìš´ì˜ì¤‘'
                AND s.position_lat IS NOT NULL 
                AND s.position_lng IS NOT NULL
                AND s.position_lat != 0
                AND s.position_lng != 0
                GROUP BY s.id
                ORDER BY s.created_at DESC
            """
            
            if limit:
                query += f" LIMIT {limit}"
                
            crawler_cursor.execute(query)
            stores = crawler_cursor.fetchall()
            
            logger.info(f"í¬ë¡¤ëŸ¬ DBì—ì„œ {len(stores)}ê°œ ê°€ê²Œ ì¡°íšŒ ì™„ë£Œ")
            
            # 2. ë°ì´í„° ê°€ê³µ ë° í”„ë¡œì íŠ¸ DBì— ì‚½ì…
            project_cursor = self.project_conn.cursor()
            
            success_count = 0
            for store in stores:
                try:
                    # ë°ì´í„° ê°€ê³µ
                    processed_data = self._process_store_data(store)
                    
                    # í”„ë¡œì íŠ¸ DBì— ì‚½ì…
                    self._insert_to_project_db(project_cursor, processed_data)
                    success_count += 1
                    
                except Exception as e:
                    logger.error(f"ê°€ê²Œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨ ({store['name']}): {e}")
                    continue
            
            self.project_conn.commit()
            logger.info(f"ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ: {success_count}/{len(stores)}ê°œ ì„±ê³µ")
            
        except Exception as e:
            logger.error(f"ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {e}")
            self.project_conn.rollback()
            raise
            
        finally:
            crawler_cursor.close()
            project_cursor.close()
    
    def _process_store_data(self, store: Dict) -> Dict:
        """í¬ë¡¤ëŸ¬ ë°ì´í„°ë¥¼ í”„ë¡œì íŠ¸ DB í˜•ì‹ìœ¼ë¡œ ê°€ê³µ"""
        
        # 1. ê°€ê²© ì •ë³´ ê°€ê³µ
        price = self._process_price(store)
        
        # 2. ë¬´í•œë¦¬í•„ ì•„ì´í…œ ê°€ê³µ
        refill_items = self._process_refill_items(store)
        
        # 3. ì´ë¯¸ì§€ URL ê²€ì¦ ë° í•„í„°ë§
        image_urls = self._process_image_urls(store)
        
        # 4. ì˜ì—…ì‹œê°„ ì •ë³´ ì •ë¦¬
        open_hours = self._process_open_hours(store)
        
        # 5. í‰ì  ì •ë³´ í†µí•©
        naver_rating = store.get('naver_rating')
        kakao_rating = store.get('kakao_rating')
        diningcode_rating = store.get('diningcode_rating')
        
        # 6. ì¹´í…Œê³ ë¦¬ ë§¤í•‘
        category_names = store.get('category_names') or []
        # None ê°’ì´ë‚˜ [None] ë°°ì—´ ì²˜ë¦¬
        if category_names and category_names[0] is None:
            category_names = []
        categories = self._map_categories(category_names)
        
        # ì¢Œí‘œ ê²€ì¦ ë° ë³€í™˜
        try:
            position_lat = float(store['position_lat']) if store.get('position_lat') else None
            position_lng = float(store['position_lng']) if store.get('position_lng') else None
            
            if position_lat is None or position_lng is None:
                raise ValueError("ì¢Œí‘œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤")
                
            # position_x, position_yê°€ Noneì¸ ê²½ìš° lat/lng ê°’ì„ ì‚¬ìš©
            position_x = float(store['position_x']) if store.get('position_x') is not None else position_lng
            position_y = float(store['position_y']) if store.get('position_y') is not None else position_lat
            
        except (ValueError, TypeError) as e:
            raise ValueError(f"ì¢Œí‘œ ë³€í™˜ ì‹¤íŒ¨: {e}")
        
        return {
            'name': store['name'],
            'address': store.get('address', ''),
            'description': self._generate_description(store),
            'position_lat': position_lat,
            'position_lng': position_lng,
            'position_x': position_x,
            'position_y': position_y,
            'naver_rating': float(naver_rating) if naver_rating else None,
            'kakao_rating': float(kakao_rating) if kakao_rating else None,
            'open_hours': open_hours,
            'price': price,
            'refill_items': refill_items,
            'image_urls': image_urls,
            'categories': categories
        }
    
    def _process_price(self, store: Dict) -> Optional[str]:
        """ê°€ê²© ì •ë³´ ê°€ê³µ"""
        # ìš°ì„ ìˆœìœ„: price > average_price > price_range
        if store.get('price'):
            return str(store['price'])
        elif store.get('average_price'):
            return store['average_price']
        elif store.get('price_range'):
            return store['price_range']
        elif store.get('price_details'):
            # price_details ë°°ì—´ì—ì„œ ì²« ë²ˆì§¸ ê°€ê²© ì •ë³´ ì¶”ì¶œ
            details = store['price_details']
            if isinstance(details, list) and len(details) > 0:
                return details[0]
        return None
    
    def _process_refill_items(self, store: Dict) -> List[str]:
        """ë¬´í•œë¦¬í•„ ì•„ì´í…œ ê°€ê³µ"""
        items = []
        
        # refill_items í•„ë“œ
        if store.get('refill_items'):
            items.extend(store['refill_items'])
        
        # refill_typeì—ì„œ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
        if store.get('refill_type'):
            refill_type = store['refill_type']
            # "ê³ ê¸° ë¬´í•œë¦¬í•„", "ì‚¼ê²¹ì‚´ ë¬´í•œë¦¬í•„" ë“±ì—ì„œ ì•„ì´í…œ ì¶”ì¶œ
            match = re.search(r'(.+?)\s*ë¬´í•œë¦¬í•„', refill_type)
            if match:
                item = match.group(1).strip()
                if item and item not in items:
                    items.append(item)
        
        # ë©”ë‰´ì—ì„œ ë¬´í•œë¦¬í•„ ê´€ë ¨ ì•„ì´í…œ ì¶”ì¶œ
        if store.get('menu_items'):
            try:
                menu_items = store['menu_items']
                if isinstance(menu_items, str):
                    menu_items = json.loads(menu_items)
                
                for item in menu_items:
                    if isinstance(item, dict):
                        name = item.get('name', '')
                        if 'ë¬´í•œ' in name or 'ë¦¬í•„' in name:
                            items.append(name)
            except:
                pass
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
        return list(set(filter(None, items)))[:10]  # ìµœëŒ€ 10ê°œê¹Œì§€
    
    def _process_image_urls(self, store: Dict) -> List[str]:
        """ì´ë¯¸ì§€ URL ê²€ì¦ ë° í•„í„°ë§"""
        urls = []
        
        # main_image
        if store.get('main_image'):
            urls.append(store['main_image'])
        
        # image_urls
        if store.get('image_urls'):
            urls.extend(store['image_urls'])
        
        # menu_images
        if store.get('menu_images'):
            urls.extend(store['menu_images'])
        
        # interior_images (ìµœëŒ€ 2ê°œë§Œ)
        if store.get('interior_images'):
            urls.extend(store['interior_images'][:2])
        
        # URL ê²€ì¦ ë° ì¤‘ë³µ ì œê±°
        valid_urls = []
        seen = set()
        
        for url in urls:
            if url and url not in seen:
                # ê¸°ë³¸ì ì¸ URL ê²€ì¦
                if url.startswith(('http://', 'https://', '//')):
                    valid_urls.append(url)
                    seen.add(url)
        
        return valid_urls[:5]  # ìµœëŒ€ 5ê°œê¹Œì§€
    
    def _process_open_hours(self, store: Dict) -> Optional[str]:
        """ì˜ì—…ì‹œê°„ ì •ë³´ ì •ë¦¬"""
        # ìš°ì„ ìˆœìœ„: open_hours > open_hours_raw
        open_hours = store.get('open_hours') or store.get('open_hours_raw')
        
        if not open_hours:
            return None
        
        # ì¶”ê°€ ì •ë³´ í¬í•¨ (ë¼ìŠ¤íŠ¸ì˜¤ë”ëŠ” í¬ë¡¤ë§ ë‹¨ê³„ì—ì„œ ì´ë¯¸ í¬í•¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì œì™¸)
        parts = [open_hours]
        
        if store.get('break_time'):
            parts.append(f"ë¸Œë ˆì´í¬íƒ€ì„: {store['break_time']}")
        
        # ë¼ìŠ¤íŠ¸ì˜¤ë”ëŠ” ì´ë¯¸ open_hoursì— í¬í•¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì¤‘ë³µ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
        # if store.get('last_order'):
        #     parts.append(f"ë¼ìŠ¤íŠ¸ì˜¤ë”: {store['last_order']}")
        
        if store.get('holiday'):
            parts.append(f"íœ´ë¬´: {store['holiday']}")
        
        return ' / '.join(parts)
    
    def _generate_description(self, store: Dict) -> str:
        """ê°€ê²Œ ì„¤ëª… ìƒì„±"""
        parts = []
        
        # ê¸°ë³¸ ì„¤ëª…
        if store.get('description'):
            parts.append(store['description'])
        
        # ë¬´í•œë¦¬í•„ ì •ë³´
        if store.get('refill_type'):
            parts.append(f"{store['refill_type']} ì „ë¬¸ì ")
        
        if store.get('refill_conditions'):
            parts.append(f"ë¦¬í•„ ì¡°ê±´: {store['refill_conditions']}")
        
        # ë¶„ìœ„ê¸°
        if store.get('atmosphere'):
            parts.append(f"ë¶„ìœ„ê¸°: {store['atmosphere']}")
        
        # ë¦¬ë·° ìš”ì•½
        if store.get('review_summary'):
            parts.append(store['review_summary'])
        
        # í‚¤ì›Œë“œ
        if store.get('keywords'):
            keywords = store['keywords'][:5]  # ìµœëŒ€ 5ê°œ
            parts.append(f"í‚¤ì›Œë“œ: {', '.join(keywords)}")
        
        return ' | '.join(parts)[:500]  # ìµœëŒ€ 500ì
    
    def _map_categories(self, crawler_categories: List[str]) -> List[str]:
        """í¬ë¡¤ëŸ¬ ì¹´í…Œê³ ë¦¬ë¥¼ í”„ë¡œì íŠ¸ ì¹´í…Œê³ ë¦¬ë¡œ ë§¤í•‘"""
        # None ê°’ ì²˜ë¦¬
        if not crawler_categories:
            return ['ë¬´í•œë¦¬í•„']  # ê¸°ë³¸ ì¹´í…Œê³ ë¦¬
        
        # ì¹´í…Œê³ ë¦¬ ë§¤í•‘ í…Œì´ë¸”
        category_mapping = {
            'ë¬´í•œë¦¬í•„': 'ë¬´í•œë¦¬í•„',
            'ê³ ê¸°ë¬´í•œë¦¬í•„': 'ê³ ê¸°',
            'ì†Œê³ ê¸°ë¬´í•œë¦¬í•„': 'ê³ ê¸°',
            'ì‚¼ê²¹ì‚´ë¬´í•œë¦¬í•„': 'ê³ ê¸°',
            'ì‚¼ê²¹ì‚´': 'ê³ ê¸°',
            'ê°ˆë¹„': 'ê³ ê¸°',
            'ë·”í˜': 'ë·”í˜',
            'ì´ˆë°¥ë·”í˜': 'ì¼ì‹',
            'ì´ˆë°¥': 'ì¼ì‹',
            'í•´ì‚°ë¬¼ë¬´í•œë¦¬í•„': 'í•´ì‚°ë¬¼',
            'í•´ì‚°ë¬¼': 'í•´ì‚°ë¬¼',
            'í•œì‹': 'í•œì‹',
            'ì¼ì‹': 'ì¼ì‹',
            'ì¤‘ì‹': 'ì¤‘ì‹',
            'ì–‘ì‹': 'ì–‘ì‹',
            'í”¼ì': 'í”¼ì',
            'ì¹˜í‚¨': 'ì¹˜í‚¨',
            'ì¡±ë°œ': 'ì¡±ë°œ',
            'ê³±ì°½': 'ê³±ì°½',
            'ìŠ¤í…Œì´í¬': 'ìŠ¤í…Œì´í¬',
            'íŒŒìŠ¤íƒ€': 'íŒŒìŠ¤íƒ€',
            'ë””ì €íŠ¸': 'ë””ì €íŠ¸',
            'ì¹´í˜': 'ì¹´í˜',
            'ë¸ŒëŸ°ì¹˜': 'ë¸ŒëŸ°ì¹˜',
            'ìƒëŸ¬ë“œ': 'ìƒëŸ¬ë“œ',
            'ë¶„ì‹': 'ë¶„ì‹',
            'ì°œë‹­': 'ì°œë‹­',
            'ë²„ê±°': 'ë²„ê±°'
        }
        
        mapped_categories = set()
        
        for cat in crawler_categories:
            if cat and cat is not None:  # None ì²´í¬ ì¶”ê°€
                # ì •í™•í•œ ë§¤ì¹­
                if cat in category_mapping:
                    mapped_categories.add(category_mapping[cat])
                # ë¶€ë¶„ ë§¤ì¹­
                else:
                    for key, value in category_mapping.items():
                        if key in cat or cat in key:
                            mapped_categories.add(value)
                            break
        
        # ë¬´í•œë¦¬í•„ ê´€ë ¨ ì¹´í…Œê³ ë¦¬ê°€ ìˆìœ¼ë©´ 'ë¬´í•œë¦¬í•„' ì¶”ê°€
        if any('ë¬´í•œ' in str(cat) or 'ë¦¬í•„' in str(cat) for cat in crawler_categories if cat):
            mapped_categories.add('ë¬´í•œë¦¬í•„')
        
        # ì¹´í…Œê³ ë¦¬ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ ì¶”ê°€
        if not mapped_categories:
            mapped_categories.add('ë¬´í•œë¦¬í•„')
        
        return list(mapped_categories)
    
    def _insert_to_project_db(self, cursor, data: Dict):
        """í”„ë¡œì íŠ¸ DBì— ë°ì´í„° ì‚½ì…"""
        # 1. ì¹´í…Œê³ ë¦¬ í™•ì¸ ë° ìƒì„±
        category_ids = []
        for category_name in data['categories']:
            cursor.execute(
                "INSERT INTO categories (name) VALUES (%s) ON CONFLICT (name) DO NOTHING",
                (category_name,)
            )
            cursor.execute("SELECT id FROM categories WHERE name = %s", (category_name,))
            result = cursor.fetchone()
            if result:
                category_ids.append(result[0])
        
        # 2. ê°€ê²Œ ì •ë³´ ì‚½ì…
        cursor.execute("""
            INSERT INTO stores (
                name, address, description, 
                position_lat, position_lng, position_x, position_y,
                naver_rating, kakao_rating, open_hours, 
                price, refill_items, image_urls, geom
            ) VALUES (
                %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,
                ST_SetSRID(ST_MakePoint(%s, %s), 4326)
            ) RETURNING id
        """, (
            data['name'], data['address'], data['description'],
            data['position_lat'], data['position_lng'], 
            data['position_x'], data['position_y'],
            data['naver_rating'], data['kakao_rating'], data['open_hours'],
            data['price'], data['refill_items'], data['image_urls'],
            data['position_lng'], data['position_lat']  # geom í•„ë“œë¥¼ ìœ„í•œ ì¢Œí‘œ
        ))
        
        store_id = cursor.fetchone()[0]
        
        # 3. ê°€ê²Œ-ì¹´í…Œê³ ë¦¬ ì—°ê²°
        for category_id in category_ids:
            cursor.execute(
                "INSERT INTO store_categories (store_id, category_id) VALUES (%s, %s)",
                (store_id, category_id)
            )
        
        logger.info(f"ê°€ê²Œ ì‚½ì… ì™„ë£Œ: {data['name']} (ID: {store_id})")
    
    def close(self):
        """ì—°ê²° ì¢…ë£Œ"""
        self.crawler_conn.close()
        self.project_conn.close()


def create_migration_from_env():
    """í™˜ê²½ë³€ìˆ˜ì—ì„œ ë§ˆì´ê·¸ë ˆì´ì…˜ ê°ì²´ ìƒì„±"""
    return DataMigration()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='í¬ë¡¤ë§ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜')
    parser.add_argument('--limit', type=int, help='ë§ˆì´ê·¸ë ˆì´ì…˜í•  ê°€ê²Œ ìˆ˜ ì œí•œ')
    parser.add_argument('--test', action='store_true', help='í…ŒìŠ¤íŠ¸ ëª¨ë“œ (10ê°œë§Œ ë§ˆì´ê·¸ë ˆì´ì…˜)')
    parser.add_argument('--crawler-db', help='í¬ë¡¤ëŸ¬ DB URL (ê¸°ë³¸ê°’: í™˜ê²½ë³€ìˆ˜ DATABASE_URL)')
    parser.add_argument('--project-db', help='í”„ë¡œì íŠ¸ DB URL (ê¸°ë³¸ê°’: í™˜ê²½ë³€ìˆ˜ PROJECT_DATABASE_URL)')
    
    args = parser.parse_args()
    
    # ë§ˆì´ê·¸ë ˆì´ì…˜ ê°ì²´ ìƒì„±
    migration = DataMigration(
        crawler_db_config=args.crawler_db,
        project_db_config=args.project_db
    )
    
    try:
        if args.test:
            logger.info("ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: 10ê°œ ê°€ê²Œë§Œ ë§ˆì´ê·¸ë ˆì´ì…˜")
            migration.migrate_stores(limit=10)
        elif args.limit:
            logger.info(f"ğŸ“Š ì œí•œ ëª¨ë“œ: {args.limit}ê°œ ê°€ê²Œ ë§ˆì´ê·¸ë ˆì´ì…˜")
            migration.migrate_stores(limit=args.limit)
        else:
            logger.info("ğŸš€ ì „ì²´ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘")
            migration.migrate_stores()
            
    except Exception as e:
        logger.error(f"âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {e}")
        raise
    finally:
        migration.close()

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    main() 