"""
í¬ë¡¤ë§ ë°ì´í„°ë¥¼ í”„ë¡œì íŠ¸ DBë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""
import psycopg2
import psycopg2.extras
import logging
from typing import List, Dict, Optional, Union
import json
import re
import os
from urllib.parse import urlparse
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataMigration:
    def __init__(self, crawler_db_config: Union[Dict, str] = None, project_db_config: Union[Dict, str] = None):
        """
        Args:
            crawler_db_config: í¬ë¡¤ëŸ¬ DB ì—°ê²° ì •ë³´ (dict ë˜ëŠ” DATABASE_URL ë¬¸ìì—´)
            project_db_config: í”„ë¡œì íŠ¸ DB ì—°ê²° ì •ë³´ (dict ë˜ëŠ” DATABASE_URL ë¬¸ìì—´)
        """
        # í¬ë¡¤ëŸ¬ DB ì—°ê²° ì„¤ì • (ê¸°ì¡´ DATABASE_URL ì‚¬ìš©)
        if crawler_db_config is None:
            crawler_db_config = os.getenv('DATABASE_URL', 
                                         'postgresql://postgres:12345@localhost:5432/refill_spot_crawler')
        
        # í”„ë¡œì íŠ¸ DB ì—°ê²° ì„¤ì • (ìƒˆë¡œìš´ PROJECT_DATABASE_URL ì‚¬ìš©)
        if project_db_config is None:
            project_db_config = os.getenv('PROJECT_DATABASE_URL',
                                         'postgresql://postgres:your_password@localhost:5432/refill_spot')
        
        # ì—°ê²° ì •ë³´ íŒŒì‹± ë° ì—°ê²°
        self.crawler_conn = self._create_connection(crawler_db_config, "í¬ë¡¤ëŸ¬ DB")
        self.project_conn = self._create_connection(project_db_config, "í”„ë¡œì íŠ¸ DB")
    
    def _create_connection(self, db_config: Union[Dict, str], db_name: str):
        """DB ì—°ê²° ìƒì„±"""
        try:
            if isinstance(db_config, str):
                # DATABASE_URL í˜•ì‹ íŒŒì‹±
                parsed = urlparse(db_config)
                config = {
                    'host': parsed.hostname,
                    'port': parsed.port or 5432,
                    'database': parsed.path[1:] if parsed.path else 'postgres',
                    'user': parsed.username,
                    'password': parsed.password
                }
                logger.info(f"{db_name} ì—°ê²°: {config['user']}@{config['host']}:{config['port']}/{config['database']}")
            else:
                # Dictionary í˜•ì‹
                config = db_config
                logger.info(f"{db_name} ì—°ê²°: {config.get('user')}@{config.get('host')}:{config.get('port')}/{config.get('database')}")
            
            return psycopg2.connect(**config)
            
        except Exception as e:
            logger.error(f"{db_name} ì—°ê²° ì‹¤íŒ¨: {e}")
            raise
        
    def migrate_stores(self, limit: Optional[int] = None):
        """í¬ë¡¤ëŸ¬ DBì—ì„œ í”„ë¡œì íŠ¸ DBë¡œ ê°€ê²Œ ì •ë³´ ë§ˆì´ê·¸ë ˆì´ì…˜"""
        try:
            # 1. í¬ë¡¤ëŸ¬ DBì—ì„œ ë°ì´í„° ì¡°íšŒ
            crawler_cursor = self.crawler_conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
            
            query = """
                SELECT 
                    s.*,
                    array_agg(DISTINCT c.name) as category_names
                FROM stores s
                LEFT JOIN store_categories sc ON s.id = sc.store_id
                LEFT JOIN categories c ON sc.category_id = c.id
                WHERE s.status = 'ìš´ì˜ì¤‘'
                AND s.position_lat IS NOT NULL 
                AND s.position_lng IS NOT NULL
                AND s.position_lat != 0
                AND s.position_lng != 0
                GROUP BY s.id
                ORDER BY s.created_at DESC
            """
            
            if limit:
                query += f" LIMIT {limit}"
                
            crawler_cursor.execute(query)
            stores = crawler_cursor.fetchall()
            
            logger.info(f"í¬ë¡¤ëŸ¬ DBì—ì„œ {len(stores)}ê°œ ê°€ê²Œ ì¡°íšŒ ì™„ë£Œ")
            
            # 2. ë°ì´í„° ê°€ê³µ ë° í”„ë¡œì íŠ¸ DBì— ì‚½ì…
            project_cursor = self.project_conn.cursor()
            
            success_count = 0
            for store in stores:
                try:
                    # ë°ì´í„° ê°€ê³µ
                    processed_data = self._process_store_data(store)
                    
                    # í”„ë¡œì íŠ¸ DBì— ì‚½ì… (ê°œë³„ íŠ¸ëœì­ì…˜)
                    self._insert_to_project_db(project_cursor, processed_data)
                    self.project_conn.commit()  # ê° ê°€ê²Œë§ˆë‹¤ ì»¤ë°‹
                    success_count += 1
                    
                except Exception as e:
                    logger.error(f"ê°€ê²Œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨ ({store['name']}): {e}")
                    self.project_conn.rollback()  # ì‹¤íŒ¨ ì‹œ ë¡¤ë°±
                    continue
            logger.info(f"ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ: {success_count}/{len(stores)}ê°œ ì„±ê³µ")
            
        except Exception as e:
            logger.error(f"ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {e}")
            self.project_conn.rollback()
            raise
            
        finally:
            crawler_cursor.close()
            project_cursor.close()
    
    def _process_store_data(self, store: Dict) -> Dict:
        """í¬ë¡¤ëŸ¬ ë°ì´í„°ë¥¼ í”„ë¡œì íŠ¸ DB í˜•ì‹ìœ¼ë¡œ ê°€ê³µ"""
        
        # 1. ê°€ê²© ì •ë³´ ê°€ê³µ
        price = self._process_price(store)
        
        # 2. ë¬´í•œë¦¬í•„ ì•„ì´í…œ ê°€ê³µ
        refill_items = self._process_refill_items(store)
        
        # 3. ì´ë¯¸ì§€ URL ê²€ì¦ ë° í•„í„°ë§
        image_urls = self._process_image_urls(store)
        
        # 4. ì˜ì—…ì‹œê°„ ì •ë³´ ì •ë¦¬
        open_hours = self._process_open_hours(store)
        
        # 5. í‰ì  ì •ë³´ í†µí•©
        naver_rating = store.get('naver_rating')
        kakao_rating = store.get('kakao_rating')
        diningcode_rating = store.get('diningcode_rating')
        
        # 6. ì¹´í…Œê³ ë¦¬ ë§¤í•‘
        category_names = store.get('category_names') or []
        # None ê°’ì´ë‚˜ [None] ë°°ì—´ ì²˜ë¦¬
        if category_names and category_names[0] is None:
            category_names = []
        categories = self._map_categories(category_names)
        
        # ì¢Œí‘œ ê²€ì¦ ë° ë³€í™˜
        try:
            position_lat = float(store['position_lat']) if store.get('position_lat') else None
            position_lng = float(store['position_lng']) if store.get('position_lng') else None
            
            if position_lat is None or position_lng is None:
                raise ValueError("ì¢Œí‘œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤")
                
            # position_x, position_yê°€ Noneì¸ ê²½ìš° lat/lng ê°’ì„ ì‚¬ìš©
            position_x = float(store['position_x']) if store.get('position_x') is not None else position_lng
            position_y = float(store['position_y']) if store.get('position_y') is not None else position_lat
            
        except (ValueError, TypeError) as e:
            raise ValueError(f"ì¢Œí‘œ ë³€í™˜ ì‹¤íŒ¨: {e}")
        
        return {
            'name': store['name'],
            'address': store.get('address', ''),
            'position_lat': position_lat,
            'position_lng': position_lng,
            'position_x': position_x,
            'position_y': position_y,
            'open_hours': open_hours,
            'break_time': store.get('break_time'),
            'refill_items': refill_items,
            'image_urls': image_urls,
            'phone_number': store.get('phone_number'),
            'categories': categories
        }
    
    def _process_price(self, store: Dict) -> Optional[str]:
        """ê°€ê²© ì •ë³´ ê°€ê³µ (price í•„ë“œë§Œ ì‚¬ìš©)"""
        if store.get('price'):
            return str(store['price'])
        elif store.get('price_details'):
            # price_details ë°°ì—´ì—ì„œ ì²« ë²ˆì§¸ ê°€ê²© ì •ë³´ ì¶”ì¶œ
            details = store['price_details']
            if isinstance(details, list) and len(details) > 0:
                return details[0]
        return None
    
    def _process_refill_items(self, store: Dict) -> List[Dict]:
        """ë¬´í•œë¦¬í•„ ì•„ì´í…œ ê°€ê³µ - JSONB í˜•íƒœë¡œ ë³€ê²½ (menu_itemsë¥¼ ê·¸ëŒ€ë¡œ ë³µì‚¬)"""
        items = []
        
        # 1. menu_itemsì—ì„œ ê·¸ëŒ€ë¡œ ë³µì‚¬ (ë©”ì¸ ì†ŒìŠ¤)
        if store.get('menu_items'):
            try:
                menu_items = store['menu_items']
                if isinstance(menu_items, str):
                    menu_items = json.loads(menu_items)
                
                if isinstance(menu_items, list):
                    # menu_itemsë¥¼ ê·¸ëŒ€ë¡œ ë³µì‚¬
                    items = menu_items.copy()
                    logger.info(f"menu_itemsì—ì„œ {len(items)}ê°œ ì•„ì´í…œ ë³µì‚¬")
                    
            except Exception as e:
                logger.warning(f"menu_items íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        # 2. menu_itemsê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ refill_items í•„ë“œì—ì„œ ì¶”ì¶œ
        if not items and store.get('refill_items'):
            refill_items = store['refill_items']
            if isinstance(refill_items, list):
                for item in refill_items:
                    if isinstance(item, str) and item.strip():
                        items.append({
                            'name': item.strip(),
                            'price': '',
                            'price_numeric': 0,
                            'is_recommended': False,
                            'description': '',
                            'type': 'refill_item',
                            'source': 'crawler'
                        })
            elif isinstance(refill_items, str) and refill_items.strip():
                items.append({
                    'name': refill_items.strip(),
                    'price': '',
                    'price_numeric': 0,
                    'is_recommended': False,
                    'description': '',
                    'type': 'refill_item',
                    'source': 'crawler'
                })
        
        # 3. refill_typeì—ì„œ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ (ë³´ì¡°)
        if store.get('refill_type') and not items:
            refill_type = store['refill_type']
            match = re.search(r'(.+?)\s*ë¬´í•œë¦¬í•„', refill_type)
            if match:
                item_name = match.group(1).strip()
                if item_name:
                    items.append({
                        'name': item_name,
                        'price': '',
                        'price_numeric': 0,
                        'is_recommended': True,
                        'description': f"{refill_type}",
                        'type': 'refill_type',
                        'source': 'crawler'
                    })
        
        # 4. ë°ì´í„° ê²€ì¦ ë° ì •ë¦¬
        valid_items = []
        for item in items:
            if isinstance(item, dict) and item.get('name'):
                # í•„ìˆ˜ í•„ë“œ ë³´ì¥
                validated_item = {
                    'name': item.get('name', ''),
                    'price': item.get('price', ''),
                    'price_numeric': item.get('price_numeric', 0),
                    'is_recommended': item.get('is_recommended', False),
                    'type': item.get('type', 'menu_item'),
                    'order': item.get('order', 0)
                }
                
                # ì¶”ê°€ í•„ë“œê°€ ìˆìœ¼ë©´ í¬í•¨
                if 'description' in item:
                    validated_item['description'] = item['description']
                if 'source' in item:
                    validated_item['source'] = item['source']
                
                valid_items.append(validated_item)
        
        logger.info(f"ìµœì¢… {len(valid_items)}ê°œ refill_items ìƒì„±")
        return valid_items
    
    def _process_image_urls(self, store: Dict) -> List[str]:
        """ì´ë¯¸ì§€ URL ê²€ì¦ ë° í•„í„°ë§ (main_imageë§Œ ì‚¬ìš©)"""
        urls = []
        
        # main_image í•„ë“œë§Œ ì‚¬ìš©
        if store.get('main_image'):
            urls.append(store['main_image'])
        
        # URL ê²€ì¦
        valid_urls = []
        
        for url in urls:
            if url:
                # ë¡œì»¬ íŒŒì¼ ê²½ë¡œì´ê±°ë‚˜ ê¸°ë³¸ì ì¸ URL ê²€ì¦
                if (url.startswith(('data/', 'data\\', '/')) or 
                    url.startswith(('http://', 'https://', '//'))):
                    valid_urls.append(url)
        
        return valid_urls  # main_imageë§Œ ë°˜í™˜
    
    def _process_open_hours(self, store: Dict) -> Optional[str]:
        """ì˜ì—…ì‹œê°„ ì •ë³´ ì •ë¦¬"""
        # ìš°ì„ ìˆœìœ„: open_hours > open_hours_raw
        open_hours = store.get('open_hours') or store.get('open_hours_raw')
        
        if not open_hours:
            return None
        
        # ì¶”ê°€ ì •ë³´ í¬í•¨ (ë¼ìŠ¤íŠ¸ì˜¤ë”ëŠ” í¬ë¡¤ë§ ë‹¨ê³„ì—ì„œ ì´ë¯¸ í¬í•¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì œì™¸)
        parts = [open_hours]
        
        if store.get('break_time'):
            parts.append(f"ë¸Œë ˆì´í¬íƒ€ì„: {store['break_time']}")
        
        # ë¼ìŠ¤íŠ¸ì˜¤ë”ëŠ” ì´ë¯¸ open_hoursì— í¬í•¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì¤‘ë³µ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
        # if store.get('last_order'):
        #     parts.append(f"ë¼ìŠ¤íŠ¸ì˜¤ë”: {store['last_order']}")
        
        if store.get('holiday'):
            parts.append(f"íœ´ë¬´: {store['holiday']}")
        
        return ' / '.join(parts)
    
    def _generate_description(self, store: Dict) -> str:
        """ê°€ê²Œ ì„¤ëª… ìƒì„±"""
        parts = []
        
        # ê¸°ë³¸ ì„¤ëª…
        if store.get('description'):
            parts.append(store['description'])
        
        # ë¬´í•œë¦¬í•„ ì •ë³´
        if store.get('refill_type'):
            parts.append(f"{store['refill_type']} ì „ë¬¸ì ")
        
        if store.get('refill_conditions'):
            parts.append(f"ë¦¬í•„ ì¡°ê±´: {store['refill_conditions']}")
        
        # ë¶„ìœ„ê¸°
        if store.get('atmosphere'):
            parts.append(f"ë¶„ìœ„ê¸°: {store['atmosphere']}")
        
        # ë¦¬ë·° ìš”ì•½
        if store.get('review_summary'):
            parts.append(store['review_summary'])
        
        # í‚¤ì›Œë“œ
        if store.get('keywords'):
            keywords = store['keywords'][:5]  # ìµœëŒ€ 5ê°œ
            parts.append(f"í‚¤ì›Œë“œ: {', '.join(keywords)}")
        
        return ' | '.join(parts)[:500]  # ìµœëŒ€ 500ì
    
    def _map_categories(self, crawler_categories: List[str]) -> List[str]:
        """í¬ë¡¤ëŸ¬ ì¹´í…Œê³ ë¦¬ë¥¼ 7ê°œ í‘œì¤€ ì¹´í…Œê³ ë¦¬ë¡œ ë§¤í•‘"""
        # None ê°’ ì²˜ë¦¬
        if not crawler_categories:
            return ['í•œì‹']  # ê¸°ë³¸ ì¹´í…Œê³ ë¦¬
        
        # ì¹´í…Œê³ ë¦¬ ë§¤í•‘ í…Œì´ë¸” (7ê°œë¡œ ì œí•œ)
        category_mapping = {
            # ê³ ê¸°
            'ê³ ê¸°ë¬´í•œë¦¬í•„': 'ê³ ê¸°',
            'ì†Œê³ ê¸°ë¬´í•œë¦¬í•„': 'ê³ ê¸°',
            'ì‚¼ê²¹ì‚´ë¬´í•œë¦¬í•„': 'ê³ ê¸°',
            'ì‚¼ê²¹ì‚´': 'ê³ ê¸°',
            'ê°ˆë¹„': 'ê³ ê¸°',
            'ì†Œê³ ê¸°': 'ê³ ê¸°',
            'ë¼ì§€ê³ ê¸°': 'ê³ ê¸°',
            'ë‹­ê³ ê¸°': 'ê³ ê¸°',
            'ìŠ¤í…Œì´í¬': 'ê³ ê¸°',
            'ë°”ë² í': 'ê³ ê¸°',
            'BBQ': 'ê³ ê¸°',
            'êµ¬ì´': 'ê³ ê¸°',
            'ìœ¡ë¥˜': 'ê³ ê¸°',
            
            # í•´ì‚°ë¬¼
            'í•´ì‚°ë¬¼ë¬´í•œë¦¬í•„': 'í•´ì‚°ë¬¼',
            'í•´ì‚°ë¬¼': 'í•´ì‚°ë¬¼',
            'ì´ˆë°¥': 'í•´ì‚°ë¬¼',
            'ì´ˆë°¥ë·”í˜': 'í•´ì‚°ë¬¼',
            'íšŒ': 'í•´ì‚°ë¬¼',
            'ì‚¬ì‹œë¯¸': 'í•´ì‚°ë¬¼',
            'ìŠ¤ì‹œ': 'í•´ì‚°ë¬¼',
            'ìˆ˜ì‚°ë¬¼': 'í•´ì‚°ë¬¼',
            'ìƒì„ ': 'í•´ì‚°ë¬¼',
            
            # ì–‘ì‹
            'ì–‘ì‹': 'ì–‘ì‹',
            'ì„œì–‘ìŒì‹': 'ì–‘ì‹',
            'ì´íƒˆë¦¬ì•ˆ': 'ì–‘ì‹',
            'íŒŒìŠ¤íƒ€': 'ì–‘ì‹',
            'í”¼ì': 'ì–‘ì‹',
            'ë²„ê±°': 'ì–‘ì‹',
            'í–„ë²„ê±°': 'ì–‘ì‹',
            'ë¸ŒëŸ°ì¹˜': 'ì–‘ì‹',
            'ìƒëŸ¬ë“œ': 'ì–‘ì‹',
            
            # í•œì‹
            'í•œì‹': 'í•œì‹',
            'í•œêµ­ìŒì‹': 'í•œì‹',
            'ì¡±ë°œ': 'í•œì‹',
            'ë³´ìŒˆ': 'í•œì‹',
            'ê³±ì°½': 'í•œì‹',
            'ë§‰ì°½': 'í•œì‹',
            'ì¹˜í‚¨': 'í•œì‹',
            'ì°œë‹­': 'í•œì‹',
            'ë¶„ì‹': 'í•œì‹',
            'ë–¡ë³¶ì´': 'í•œì‹',
            'ëƒ‰ë©´': 'í•œì‹',
            'ë¶ˆê³ ê¸°': 'í•œì‹',
            
            # ì¤‘ì‹
            'ì¤‘ì‹': 'ì¤‘ì‹',
            'ì¤‘êµ­ìŒì‹': 'ì¤‘ì‹',
            'ì°¨ì´ë‹ˆì¦ˆ': 'ì¤‘ì‹',
            'ì§œì¥ë©´': 'ì¤‘ì‹',
            'ì§¬ë½•': 'ì¤‘ì‹',
            'íƒ•ìˆ˜ìœ¡': 'ì¤‘ì‹',
            
            # ì¼ì‹
            'ì¼ì‹': 'ì¼ì‹',
            'ì¼ë³¸ìŒì‹': 'ì¼ì‹',
            'ëˆê¹ŒìŠ¤': 'ì¼ì‹',
            'ìš°ë™': 'ì¼ì‹',
            'ë¼ë©˜': 'ì¼ì‹',
            'ì†Œë°”': 'ì¼ì‹',
            
            # ë””ì €íŠ¸
            'ë””ì €íŠ¸': 'ë””ì €íŠ¸',
            'ì¹´í˜': 'ë””ì €íŠ¸',
            'ì¼€ì´í¬': 'ë””ì €íŠ¸',
            'ì•„ì´ìŠ¤í¬ë¦¼': 'ë””ì €íŠ¸',
            'ë² ì´ì»¤ë¦¬': 'ë””ì €íŠ¸',
            'ë¹µ': 'ë””ì €íŠ¸',
            'ë¸ŒëŸ°ì¹˜': 'ë””ì €íŠ¸'
        }
        
        # í‘œì¤€ ì¹´í…Œê³ ë¦¬ (7ê°œ)
        standard_categories = ['ê³ ê¸°', 'í•´ì‚°ë¬¼', 'ì–‘ì‹', 'í•œì‹', 'ì¤‘ì‹', 'ì¼ì‹', 'ë””ì €íŠ¸']
        
        mapped_categories = set()
        
        for cat in crawler_categories:
            if cat and cat is not None:  # None ì²´í¬ ì¶”ê°€
                # ì •í™•í•œ ë§¤ì¹­
                if cat in category_mapping:
                    mapped_cat = category_mapping[cat]
                    if mapped_cat in standard_categories:
                        mapped_categories.add(mapped_cat)
                # ë¶€ë¶„ ë§¤ì¹­
                else:
                    for key, value in category_mapping.items():
                        if (key in cat or cat in key) and value in standard_categories:
                            mapped_categories.add(value)
                            break
        
        # ì¹´í…Œê³ ë¦¬ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ ì¶”ê°€
        if not mapped_categories:
            mapped_categories.add('í•œì‹')
        
        return list(mapped_categories)
    
    def _insert_to_project_db(self, cursor, data: Dict):
        """í”„ë¡œì íŠ¸ DBì— ë°ì´í„° ì‚½ì…"""
        # 1. ì¹´í…Œê³ ë¦¬ í™•ì¸ ë° ìƒì„±
        category_ids = []
        for category_name in data['categories']:
            cursor.execute(
                "INSERT INTO categories (name) VALUES (%s) ON CONFLICT (name) DO NOTHING",
                (category_name,)
            )
            cursor.execute("SELECT id FROM categories WHERE name = %s", (category_name,))
            result = cursor.fetchone()
            if result:
                category_ids.append(result[0])
        
        # 2. ê°€ê²Œ ì •ë³´ ì‚½ì… (refill_itemsë¥¼ JSONB ë°°ì—´ë¡œ ì²˜ë¦¬)
        # ê° refill_itemì„ ê°œë³„ JSONBë¡œ ë³€í™˜í•˜ì—¬ ë°°ì—´ë¡œ ë§Œë“¦
        refill_items_jsonb_array = [json.dumps(item, ensure_ascii=False) for item in data['refill_items']]
        
        cursor.execute("""
            INSERT INTO stores (
                name, address, 
                position_lat, position_lng, position_x, position_y,
                open_hours, break_time, refill_items, image_urls, phone_number, geom
            ) VALUES (
                %s, %s, %s, %s, %s, %s, %s, %s, %s::text[]::jsonb[], %s, %s,
                ST_SetSRID(ST_MakePoint(%s, %s), 4326)
            ) RETURNING id
        """, (
            data['name'], data['address'],
            data['position_lat'], data['position_lng'], 
            data['position_x'], data['position_y'],
            data['open_hours'], data['break_time'], refill_items_jsonb_array, data['image_urls'],
            data['phone_number'],
            data['position_lng'], data['position_lat']  # geom í•„ë“œë¥¼ ìœ„í•œ ì¢Œí‘œ
        ))
        
        store_id = cursor.fetchone()[0]
        
        # 3. ê°€ê²Œ-ì¹´í…Œê³ ë¦¬ ì—°ê²°
        for category_id in category_ids:
            cursor.execute(
                "INSERT INTO store_categories (store_id, category_id) VALUES (%s, %s)",
                (store_id, category_id)
            )
        
        logger.info(f"ê°€ê²Œ ì‚½ì… ì™„ë£Œ: {data['name']} (ID: {store_id})")
    
    def close(self):
        """ì—°ê²° ì¢…ë£Œ"""
        self.crawler_conn.close()
        self.project_conn.close()


def create_migration_from_env():
    """í™˜ê²½ë³€ìˆ˜ì—ì„œ ë§ˆì´ê·¸ë ˆì´ì…˜ ê°ì²´ ìƒì„±"""
    return DataMigration()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='í¬ë¡¤ë§ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜')
    parser.add_argument('--limit', type=int, help='ë§ˆì´ê·¸ë ˆì´ì…˜í•  ê°€ê²Œ ìˆ˜ ì œí•œ')
    parser.add_argument('--test', action='store_true', help='í…ŒìŠ¤íŠ¸ ëª¨ë“œ (10ê°œë§Œ ë§ˆì´ê·¸ë ˆì´ì…˜)')
    parser.add_argument('--crawler-db', help='í¬ë¡¤ëŸ¬ DB URL (ê¸°ë³¸ê°’: í™˜ê²½ë³€ìˆ˜ DATABASE_URL)')
    parser.add_argument('--project-db', help='í”„ë¡œì íŠ¸ DB URL (ê¸°ë³¸ê°’: í™˜ê²½ë³€ìˆ˜ PROJECT_DATABASE_URL)')
    
    args = parser.parse_args()
    
    # ë§ˆì´ê·¸ë ˆì´ì…˜ ê°ì²´ ìƒì„±
    migration = DataMigration(
        crawler_db_config=args.crawler_db,
        project_db_config=args.project_db
    )
    
    try:
        if args.test:
            logger.info("ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: 10ê°œ ê°€ê²Œë§Œ ë§ˆì´ê·¸ë ˆì´ì…˜")
            migration.migrate_stores(limit=10)
        elif args.limit:
            logger.info(f"ğŸ“Š ì œí•œ ëª¨ë“œ: {args.limit}ê°œ ê°€ê²Œ ë§ˆì´ê·¸ë ˆì´ì…˜")
            migration.migrate_stores(limit=args.limit)
        else:
            logger.info("ğŸš€ ì „ì²´ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘")
            migration.migrate_stores()
            
    except Exception as e:
        logger.error(f"âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {e}")
        raise
    finally:
        migration.close()

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    main() 