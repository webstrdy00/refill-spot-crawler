"""
ë¦¬í•„ìŠ¤íŒŸ í¬ë¡¤ëŸ¬ ë©”ì¸ ì‹¤í–‰ ëª¨ë“ˆ (ê°„ì†Œí™”ëœ ë²„ì „)
ì•ì„œ ê°œì„ í•œ ì£¼ì†Œ, ì˜ì—…ì‹œê°„, break_time, last_order ìˆ˜ì§‘ ê¸°ëŠ¥ ì ìš©
"""

import sys
import os
import logging
from datetime import datetime
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'logs/main_crawler_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def run_gangnam_test():
    """ê°•ë‚¨ ì§€ì—­ í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§ (ê°œì„ ëœ ê¸°ëŠ¥ í™•ì¸ìš©)"""
    try:
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs('logs', exist_ok=True)
        
        logger.info("ğŸš€ ê°•ë‚¨ ì§€ì—­ í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§ ì‹œì‘")
        logger.info("âœ¨ ê°œì„ ëœ ê¸°ëŠ¥: ì£¼ì†Œ ì¶”ì¶œ, ì˜ì—…ì‹œê°„, break_time, last_order ìˆ˜ì§‘")
        
        # í¬ë¡¤ëŸ¬ ì„í¬íŠ¸ ë° ì‹¤í–‰
        from src.core.crawler import DiningCodeCrawler
        from src.core.database import DatabaseManager
        
        # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        crawler = DiningCodeCrawler()
        db = DatabaseManager()
        
        # ê°•ë‚¨ ì§€ì—­ í¬ë¡¤ë§ ì„¤ì •
        keyword = "ì„œìš¸ ê°•ë‚¨ ë¬´í•œë¦¬í•„"
        rect = "37.4979,127.0276,37.5279,127.0576"  # ê°•ë‚¨ ì§€ì—­ ì¢Œí‘œ
        
        logger.info(f"ê²€ìƒ‰ í‚¤ì›Œë“œ: {keyword}")
        logger.info(f"ê²€ìƒ‰ ì˜ì—­: {rect}")
        
        # ê°€ê²Œ ëª©ë¡ ìˆ˜ì§‘
        stores = crawler.get_store_list(keyword, rect)
        logger.info(f"ìˆ˜ì§‘ëœ ê°€ê²Œ ìˆ˜: {len(stores)}ê°œ")
        
        if not stores:
            logger.warning("ìˆ˜ì§‘ëœ ê°€ê²Œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
        detailed_stores = []
        success_count = 0
        
        for i, store in enumerate(stores, 1):
            try:
                logger.info(f"ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘... ({i}/{len(stores)}) {store.get('name', 'Unknown')}")
                
                detail_info = crawler.get_store_detail(store)
                if detail_info:
                    detailed_stores.append(detail_info)
                    success_count += 1
                    
                    # ê°œì„ ëœ ê¸°ëŠ¥ í™•ì¸ ë¡œê·¸
                    logger.info(f"  âœ… ì£¼ì†Œ: {detail_info.get('address', 'N/A')[:50]}...")
                    logger.info(f"  âœ… ì˜ì—…ì‹œê°„: {detail_info.get('open_hours', 'N/A')[:50]}...")
                    logger.info(f"  âœ… ë¸Œë ˆì´í¬íƒ€ì„: {detail_info.get('break_time', 'N/A')}")
                    logger.info(f"  âœ… ë¼ìŠ¤íŠ¸ì˜¤ë”: {detail_info.get('last_order', 'N/A')}")
                    logger.info(f"  âœ… íœ´ë¬´ì¼: {detail_info.get('holiday', 'N/A')}")
                    
            except Exception as e:
                logger.error(f"ê°€ê²Œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                continue
        
        logger.info(f"ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {success_count}/{len(stores)}ê°œ")
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        if detailed_stores:
            try:
                inserted_count = db.insert_stores_batch(detailed_stores)
                logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ: {inserted_count}ê°œ")
                
                # ì„±ê³µ í†µê³„
                success_rate = (success_count / len(stores)) * 100
                logger.info(f"ì „ì²´ ì„±ê³µë¥ : {success_rate:.1f}%")
                
                # ê°œì„ ëœ ê¸°ëŠ¥ í†µê³„
                check_improvement_stats(detailed_stores)
                
                return True
                
            except Exception as e:
                logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
                return False
        else:
            logger.warning("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
            
    except Exception as e:
        logger.error(f"ê°•ë‚¨ í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False
    
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        try:
            crawler.close()
            db.close()
        except:
            pass

def run_basic_test():
    """ê¸°ë³¸ í…ŒìŠ¤íŠ¸ (ë‹¨ì¼ ê°€ê²Œ ìƒì„¸ ì •ë³´ í™•ì¸)"""
    try:
        logger.info("ğŸ” ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì‹œì‘ (ë‹¨ì¼ ê°€ê²Œ ìƒì„¸ ì •ë³´ í™•ì¸)")
        
        from src.core.crawler import DiningCodeCrawler
        
        crawler = DiningCodeCrawler()
        
        # ê°•ë‚¨ ì§€ì—­ì—ì„œ ì²« ë²ˆì§¸ ê°€ê²Œ ì°¾ê¸°
        keyword = "ì„œìš¸ ê°•ë‚¨ ë¬´í•œë¦¬í•„"
        rect = "37.4979,127.0276,37.5279,127.0576"
        
        stores = crawler.get_store_list(keyword, rect)
        
        if not stores:
            logger.warning("í…ŒìŠ¤íŠ¸í•  ê°€ê²Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # ì²« ë²ˆì§¸ ê°€ê²Œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
        test_store = stores[0]
        logger.info(f"í…ŒìŠ¤íŠ¸ ëŒ€ìƒ: {test_store.get('name')}")
        
        detail_info = crawler.get_store_detail(test_store)
        
        if detail_info:
            logger.info("=== ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===")
            logger.info(f"ğŸ“ ê°€ê²Œëª…: {detail_info.get('name')}")
            logger.info(f"ğŸ“ ì£¼ì†Œ: {detail_info.get('address')}")
            logger.info(f"ğŸ“ ì „í™”ë²ˆí˜¸: {detail_info.get('phone_number')}")
            logger.info(f"â­ í‰ì : {detail_info.get('diningcode_rating')}")
            logger.info(f"ğŸ• ì˜ì—…ì‹œê°„: {detail_info.get('open_hours')}")
            logger.info(f"â˜• ë¸Œë ˆì´í¬íƒ€ì„: {detail_info.get('break_time')}")
            logger.info(f"ğŸ½ï¸ ë¼ìŠ¤íŠ¸ì˜¤ë”: {detail_info.get('last_order')}")
            logger.info(f"ğŸš« íœ´ë¬´ì¼: {detail_info.get('holiday')}")
            logger.info(f"ğŸ’° ê°€ê²©: {detail_info.get('price')}")
            logger.info(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ìˆ˜: {len(detail_info.get('image_urls', []))}")
            
            return True
        else:
            logger.error("ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨")
            return False
            
    except Exception as e:
        logger.error(f"ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return False
    
    finally:
        try:
            crawler.close()
        except:
            pass

def run_seoul_full_crawling():
    """ì„œìš¸ ì „ì§€ì—­ í¬ë¡¤ë§"""
    try:
        logger.info("ğŸŒ ì„œìš¸ ì „ì§€ì—­ í¬ë¡¤ë§ ì‹œì‘")
        
        from src.core.crawler import DiningCodeCrawler
        from src.core.database import DatabaseManager
        
        crawler = DiningCodeCrawler()
        db = DatabaseManager()
        
        # ì„œìš¸ì‹œ 25ê°œ êµ¬ ì„¤ì • (ê°„ì†Œí™”)
        seoul_regions = [
            {"name": "ê°•ë‚¨êµ¬", "keyword": "ì„œìš¸ ê°•ë‚¨ ë¬´í•œë¦¬í•„", "rect": "37.4979,127.0276,37.5279,127.0576"},
            {"name": "ê°•ë¶êµ¬", "keyword": "ì„œìš¸ ê°•ë¶ ë¬´í•œë¦¬í•„", "rect": "37.6279,127.0076,37.6579,127.0376"},
            {"name": "ê°•ì„œêµ¬", "keyword": "ì„œìš¸ ê°•ì„œ ë¬´í•œë¦¬í•„", "rect": "37.5379,126.8176,37.5679,126.8476"},
            {"name": "ê´€ì•…êµ¬", "keyword": "ì„œìš¸ ê´€ì•… ë¬´í•œë¦¬í•„", "rect": "37.4579,126.9276,37.4879,126.9576"},
            {"name": "ê´‘ì§„êµ¬", "keyword": "ì„œìš¸ ê´‘ì§„ ë¬´í•œë¦¬í•„", "rect": "37.5279,127.0676,37.5579,127.0976"},
            {"name": "êµ¬ë¡œêµ¬", "keyword": "ì„œìš¸ êµ¬ë¡œ ë¬´í•œë¦¬í•„", "rect": "37.4879,126.8576,37.5179,126.8876"},
            {"name": "ê¸ˆì²œêµ¬", "keyword": "ì„œìš¸ ê¸ˆì²œ ë¬´í•œë¦¬í•„", "rect": "37.4479,126.8776,37.4779,126.9076"},
            {"name": "ë…¸ì›êµ¬", "keyword": "ì„œìš¸ ë…¸ì› ë¬´í•œë¦¬í•„", "rect": "37.6379,127.0576,37.6679,127.0876"},
            {"name": "ë„ë´‰êµ¬", "keyword": "ì„œìš¸ ë„ë´‰ ë¬´í•œë¦¬í•„", "rect": "37.6579,127.0276,37.6879,127.0576"},
            {"name": "ë™ëŒ€ë¬¸êµ¬", "keyword": "ì„œìš¸ ë™ëŒ€ë¬¸ ë¬´í•œë¦¬í•„", "rect": "37.5579,127.0376,37.5879,127.0676"},
            {"name": "ë™ì‘êµ¬", "keyword": "ì„œìš¸ ë™ì‘ ë¬´í•œë¦¬í•„", "rect": "37.4979,126.9376,37.5279,126.9676"},
            {"name": "ë§ˆí¬êµ¬", "keyword": "ì„œìš¸ ë§ˆí¬ ë¬´í•œë¦¬í•„", "rect": "37.5379,126.8976,37.5679,126.9276"},
            {"name": "ì„œëŒ€ë¬¸êµ¬", "keyword": "ì„œìš¸ ì„œëŒ€ë¬¸ ë¬´í•œë¦¬í•„", "rect": "37.5679,126.9176,37.5979,126.9476"},
            {"name": "ì„œì´ˆêµ¬", "keyword": "ì„œìš¸ ì„œì´ˆ ë¬´í•œë¦¬í•„", "rect": "37.4679,127.0076,37.4979,127.0376"},
            {"name": "ì„±ë™êµ¬", "keyword": "ì„œìš¸ ì„±ë™ ë¬´í•œë¦¬í•„", "rect": "37.5479,127.0176,37.5779,127.0476"},
            {"name": "ì„±ë¶êµ¬", "keyword": "ì„œìš¸ ì„±ë¶ ë¬´í•œë¦¬í•„", "rect": "37.5879,127.0076,37.6179,127.0376"},
            {"name": "ì†¡íŒŒêµ¬", "keyword": "ì„œìš¸ ì†¡íŒŒ ë¬´í•œë¦¬í•„", "rect": "37.4779,127.0876,37.5079,127.1176"},
            {"name": "ì–‘ì²œêµ¬", "keyword": "ì„œìš¸ ì–‘ì²œ ë¬´í•œë¦¬í•„", "rect": "37.5179,126.8476,37.5479,126.8776"},
            {"name": "ì˜ë“±í¬êµ¬", "keyword": "ì„œìš¸ ì˜ë“±í¬ ë¬´í•œë¦¬í•„", "rect": "37.5079,126.8876,37.5379,126.9176"},
            {"name": "ìš©ì‚°êµ¬", "keyword": "ì„œìš¸ ìš©ì‚° ë¬´í•œë¦¬í•„", "rect": "37.5179,126.9676,37.5479,126.9976"},
            {"name": "ì€í‰êµ¬", "keyword": "ì„œìš¸ ì€í‰ ë¬´í•œë¦¬í•„", "rect": "37.5879,126.9076,37.6179,126.9376"},
            {"name": "ì¢…ë¡œêµ¬", "keyword": "ì„œìš¸ ì¢…ë¡œ ë¬´í•œë¦¬í•„", "rect": "37.5679,126.9776,37.5979,127.0076"},
            {"name": "ì¤‘êµ¬", "keyword": "ì„œìš¸ ì¤‘êµ¬ ë¬´í•œë¦¬í•„", "rect": "37.5479,126.9776,37.5779,127.0076"},
            {"name": "ì¤‘ë‘êµ¬", "keyword": "ì„œìš¸ ì¤‘ë‘ ë¬´í•œë¦¬í•„", "rect": "37.5979,127.0676,37.6279,127.0976"},
            {"name": "ê°•ë™êµ¬", "keyword": "ì„œìš¸ ê°•ë™ ë¬´í•œë¦¬í•„", "rect": "37.5179,127.1076,37.5479,127.1376"}
        ]
        
        total_stores = 0
        successful_regions = 0
        
        for i, region in enumerate(seoul_regions, 1):
            logger.info(f"ğŸ“ [{i}/{len(seoul_regions)}] {region['name']} í¬ë¡¤ë§ ì‹œì‘")
            
            try:
                # ì§€ì—­ë³„ í¬ë¡¤ë§
                stores = crawler.get_store_list(region['keyword'], region['rect'])
                
                if stores:
                    # ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
                    detailed_stores = []
                    for store in stores:
                        try:
                            detail_info = crawler.get_store_detail(store)
                            if detail_info:
                                detailed_stores.append(detail_info)
                        except Exception as e:
                            logger.warning(f"ê°€ê²Œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                            continue
                    
                    if detailed_stores:
                        inserted_count = db.insert_stores_batch(detailed_stores)
                        total_stores += inserted_count
                        successful_regions += 1
                        logger.info(f"âœ… {region['name']}: {inserted_count}ê°œ ì €ì¥")
                    else:
                        logger.warning(f"âŒ {region['name']}: ì €ì¥í•  ë°ì´í„° ì—†ìŒ")
                else:
                    logger.warning(f"âŒ {region['name']}: ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            
            except Exception as e:
                logger.error(f"âŒ {region['name']} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                continue
            
            # ì§€ì—­ ê°„ íœ´ì‹ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            if i < len(seoul_regions):
                logger.info("â° 5ì´ˆ íœ´ì‹...")
                import time
                time.sleep(5)
        
        logger.info(f"ğŸ‰ ì„œìš¸ ì „ì§€ì—­ í¬ë¡¤ë§ ì™„ë£Œ!")
        logger.info(f"ğŸ“Š ì„±ê³µí•œ êµ¬: {successful_regions}/{len(seoul_regions)}ê°œ")
        logger.info(f"ğŸ“Š ì´ ìˆ˜ì§‘ ê°€ê²Œ: {total_stores}ê°œ")
        
        return True
        
    except Exception as e:
        logger.error(f"ì„œìš¸ ì „ì§€ì—­ í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return False
    
    finally:
        try:
            crawler.close()
            db.close()
        except:
            pass

def check_improvement_stats(stores):
    """ê°œì„ ëœ ê¸°ëŠ¥ í†µê³„ í™•ì¸"""
    logger.info("ğŸ” ê°œì„ ëœ ê¸°ëŠ¥ í†µê³„ í™•ì¸:")
    
    # ì£¼ì†Œ ìˆ˜ì§‘ë¥ 
    address_count = sum(1 for store in stores if store.get('address'))
    address_rate = (address_count / len(stores)) * 100
    logger.info(f"  ğŸ“ ì£¼ì†Œ ìˆ˜ì§‘ë¥ : {address_count}/{len(stores)} ({address_rate:.1f}%)")
    
    # ì˜ì—…ì‹œê°„ ìˆ˜ì§‘ë¥ 
    hours_count = sum(1 for store in stores if store.get('open_hours'))
    hours_rate = (hours_count / len(stores)) * 100
    logger.info(f"  ğŸ• ì˜ì—…ì‹œê°„ ìˆ˜ì§‘ë¥ : {hours_count}/{len(stores)} ({hours_rate:.1f}%)")
    
    # ë¸Œë ˆì´í¬íƒ€ì„ ìˆ˜ì§‘ë¥ 
    break_count = sum(1 for store in stores if store.get('break_time'))
    break_rate = (break_count / len(stores)) * 100
    logger.info(f"  â˜• ë¸Œë ˆì´í¬íƒ€ì„ ìˆ˜ì§‘ë¥ : {break_count}/{len(stores)} ({break_rate:.1f}%)")
    
    # ë¼ìŠ¤íŠ¸ì˜¤ë” ìˆ˜ì§‘ë¥ 
    last_order_count = sum(1 for store in stores if store.get('last_order'))
    last_order_rate = (last_order_count / len(stores)) * 100
    logger.info(f"  ğŸ½ï¸ ë¼ìŠ¤íŠ¸ì˜¤ë” ìˆ˜ì§‘ë¥ : {last_order_count}/{len(stores)} ({last_order_rate:.1f}%)")
    
    # íœ´ë¬´ì¼ ìˆ˜ì§‘ë¥ 
    holiday_count = sum(1 for store in stores if store.get('holiday'))
    holiday_rate = (holiday_count / len(stores)) * 100
    logger.info(f"  ğŸš« íœ´ë¬´ì¼ ìˆ˜ì§‘ë¥ : {holiday_count}/{len(stores)} ({holiday_rate:.1f}%)")
    
    # ìš”ì¼ë³„ ì˜ì—…ì‹œê°„ ìˆ˜ì§‘ë¥  (ì›”í™”ìˆ˜ëª©ê¸ˆí† ì¼ íŒ¨í„´ í™•ì¸)
    weekday_pattern_count = 0
    for store in stores:
        hours = store.get('open_hours', '')
        if any(day in hours for day in ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼']):
            weekday_pattern_count += 1
    
    weekday_rate = (weekday_pattern_count / len(stores)) * 100
    logger.info(f"  ğŸ“… ìš”ì¼ë³„ ì˜ì—…ì‹œê°„ ìˆ˜ì§‘ë¥ : {weekday_pattern_count}/{len(stores)} ({weekday_rate:.1f}%)")

def check_database_status():
    """ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸"""
    try:
        logger.info("ğŸ” ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸")
        
        from src.core.database import DatabaseManager
        db = DatabaseManager()
        
        # ê¸°ë³¸ ì—°ê²° í…ŒìŠ¤íŠ¸
        if not db.test_connection():
            logger.error("âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨")
            return False
        
        logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")
        
        # í…Œì´ë¸” í™•ì¸
        try:
            cursor = db.connection.cursor()
            cursor.execute("SELECT COUNT(*) FROM stores")
            store_count = cursor.fetchone()[0]
            logger.info(f"ğŸ“Š í˜„ì¬ ì €ì¥ëœ ê°€ê²Œ ìˆ˜: {store_count}ê°œ")
            
            if store_count > 0:
                # ê°œì„ ëœ í•„ë“œ í™•ì¸
                cursor.execute("SELECT COUNT(*) FROM stores WHERE address IS NOT NULL AND address != ''")
                address_count = cursor.fetchone()[0]
                address_rate = (address_count / store_count * 100)
                logger.info(f"ğŸ“ ì£¼ì†Œ ë³´ìœ  ê°€ê²Œ: {address_count}ê°œ ({address_rate:.1f}%)")
                
                cursor.execute("SELECT COUNT(*) FROM stores WHERE break_time IS NOT NULL AND break_time != ''")
                break_time_count = cursor.fetchone()[0]
                break_time_rate = (break_time_count / store_count * 100)
                logger.info(f"â˜• ë¸Œë ˆì´í¬íƒ€ì„ ë³´ìœ  ê°€ê²Œ: {break_time_count}ê°œ ({break_time_rate:.1f}%)")
                
                cursor.execute("SELECT COUNT(*) FROM stores WHERE last_order IS NOT NULL AND last_order != ''")
                last_order_count = cursor.fetchone()[0]
                last_order_rate = (last_order_count / store_count * 100)
                logger.info(f"ğŸ½ï¸ ë¼ìŠ¤íŠ¸ì˜¤ë” ë³´ìœ  ê°€ê²Œ: {last_order_count}ê°œ ({last_order_rate:.1f}%)")
            
            cursor.close()
            
        except Exception as e:
            logger.warning(f"í…Œì´ë¸” ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
        
        db.close()
        return True
        
    except Exception as e:
        logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
        return False

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ë¦¬í•„ìŠ¤íŒŸ í¬ë¡¤ëŸ¬ (ê°„ì†Œí™”ëœ ë²„ì „)')
    parser.add_argument('mode', choices=['gangnam', 'basic', 'seoul', 'check', 'stage4'], 
                       help='ì‹¤í–‰ ëª¨ë“œ: gangnam(ê°•ë‚¨í…ŒìŠ¤íŠ¸), basic(ê¸°ë³¸í…ŒìŠ¤íŠ¸), seoul(ì„œìš¸ì „ì§€ì—­), check(DBìƒíƒœ), stage4(í˜¸í™˜ì„±)')
    
    args = parser.parse_args()
    
    print("ğŸ½ï¸ ë¦¬í•„ìŠ¤íŒŸ í¬ë¡¤ëŸ¬ (ê°„ì†Œí™”ëœ ë²„ì „)")
    print("=" * 50)
    print("âœ¨ ê°œì„ ëœ ê¸°ëŠ¥:")
    print("  - ì£¼ì†Œ ì •í™• ì¶”ì¶œ")
    print("  - ìš”ì¼ë³„ ì˜ì—…ì‹œê°„ ìˆ˜ì§‘")
    print("  - ë¸Œë ˆì´í¬íƒ€ì„ ì •ë³´ ìˆ˜ì§‘")
    print("  - ë¼ìŠ¤íŠ¸ì˜¤ë” ì •ë³´ ìˆ˜ì§‘")
    print("  - íœ´ë¬´ì¼ ì •ë³´ ìˆ˜ì§‘")
    print("=" * 50)
    
    if args.mode == 'gangnam':
        print("ğŸš€ ê°•ë‚¨ ì§€ì—­ í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§")
        success = run_gangnam_test()
    elif args.mode == 'basic':
        print("ğŸ” ê¸°ë³¸ í…ŒìŠ¤íŠ¸ (ë‹¨ì¼ ê°€ê²Œ)")
        success = run_basic_test()
    elif args.mode == 'seoul':
        print("ğŸŒ ì„œìš¸ ì „ì§€ì—­ í¬ë¡¤ë§")
        success = run_seoul_full_crawling()
    elif args.mode == 'check':
        print("ğŸ” ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸")
        success = check_database_status()
    elif args.mode == 'stage4':
        print("ğŸ”„ Stage4 í˜¸í™˜ ëª¨ë“œ (ê°•ë‚¨ ì§€ì—­)")
        success = run_gangnam_test()
    
    if success:
        print("âœ… ì‹¤í–‰ ì™„ë£Œ!")
    else:
        print("âŒ ì‹¤í–‰ ì‹¤íŒ¨!")
        sys.exit(1)

if __name__ == "__main__":
    main()