"""
ë‹¤ì´ë‹ì½”ë“œ ë¬´í•œë¦¬í•„ ê°€ê²Œ í¬ë¡¤ë§ ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸
3ë‹¨ê³„: ë°ì´í„° í’ˆì§ˆ ê³ ë„í™” ë²„ì „ (ì§€ì˜¤ì½”ë”©, ê°€ê²©ì •ê·œí™”, ì¹´í…Œê³ ë¦¬ë§¤í•‘, ì¤‘ë³µì œê±°)
"""

import logging
import pandas as pd
import time
import json
from typing import List, Dict
from collections import Counter
from datetime import datetime
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'config'))
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'core'))

import config
from crawler import DiningCodeCrawler
from database import DatabaseManager
from data_enhancement import DataEnhancer  # 3ë‹¨ê³„ ê³ ë„í™” ëª¨ë“ˆ

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('refill_spot_crawler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class CrawlingProgressMonitor:
    """í¬ë¡¤ë§ ì§„í–‰ìƒí™© ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.start_time = None
        self.total_keywords = 0
        self.completed_keywords = 0
        self.total_stores = 0
        self.processed_stores = 0
        self.failed_stores = 0
        self.current_keyword = ""
        
    def start_session(self, total_keywords: int):
        """í¬ë¡¤ë§ ì„¸ì…˜ ì‹œì‘"""
        self.start_time = datetime.now()
        self.total_keywords = total_keywords
        self.completed_keywords = 0
        self.total_stores = 0
        self.processed_stores = 0
        self.failed_stores = 0
        logger.info(f"=== í¬ë¡¤ë§ ì„¸ì…˜ ì‹œì‘: {total_keywords}ê°œ í‚¤ì›Œë“œ ===")
        
    def start_keyword(self, keyword: str):
        """í‚¤ì›Œë“œ ì²˜ë¦¬ ì‹œì‘"""
        self.current_keyword = keyword
        logger.info(f"í‚¤ì›Œë“œ '{keyword}' ì²˜ë¦¬ ì‹œì‘ ({self.completed_keywords + 1}/{self.total_keywords})")
        
    def update_stores_found(self, count: int):
        """ë°œê²¬ëœ ê°€ê²Œ ìˆ˜ ì—…ë°ì´íŠ¸"""
        self.total_stores += count
        logger.info(f"í‚¤ì›Œë“œ '{self.current_keyword}': {count}ê°œ ê°€ê²Œ ë°œê²¬")
        
    def update_store_processed(self, success: bool = True):
        """ê°€ê²Œ ì²˜ë¦¬ ì™„ë£Œ ì—…ë°ì´íŠ¸"""
        if success:
            self.processed_stores += 1
        else:
            self.failed_stores += 1
            
    def complete_keyword(self):
        """í‚¤ì›Œë“œ ì²˜ë¦¬ ì™„ë£Œ"""
        self.completed_keywords += 1
        elapsed = datetime.now() - self.start_time
        avg_time_per_keyword = elapsed.total_seconds() / self.completed_keywords
        remaining_keywords = self.total_keywords - self.completed_keywords
        estimated_remaining = remaining_keywords * avg_time_per_keyword
        
        logger.info(f"í‚¤ì›Œë“œ '{self.current_keyword}' ì™„ë£Œ")
        logger.info(f"ì§„í–‰ë¥ : {self.completed_keywords}/{self.total_keywords} ({self.completed_keywords/self.total_keywords*100:.1f}%)")
        logger.info(f"ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {estimated_remaining/60:.1f}ë¶„")
        
    def get_summary(self) -> Dict:
        """ì„¸ì…˜ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        elapsed = datetime.now() - self.start_time if self.start_time else None
        return {
            'total_keywords': self.total_keywords,
            'completed_keywords': self.completed_keywords,
            'total_stores_found': self.total_stores,
            'processed_stores': self.processed_stores,
            'failed_stores': self.failed_stores,
            'success_rate': self.processed_stores / max(self.total_stores, 1) * 100,
            'elapsed_time_minutes': elapsed.total_seconds() / 60 if elapsed else 0
        }

def process_crawled_data_enhanced(stores_data: List[Dict]) -> List[Dict]:
    """í¬ë¡¤ë§ëœ ë°ì´í„° ì •ì œ ë° ì²˜ë¦¬ (ê°•í™”ëœ ë²„ì „)"""
    processed_stores = []
    
    for store in stores_data:
        # í•„ìˆ˜ í•„ë“œ ê²€ì¦
        if not store.get('diningcode_place_id') or not store.get('name'):
            logger.warning(f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {store}")
            continue
        
        # ë¬´í•œë¦¬í•„ ê´€ë ¨ì„± ê²€ì¦ (ê°•í™”)
        is_refill = validate_refill_relevance(store)
        if not is_refill:
            logger.warning(f"ë¬´í•œë¦¬í•„ ê´€ë ¨ì„± ì—†ìŒ: {store.get('name')}")
            continue
        
        # ì¢Œí‘œ ìœ íš¨ì„± ê²€ì¦
        lat, lng = validate_coordinates(store)
        if not lat or not lng:
            logger.warning(f"ì¢Œí‘œ ì •ë³´ ì—†ìŒ: {store.get('name')}")
            continue
        
        # í‰ì  ì •ë³´ ì •ê·œí™”
        rating = normalize_rating(store.get('diningcode_rating'))
        
        # ê°€ê²© ì •ë³´ ì •ê·œí™”
        price_info = normalize_price_info(store)
        
        # ì •ì œëœ ë°ì´í„° êµ¬ì„± (ê°•í™”ëœ ìŠ¤í‚¤ë§ˆ)
        processed_store = {
            # ê¸°ë³¸ ì •ë³´
            'name': store.get('name', '').strip(),
            'address': store.get('address', '').strip(),
            'description': store.get('description', '').strip(),
            
            # ìœ„ì¹˜ ì •ë³´
            'position_lat': lat,
            'position_lng': lng,
            'position_x': None,
            'position_y': None,
            
            # í‰ì  ì •ë³´
            'naver_rating': None,
            'kakao_rating': None,
            'diningcode_rating': rating,
            
            # ì˜ì—…ì‹œê°„ ì •ë³´ (ê°•í™”)
            'open_hours': store.get('open_hours', ''),
            'open_hours_raw': store.get('open_hours_raw', ''),
            'break_time': store.get('break_time', ''),
            'last_order': store.get('last_order', ''),
            'holiday': store.get('holiday', ''),
            
            # ê°€ê²© ì •ë³´ (ê°•í™”)
            'price': price_info.get('price'),
            'price_range': store.get('price_range', ''),
            'average_price': store.get('average_price', ''),
            'price_details': store.get('price_details', []),
            
            # ë¬´í•œë¦¬í•„ ì •ë³´ (ê°•í™”)
            'refill_items': store.get('refill_items', []),
            'refill_type': store.get('refill_type', ''),
            'refill_conditions': store.get('refill_conditions', ''),
            'is_confirmed_refill': store.get('is_confirmed_refill', False),
            
            # ì´ë¯¸ì§€ ì •ë³´ (ê°•í™”)
            'image_urls': store.get('image_urls', []),
            'main_image': store.get('main_image', ''),
            'menu_images': store.get('menu_images', []),
            'interior_images': store.get('interior_images', []),
            
            # ë©”ë‰´ ì •ë³´ (ê°•í™”)
            'menu_items': store.get('menu_items', []),
            'menu_categories': store.get('menu_categories', []),
            'signature_menu': store.get('signature_menu', []),
            
            # ë¦¬ë·° ë° ì„¤ëª… ì •ë³´ (ê°•í™”)
            'review_summary': store.get('review_summary', ''),
            'keywords': store.get('keywords', []),
            'atmosphere': store.get('atmosphere', ''),
            
            # ì—°ë½ì²˜ ì •ë³´ (ê°•í™”)
            'phone_number': store.get('phone_number', '').strip(),
            'website': store.get('website', ''),
            'social_media': store.get('social_media', []),
            
            # ê¸°ì¡´ í•„ë“œ
            'diningcode_place_id': store.get('diningcode_place_id'),
            'raw_categories_diningcode': store.get('raw_categories_diningcode', []),
            'status': 'ìš´ì˜ì¤‘'
        }
        
        processed_stores.append(processed_store)
    
    logger.info(f"ë°ì´í„° ì •ì œ ì™„ë£Œ: {len(stores_data)} -> {len(processed_stores)}")
    return processed_stores

def validate_refill_relevance(store: Dict) -> bool:
    """ë¬´í•œë¦¬í•„ ê´€ë ¨ì„± ê²€ì¦ (ê°•í™”)"""
    refill_keywords = ['ë¬´í•œë¦¬í•„', 'ë·”í˜', 'ë¬´ì œí•œ', 'ë¦¬í•„', 'ì…€í”„ë°”', 'ë¬´ë£Œë¦¬í•„']
    
    # ì´ë¦„ì—ì„œ í™•ì¸
    name = store.get('name', '').lower()
    for keyword in refill_keywords:
        if keyword in name:
            return True
    
    # ì¹´í…Œê³ ë¦¬ì—ì„œ í™•ì¸
    categories = store.get('raw_categories_diningcode', [])
    for category in categories:
        for keyword in refill_keywords:
            if keyword in category.lower():
                return True
    
    # ë¬´í•œë¦¬í•„ í™•ì • í•„ë“œ í™•ì¸
    if store.get('is_confirmed_refill'):
        return True
    
    # ë¦¬í•„ ì•„ì´í…œì´ ìˆëŠ” ê²½ìš°
    if store.get('refill_items') and len(store.get('refill_items', [])) > 0:
        return True
    
    # í‚¤ì›Œë“œì—ì„œ í™•ì¸
    keywords = store.get('keywords', [])
    for keyword in keywords:
        for refill_kw in refill_keywords:
            if refill_kw in keyword.lower():
                return True
    
    return False

def validate_coordinates(store: Dict) -> tuple:
    """ì¢Œí‘œ ìœ íš¨ì„± ê²€ì¦"""
    lat = store.get('position_lat')
    lng = store.get('position_lng')
    
    if not lat or not lng:
        return None, None
    
    try:
        lat = float(lat)
        lng = float(lng)
        # í•œêµ­ ì¢Œí‘œ ë²”ìœ„ í™•ì¸
        if not (33 <= lat <= 39 and 124 <= lng <= 132):
            return None, None
        return lat, lng
    except (ValueError, TypeError):
        return None, None

def normalize_rating(rating) -> float:
    """í‰ì  ì •ê·œí™”"""
    if not rating:
        return None
    try:
        rating = float(rating)
        if 0 <= rating <= 5:
            return rating
    except (ValueError, TypeError):
        pass
    return None

def normalize_price_info(store: Dict) -> Dict:
    """ê°€ê²© ì •ë³´ ì •ê·œí™”"""
    price_info = {'price': None}
    
    # ê¸°ì¡´ price í•„ë“œ ì²˜ë¦¬
    price = store.get('price')
    if price:
        try:
            if isinstance(price, str):
                # ë¬¸ìì—´ì—ì„œ ìˆ«ì ì¶”ì¶œ
                import re
                numbers = re.findall(r'\d+', price.replace(',', ''))
                if numbers:
                    price_info['price'] = int(numbers[0])
            else:
                price_info['price'] = int(price)
        except (ValueError, TypeError):
            pass
    
    # average_priceì—ì„œ ì¶”ì¶œ ì‹œë„
    if not price_info['price']:
        avg_price = store.get('average_price', '')
        if avg_price:
            try:
                import re
                numbers = re.findall(r'\d+', avg_price.replace(',', ''))
                if numbers:
                    price_info['price'] = int(numbers[0])
            except:
                pass
    
    return price_info

def run_enhanced_crawling():
    """ê°•í™”ëœ í¬ë¡¤ë§ ì‹¤í–‰ (3ë‹¨ê³„: ë°ì´í„° í’ˆì§ˆ ê³ ë„í™” í¬í•¨)"""
    crawler = None
    db = None
    monitor = CrawlingProgressMonitor()
    
    try:
        logger.info("=== Refill Spot í¬ë¡¤ë§ ì‹œì‘ (3ë‹¨ê³„ ê³ ë„í™” ë²„ì „) ===")
        
        # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        crawler = DiningCodeCrawler()
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        db = DatabaseManager()
        if not db.test_connection():
            logger.error("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨")
            return
        
        db.create_tables()
        
        # í¬ë¡¤ë§ ì„¤ì •
        region_name = config.REGIONS[config.TEST_REGION]["name"]
        keywords = config.TEST_KEYWORDS
        rect = config.TEST_RECT
        
        logger.info(f"=== {region_name} ì§€ì—­ í¬ë¡¤ë§ ì‹œì‘ ===")
        logger.info(f"ì‚¬ìš©í•  í‚¤ì›Œë“œ: {keywords}")
        logger.info(f"ê²€ìƒ‰ ì˜ì—­: {rect}")
        
        # ì§„í–‰ìƒí™© ëª¨ë‹ˆí„°ë§ ì‹œì‘
        monitor.start_session(len(keywords))
        
        all_stores = []
        
        # ê° í‚¤ì›Œë“œë³„ë¡œ í¬ë¡¤ë§
        for keyword in keywords:
            monitor.start_keyword(keyword)
            
            try:
                # ëª©ë¡ ìˆ˜ì§‘
                stores = crawler.get_store_list(keyword, rect)
                monitor.update_stores_found(len(stores))
                
                if not stores:
                    logger.warning(f"í‚¤ì›Œë“œ '{keyword}'ë¡œ ê²€ìƒ‰ëœ ê°€ê²Œê°€ ì—†ìŠµë‹ˆë‹¤.")
                    monitor.complete_keyword()
                    continue
                
                # ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ (ë°°ì¹˜ ì²˜ë¦¬)
                detailed_stores = process_stores_batch(crawler, stores, monitor)
                all_stores.extend(detailed_stores)
                
                monitor.complete_keyword()
                
                # í‚¤ì›Œë“œ ê°„ íœ´ì‹ ì‹œê°„
                time.sleep(2)
                
            except Exception as e:
                logger.error(f"í‚¤ì›Œë“œ '{keyword}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                monitor.complete_keyword()
                continue
        
        # 3ë‹¨ê³„ ê³ ë„í™”: ë°ì´í„° í’ˆì§ˆ ê°•í™”
        if all_stores:
            logger.info("=== 3ë‹¨ê³„ ë°ì´í„° í’ˆì§ˆ ê°•í™” ì‹œì‘ ===")
            
            # ë°ì´í„° ê°•í™” ì‹¤í–‰
            enhancer = DataEnhancer()
            enhanced_stores, enhancement_stats = enhancer.enhance_stores_data(all_stores)
            
            # ê°•í™” ê²°ê³¼ ë¡œê¹…
            logger.info("=== ë°ì´í„° ê°•í™” ì™„ë£Œ ===")
            logger.info(f"ì›ë³¸ ê°€ê²Œ ìˆ˜: {enhancement_stats.total_stores}")
            logger.info(f"ìµœì¢… ê°€ê²Œ ìˆ˜: {len(enhanced_stores)}")
            logger.info(f"ì§€ì˜¤ì½”ë”© ì„±ê³µ: {enhancement_stats.geocoding_success}/{enhancement_stats.total_stores}")
            logger.info(f"ê°€ê²© ì •ê·œí™”: {enhancement_stats.price_normalized}/{enhancement_stats.total_stores}")
            logger.info(f"ì¹´í…Œê³ ë¦¬ ë§¤í•‘: {enhancement_stats.categories_mapped}/{enhancement_stats.total_stores}")
            logger.info(f"ì¤‘ë³µ ì œê±°: {enhancement_stats.duplicates_removed}ê°œ")
            logger.info(f"ê°•í™” ì²˜ë¦¬ ì‹œê°„: {enhancement_stats.processing_time:.2f}ì´ˆ")
            
            # ê°•í™”ëœ ë°ì´í„° ì €ì¥
            if enhanced_stores:
                processed_data = process_crawled_data_enhanced(enhanced_stores)
                if processed_data:
                    db.save_crawled_data(processed_data, "enhanced_crawling", rect)
                    logger.info(f"ê°•í™”ëœ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {len(processed_data)}ê°œ")
            
            # ê°•í™” í†µê³„ ìƒì„¸ ì •ë³´
            enhancement_summary = enhancer.get_enhancement_summary()
            logger.info("=== ê°•í™” í†µê³„ ìƒì„¸ ===")
            logger.info(f"ì¢Œí‘œ ì™„ì„±ë„: {enhancement_summary.get('geocoding_rate', 0):.1f}%")
            logger.info(f"ê°€ê²© ì •ê·œí™”ìœ¨: {enhancement_summary.get('price_normalization_rate', 0):.1f}%")
            logger.info(f"ì¹´í…Œê³ ë¦¬ ë§¤í•‘ìœ¨: {enhancement_summary.get('category_mapping_rate', 0):.1f}%")
            logger.info(f"ì¤‘ë³µ ì œê±°ìœ¨: {enhancement_summary.get('duplicate_removal_rate', 0):.1f}%")
            
            # ì§€ì˜¤ì½”ë”© ìƒì„¸ í†µê³„
            geocoding_stats = enhancement_summary.get('geocoding_stats', {})
            if geocoding_stats:
                logger.info("=== ì§€ì˜¤ì½”ë”© í†µê³„ ===")
                logger.info(f"ì¹´ì¹´ì˜¤ API ì„±ê³µë¥ : {geocoding_stats.get('kakao_rate', 0):.1f}%")
                logger.info(f"ë„¤ì´ë²„ API ì„±ê³µë¥ : {geocoding_stats.get('naver_rate', 0):.1f}%")
                logger.info(f"ì „ì²´ ì„±ê³µë¥ : {geocoding_stats.get('success_rate', 0):.1f}%")
            
            # ê°€ê²© ì •ê·œí™” ìƒì„¸ í†µê³„
            price_stats = enhancement_summary.get('price_stats', {})
            if price_stats:
                logger.info("=== ê°€ê²© ì •ê·œí™” í†µê³„ ===")
                logger.info(f"ë‹¨ì¼ ê°€ê²©: {price_stats.get('single_price', 0)}ê°œ")
                logger.info(f"ë²”ìœ„ ê°€ê²©: {price_stats.get('range_price', 0)}ê°œ")
                logger.info(f"ì‹œê°„ëŒ€ë³„ ê°€ê²©: {price_stats.get('time_based_price', 0)}ê°œ")
                logger.info(f"ì¡°ê±´ë¶€ ê°€ê²©: {price_stats.get('conditional_price', 0)}ê°œ")
                logger.info(f"ê°€ê²© ë¬¸ì˜: {price_stats.get('inquiry_price', 0)}ê°œ")
                logger.info(f"ì •ê·œí™” ì„±ê³µë¥ : {price_stats.get('success_rate', 0):.1f}%")
        
        # ìµœì¢… ê²°ê³¼ ìš”ì•½
        summary = monitor.get_summary()
        logger.info("=== í¬ë¡¤ë§ ì™„ë£Œ ===")
        logger.info(f"ì´ í‚¤ì›Œë“œ: {summary['total_keywords']}ê°œ")
        logger.info(f"ì´ ë°œê²¬ ê°€ê²Œ: {summary['total_stores_found']}ê°œ")
        logger.info(f"ì„±ê³µ ì²˜ë¦¬: {summary['processed_stores']}ê°œ")
        logger.info(f"ì‹¤íŒ¨: {summary['failed_stores']}ê°œ")
        logger.info(f"ì„±ê³µë¥ : {summary['success_rate']:.1f}%")
        logger.info(f"ì†Œìš” ì‹œê°„: {summary['elapsed_time_minutes']:.1f}ë¶„")
        
        # ê°•í™”ëœ í†µê³„ ì¡°íšŒ
        stats = db.get_enhanced_crawling_stats()
        if stats:
            logger.info("=== ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ===")
            basic = stats.get('basic_stats', {})
            logger.info(f"ì´ ê°€ê²Œ ìˆ˜: {basic.get('total_stores', 0)}ê°œ")
            logger.info(f"ë¬´í•œë¦¬í•„ í™•ì •: {basic.get('confirmed_refill_stores', 0)}ê°œ")
            logger.info(f"ë©”ë‰´ ì •ë³´ ë³´ìœ : {basic.get('stores_with_menu', 0)}ê°œ")
            logger.info(f"ì´ë¯¸ì§€ ë³´ìœ : {basic.get('stores_with_images', 0)}ê°œ")
            logger.info(f"ê°€ê²© ì •ë³´ ë³´ìœ : {basic.get('stores_with_price', 0)}ê°œ")
            logger.info(f"í‰ê·  í‰ì : {basic.get('avg_rating', 0):.2f}")
        
        return enhanced_stores if 'enhanced_stores' in locals() else all_stores
        
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return []
        
    finally:
        if crawler:
            crawler.close()
        if db:
            db.close()

def process_stores_batch(crawler: DiningCodeCrawler, stores: List[Dict], monitor: CrawlingProgressMonitor, batch_size: int = 5) -> List[Dict]:
    """ê°€ê²Œ ìƒì„¸ ì •ë³´ ë°°ì¹˜ ì²˜ë¦¬"""
    detailed_stores = []
    
    for i in range(0, len(stores), batch_size):
        batch = stores[i:i + batch_size]
        logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ {i//batch_size + 1}: {len(batch)}ê°œ ê°€ê²Œ")
        
        for store in batch:
            try:
                logger.info(f"ìƒì„¸ ì •ë³´ ìˆ˜ì§‘: {store.get('name')}")
                detailed_store = crawler.get_store_detail(store)
                detailed_stores.append(detailed_store)
                monitor.update_store_processed(True)
                
                # ê°€ê²Œ ê°„ íœ´ì‹ ì‹œê°„
                time.sleep(1)
                
            except Exception as e:
                logger.error(f"ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {store.get('name')} - {e}")
                monitor.update_store_processed(False)
                continue
        
        # ë°°ì¹˜ ê°„ íœ´ì‹ ì‹œê°„
        if i + batch_size < len(stores):
            logger.info(f"ë°°ì¹˜ ì™„ë£Œ. 3ì´ˆ íœ´ì‹...")
            time.sleep(3)
    
    return detailed_stores

def run_region_expansion():
    """ì§€ì—­ í™•ì¥ í¬ë¡¤ë§"""
    logger.info("=== ì§€ì—­ í™•ì¥ í¬ë¡¤ë§ ì‹œì‘ ===")
    
    # ëª¨ë“  ì§€ì—­ì— ëŒ€í•´ í¬ë¡¤ë§ ì‹¤í–‰
    for region_key, region_info in config.REGIONS.items():
        logger.info(f"=== {region_info['name']} ì§€ì—­ í¬ë¡¤ë§ ===")
        
        # ì„ì‹œë¡œ ì„¤ì • ë³€ê²½
        original_region = config.TEST_REGION
        original_rect = config.TEST_RECT
        original_keywords = config.TEST_KEYWORDS
        
        try:
            config.TEST_REGION = region_key
            config.TEST_RECT = region_info["rect"]
            config.TEST_KEYWORDS = region_info["keywords"]
            
            # í•´ë‹¹ ì§€ì—­ í¬ë¡¤ë§ ì‹¤í–‰
            run_enhanced_crawling()
            
        finally:
            # ì„¤ì • ë³µì›
            config.TEST_REGION = original_region
            config.TEST_RECT = original_rect
            config.TEST_KEYWORDS = original_keywords
        
        # ì§€ì—­ ê°„ íœ´ì‹ ì‹œê°„
        logger.info("ì§€ì—­ ì™„ë£Œ. 10ì´ˆ íœ´ì‹...")
        time.sleep(10)

def process_crawled_data(stores_data: List[Dict]) -> List[Dict]:
    """í¬ë¡¤ë§ëœ ë°ì´í„° ì •ì œ ë° ì²˜ë¦¬ (ìƒˆ ìŠ¤í‚¤ë§ˆ ë°˜ì˜)"""
    processed_stores = []
    
    for store in stores_data:
        # í•„ìˆ˜ í•„ë“œ ê²€ì¦
        if not store.get('diningcode_place_id') or not store.get('name'):
            logger.warning(f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {store}")
            continue
        
        # ë¬´í•œë¦¬í•„ ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸
        is_refill = False
        refill_keywords = ['ë¬´í•œë¦¬í•„', 'ë·”í˜', 'ë¬´ì œí•œ', 'ë¦¬í•„']
        
        # ì´ë¦„ì—ì„œ í™•ì¸
        for keyword in refill_keywords:
            if keyword in store.get('name', ''):
                is_refill = True
                break
        
        # ì¹´í…Œê³ ë¦¬ì—ì„œ í™•ì¸
        categories = store.get('raw_categories_diningcode', [])
        for category in categories:
            for keyword in refill_keywords:
                if keyword in category:
                    is_refill = True
                    break
        
        # ê²€ìƒ‰ í‚¤ì›Œë“œ ê¸°ë°˜ í™•ì¸
        search_keyword = store.get('keyword', '')
        if 'ë¬´í•œë¦¬í•„' in search_keyword:
            is_refill = True
        
        if not is_refill:
            logger.warning(f"ë¬´í•œë¦¬í•„ ê´€ë ¨ì„± ì—†ìŒ: {store.get('name')}")
            continue
        
        # ì¢Œí‘œ ìœ íš¨ì„± ê²€ì¦ (í•„ìˆ˜ í•„ë“œ)
        lat = store.get('position_lat')
        lng = store.get('position_lng')
        
        if not lat or not lng:
            logger.warning(f"ì¢Œí‘œ ì •ë³´ ì—†ìŒ: {store.get('name')}")
            continue
        
        try:
            lat = float(lat)
            lng = float(lng)
            # í•œêµ­ ì¢Œí‘œ ë²”ìœ„ í™•ì¸ (ëŒ€ëµì )
            if not (33 <= lat <= 39 and 124 <= lng <= 132):
                logger.warning(f"ì¢Œí‘œ ë²”ìœ„ ë²—ì–´ë‚¨: {store.get('name')} ({lat}, {lng})")
                continue
        except (ValueError, TypeError):
            logger.warning(f"ì¢Œí‘œ ë³€í™˜ ì‹¤íŒ¨: {store.get('name')}")
            continue
        
        # í‰ì  ì²˜ë¦¬ (diningcode_rating í•„ë“œ ì‚¬ìš©)
        rating = store.get('diningcode_rating')
        if rating:
            try:
                rating = float(rating)
                if not (0 <= rating <= 5):
                    rating = None
            except (ValueError, TypeError):
                rating = None
        
        # ì •ì œëœ ë°ì´í„° êµ¬ì„± (ìƒˆ ìŠ¤í‚¤ë§ˆì— ë§ì¶¤)
        processed_store = {
            'name': store.get('name', '').strip(),
            'address': store.get('address', '').strip(),
            'description': store.get('description', '').strip(),
            'position_lat': lat,
            'position_lng': lng,
            'position_x': None,  # ì¹´ì¹´ì˜¤ë§µ ì¢Œí‘œ (ë‚˜ì¤‘ì— ì¶”ê°€)
            'position_y': None,  # ì¹´ì¹´ì˜¤ë§µ ì¢Œí‘œ (ë‚˜ì¤‘ì— ì¶”ê°€)
            'naver_rating': None,  # ë„¤ì´ë²„ í‰ì  (ë‚˜ì¤‘ì— ì¶”ê°€)
            'kakao_rating': None,  # ì¹´ì¹´ì˜¤ í‰ì  (ë‚˜ì¤‘ì— ì¶”ê°€)
            'diningcode_rating': rating,
            'open_hours': store.get('open_hours', ''),
            'open_hours_raw': store.get('open_hours_raw', ''),
            'price': store.get('price'),
            'refill_items': store.get('refill_items', []),
            'image_urls': store.get('image_urls', []),
            'phone_number': store.get('phone_number', '').strip(),
            'diningcode_place_id': store.get('diningcode_place_id'),
            'raw_categories_diningcode': categories,
            'status': 'ìš´ì˜ì¤‘'
        }
        
        processed_stores.append(processed_store)
    
    logger.info(f"ë°ì´í„° ì •ì œ ì™„ë£Œ: {len(stores_data)} -> {len(processed_stores)}")
    return processed_stores

def run_mvp_crawling():
    """MVP í¬ë¡¤ë§ ì‹¤í–‰ (ì§€ì—­ë³„ í‚¤ì›Œë“œ ì‚¬ìš©)"""
    crawler = None
    db = None
    
    try:
        logger.info("=== Refill Spot í¬ë¡¤ë§ ì‹œì‘ (MVP - ì§€ì—­ë³„ í‚¤ì›Œë“œ) ===")
        
        # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        crawler = DiningCodeCrawler()
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        db = DatabaseManager()
        if not db.test_connection():
            logger.error("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨")
            return
        
        db.create_tables()
        
        all_stores = []
        
        # ì§€ì—­ë³„ í‚¤ì›Œë“œë¡œ í¬ë¡¤ë§
        region_name = config.REGIONS[config.TEST_REGION]["name"]
        keywords = config.TEST_KEYWORDS
        rect = config.TEST_RECT
        
        logger.info(f"=== {region_name} ì§€ì—­ í¬ë¡¤ë§ ì‹œì‘ ===")
        logger.info(f"ì‚¬ìš©í•  í‚¤ì›Œë“œ: {keywords}")
        logger.info(f"ê²€ìƒ‰ ì˜ì—­: {rect}")
        
        # ê° í‚¤ì›Œë“œë³„ë¡œ í¬ë¡¤ë§ (MVPì—ì„œëŠ” ì²˜ìŒ 2ê°œë§Œ)
        for keyword in keywords[:2]:  # MVPì—ì„œëŠ” ì²˜ìŒ 2ê°œ í‚¤ì›Œë“œë§Œ
            logger.info(f"í‚¤ì›Œë“œ '{keyword}' í¬ë¡¤ë§ ì‹œì‘")
            
            # ëª©ë¡ ìˆ˜ì§‘
            stores = crawler.get_store_list(keyword, rect)
            logger.info(f"í‚¤ì›Œë“œ '{keyword}': {len(stores)}ê°œ ê°€ê²Œ ë°œê²¬")
            
            if not stores:
                logger.warning(f"í‚¤ì›Œë“œ '{keyword}'ë¡œ ê²€ìƒ‰ëœ ê°€ê²Œê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.")
                continue
            
            # ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ (MVPì—ì„œëŠ” ì²˜ìŒ 3ê°œë§Œ)
            detailed_stores = []
            for i, store in enumerate(stores[:3]):  
                try:
                    logger.info(f"ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ {i+1}/{min(3, len(stores))}: {store.get('name')}")
                    detailed_store = crawler.get_store_detail(store)
                    detailed_stores.append(detailed_store)
                    
                except Exception as e:
                    logger.error(f"ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {store.get('name')} - {e}")
                    continue
            
            all_stores.extend(detailed_stores)
            
            # í‚¤ì›Œë“œë³„ ê²°ê³¼ ìš”ì•½
            logger.info(f"í‚¤ì›Œë“œ '{keyword}' ì™„ë£Œ: {len(detailed_stores)}ê°œ ê°€ê²Œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘")
        
        if not all_stores:
            logger.warning("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            logger.info("ë‹¤ìŒì„ í™•ì¸í•´ë³´ì„¸ìš”:")
            logger.info("1. ë‹¤ì´ë‹ì½”ë“œ ì‚¬ì´íŠ¸ ì ‘ì† ê°€ëŠ¥ ì—¬ë¶€")
            logger.info("2. ChromeDriver ì •ìƒ ë™ì‘ ì—¬ë¶€") 
            logger.info("3. ë„¤íŠ¸ì›Œí¬ ì—°ê²° ìƒíƒœ")
            return
        
        # ë°ì´í„° ì •ì œ
        processed_stores = process_crawled_data(all_stores)
        
        if not processed_stores:
            logger.warning("ì •ì œ í›„ ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return
        
        # CSV ì €ì¥
        df = pd.DataFrame(processed_stores)
        df.to_csv('mvp_crawling_result.csv', index=False, encoding='utf-8-sig')
        logger.info(f"CSV ì €ì¥ ì™„ë£Œ: mvp_crawling_result.csv ({len(processed_stores)}ê°œ ê°€ê²Œ)")
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ (ì§€ì—­ëª…ê³¼ rect ì •ë³´ í¬í•¨)
        db.save_crawled_data(processed_stores, keyword=region_name, rect_area=rect)
        logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ")
        
        # ê²°ê³¼ ìš”ì•½
        logger.info("=== í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½ ===")
        logger.info(f"ê²€ìƒ‰ ì§€ì—­: {region_name}")
        logger.info(f"ì´ ìˆ˜ì§‘ ê°€ê²Œ ìˆ˜: {len(processed_stores)}")
        logger.info(f"ì¢Œí‘œ ìˆëŠ” ê°€ê²Œ: {sum(1 for s in processed_stores if s['position_lat'])}")
        logger.info(f"ì „í™”ë²ˆí˜¸ ìˆëŠ” ê°€ê²Œ: {sum(1 for s in processed_stores if s['phone_number'])}")
        logger.info(f"ë‹¤ì´ë‹ì½”ë“œ í‰ì  ìˆëŠ” ê°€ê²Œ: {sum(1 for s in processed_stores if s['diningcode_rating'])}")
        
        # í‰ì  í†µê³„
        ratings = [s['diningcode_rating'] for s in processed_stores if s['diningcode_rating']]
        if ratings:
            avg_rating = sum(ratings) / len(ratings)
            logger.info(f"í‰ê·  ë‹¤ì´ë‹ì½”ë“œ í‰ì : {avg_rating:.2f}")
        
        # ì¹´í…Œê³ ë¦¬ ë¶„í¬
        all_categories = []
        for store in processed_stores:
            all_categories.extend(store.get('raw_categories_diningcode', []))
        
        category_count = Counter(all_categories)
        logger.info("ì£¼ìš” ì¹´í…Œê³ ë¦¬:")
        for category, count in category_count.most_common(10):
            logger.info(f"  {category}: {count}ê°œ")
        
        # ë°ì´í„°ë² ì´ìŠ¤ í†µê³„
        stats = db.get_crawling_stats()
        logger.info("=== ë°ì´í„°ë² ì´ìŠ¤ ì „ì²´ í†µê³„ ===")
        for key, value in stats.items():
            logger.info(f"{key}: {value}")
        
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise
        
    finally:
        if crawler:
            crawler.close()
        if db:
            db.close()
        logger.info("=== í¬ë¡¤ë§ ì™„ë£Œ ===")

def test_single_store_enhanced():
    """ë‹¨ì¼ ê°€ê²Œ í…ŒìŠ¤íŠ¸ (ê°•í™”ëœ ë²„ì „)"""
    crawler = None
    
    try:
        logger.info("=== ë‹¨ì¼ ê°€ê²Œ í…ŒìŠ¤íŠ¸ (ê°•í™”ëœ ë²„ì „) ===")
        crawler = DiningCodeCrawler()
        
        # ì§€ì—­ëª…ì„ í¬í•¨í•œ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
        region_name = config.REGIONS[config.TEST_REGION]["name"]
        test_keyword = f"{region_name} ë¬´í•œë¦¬í•„"
        logger.info(f"í…ŒìŠ¤íŠ¸ í‚¤ì›Œë“œ: {test_keyword}")
        
        # ëª©ë¡ì—ì„œ ì²« ë²ˆì§¸ ê°€ê²Œë§Œ
        stores = crawler.get_store_list(test_keyword, config.TEST_RECT)
        
        if not stores:
            # ë°±ì—… í‚¤ì›Œë“œë¡œ ì¬ì‹œë„
            backup_keyword = "ê°•ë‚¨ ê³ ê¸°ë¬´í•œë¦¬í•„"
            logger.info(f"ë°±ì—… í‚¤ì›Œë“œë¡œ ì¬ì‹œë„: {backup_keyword}")
            stores = crawler.get_store_list(backup_keyword, config.TEST_RECT)
        
        if stores:
            first_store = stores[0]
            logger.info(f"í…ŒìŠ¤íŠ¸ ëŒ€ìƒ: {first_store.get('name')}")
            logger.info(f"ê°€ê²Œ ID: {first_store.get('diningcode_place_id')}")
            
            detailed_store = crawler.get_store_detail(first_store)
            
            logger.info("=== ê°•í™”ëœ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===")
            
            # ê¸°ë³¸ ì •ë³´
            logger.info("ğŸ“ ê¸°ë³¸ ì •ë³´:")
            logger.info(f"  ì´ë¦„: {detailed_store.get('name')}")
            logger.info(f"  ì£¼ì†Œ: {detailed_store.get('address')}")
            logger.info(f"  ì „í™”ë²ˆí˜¸: {detailed_store.get('phone_number')}")
            logger.info(f"  í‰ì : {detailed_store.get('diningcode_rating')}")
            
            # ë¬´í•œë¦¬í•„ ì •ë³´
            logger.info("ğŸ½ï¸ ë¬´í•œë¦¬í•„ ì •ë³´:")
            logger.info(f"  í™•ì • ì—¬ë¶€: {detailed_store.get('is_confirmed_refill')}")
            logger.info(f"  ë¦¬í•„ íƒ€ì…: {detailed_store.get('refill_type')}")
            logger.info(f"  ë¦¬í•„ ì•„ì´í…œ: {detailed_store.get('refill_items', [])}")
            
            # ë©”ë‰´ ì •ë³´
            menu_items = detailed_store.get('menu_items', [])
            signature_menu = detailed_store.get('signature_menu', [])
            logger.info("ğŸ´ ë©”ë‰´ ì •ë³´:")
            logger.info(f"  ë©”ë‰´ ê°œìˆ˜: {len(menu_items)}")
            logger.info(f"  ëŒ€í‘œ ë©”ë‰´: {signature_menu}")
            
            # ê°€ê²© ì •ë³´
            logger.info("ğŸ’° ê°€ê²© ì •ë³´:")
            logger.info(f"  ê°€ê²© ë²”ìœ„: {detailed_store.get('price_range')}")
            logger.info(f"  í‰ê·  ê°€ê²©: {detailed_store.get('average_price')}")
            
            # ì˜ì—…ì‹œê°„ ì •ë³´
            logger.info("ğŸ• ì˜ì—…ì‹œê°„ ì •ë³´:")
            logger.info(f"  ì˜ì—…ì‹œê°„: {detailed_store.get('open_hours')}")
            logger.info(f"  ë¸Œë ˆì´í¬íƒ€ì„: {detailed_store.get('break_time')}")
            logger.info(f"  íœ´ë¬´ì¼: {detailed_store.get('holiday')}")
            
            # ì´ë¯¸ì§€ ì •ë³´
            image_urls = detailed_store.get('image_urls', [])
            menu_images = detailed_store.get('menu_images', [])
            logger.info("ğŸ“¸ ì´ë¯¸ì§€ ì •ë³´:")
            logger.info(f"  ì´ ì´ë¯¸ì§€: {len(image_urls)}ê°œ")
            logger.info(f"  ë©”ë‰´ ì´ë¯¸ì§€: {len(menu_images)}ê°œ")
            
            # í‚¤ì›Œë“œ ì •ë³´
            keywords = detailed_store.get('keywords', [])
            logger.info("ğŸ·ï¸ í‚¤ì›Œë“œ:")
            logger.info(f"  {keywords}")
            
        else:
            logger.warning("í…ŒìŠ¤íŠ¸í•  ê°€ê²Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            logger.info("ë‹¤ìŒì„ í™•ì¸í•´ë³´ì„¸ìš”:")
            logger.info("1. ë„¤íŠ¸ì›Œí¬ ì—°ê²° ìƒíƒœ")
            logger.info("2. ë‹¤ì´ë‹ì½”ë“œ ì‚¬ì´íŠ¸ ì ‘ê·¼ ê°€ëŠ¥ ì—¬ë¶€")
            logger.info("3. ê²€ìƒ‰ í‚¤ì›Œë“œ ë° ì§€ì—­ ì„¤ì •")
            
    except Exception as e:
        logger.error(f"ë‹¨ì¼ ê°€ê²Œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    finally:
        if crawler:
            crawler.close()

def show_database_stats():
    """ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì¡°íšŒ ë° ì¶œë ¥"""
    db = None
    
    try:
        logger.info("=== ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì¡°íšŒ ===")
        db = DatabaseManager()
        
        if not db.test_connection():
            logger.error("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨")
            return
        
        stats = db.get_enhanced_crawling_stats()
        
        if not stats:
            logger.warning("í†µê³„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ê¸°ë³¸ í†µê³„
        basic = stats.get('basic_stats', {})
        logger.info("ğŸ“Š ê¸°ë³¸ í†µê³„:")
        logger.info(f"  ì´ ê°€ê²Œ ìˆ˜: {basic.get('total_stores', 0):,}ê°œ")
        logger.info(f"  ë¬´í•œë¦¬í•„ í™•ì •: {basic.get('confirmed_refill_stores', 0):,}ê°œ")
        logger.info(f"  ë©”ë‰´ ì •ë³´ ë³´ìœ : {basic.get('stores_with_menu', 0):,}ê°œ")
        logger.info(f"  ì´ë¯¸ì§€ ë³´ìœ : {basic.get('stores_with_images', 0):,}ê°œ")
        logger.info(f"  ê°€ê²© ì •ë³´ ë³´ìœ : {basic.get('stores_with_price', 0):,}ê°œ")
        logger.info(f"  í‰ê·  í‰ì : {basic.get('avg_rating', 0):.2f}/5.0")
        
        # ë¦¬í•„ íƒ€ì…ë³„ í†µê³„
        refill_types = stats.get('refill_type_stats', [])
        if refill_types:
            logger.info("ğŸ½ï¸ ë¦¬í•„ íƒ€ì…ë³„ í†µê³„:")
            for item in refill_types[:5]:  # ìƒìœ„ 5ê°œë§Œ
                logger.info(f"  {item['refill_type']}: {item['count']}ê°œ")
        
        # ì§€ì—­ë³„ í†µê³„
        regions = stats.get('region_stats', [])
        if regions:
            logger.info("ğŸ“ ì§€ì—­ë³„ í†µê³„:")
            for item in regions:
                logger.info(f"  {item['region']}: {item['count']}ê°œ")
        
        # ìµœê·¼ í¬ë¡¤ë§ ì„¸ì…˜
        recent = stats.get('recent_sessions', [])
        if recent:
            logger.info("ğŸ“… ìµœê·¼ í¬ë¡¤ë§ ì„¸ì…˜:")
            for session in recent[:3]:  # ìµœê·¼ 3ê°œë§Œ
                created_at = session['created_at'].strftime('%Y-%m-%d %H:%M')
                logger.info(f"  {created_at} | {session['keyword']} | {session['stores_processed']}ê°œ ì²˜ë¦¬")
        
    except Exception as e:
        logger.error(f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
    finally:
        if db:
            db.close()

def test_stage3_enhancement():
    """3ë‹¨ê³„ ê³ ë„í™” ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    logger.info("=== 3ë‹¨ê³„ ê³ ë„í™” ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    try:
        # í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„±
        test_stores = [
            {
                'name': 'ë§›ìˆëŠ” ì‚¼ê²¹ì‚´ì§‘',
                'address': 'ì„œìš¸ ê°•ë‚¨êµ¬ í…Œí—¤ë€ë¡œ 123',
                'price': '1ë§Œ5ì²œì›',
                'raw_categories_diningcode': ['#ì‚¼ê²¹ì‚´ë¬´í•œë¦¬í•„', '#ê³ ê¸°', '#ê°•ë‚¨ë§›ì§‘'],
                'diningcode_place_id': 'test1',
                'menu_items': ['ì‚¼ê²¹ì‚´', 'ëª©ì‚´', 'ê°ˆë¹„ì‚´']
            },
            {
                'name': 'ë§›ìˆëŠ”ì‚¼ê²¹ì‚´ì§‘',  # ì¤‘ë³µ (ë„ì–´ì“°ê¸° ì°¨ì´)
                'address': 'ì„œìš¸ ê°•ë‚¨êµ¬ í…Œí—¤ë€ë¡œ 125',
                'price': '15000ì›',
                'raw_categories_diningcode': ['#ë¬´í•œë¦¬í•„', '#ì‚¼ê²¹ì‚´'],
                'diningcode_place_id': 'test2',
                'phone_number': '02-123-4567'
            },
            {
                'name': 'ì´ˆë°¥ë·”í˜ ìŠ¤ì‹œë¡œ',
                'address': 'ì„œìš¸ ê°•ë‚¨êµ¬ ì—­ì‚¼ë™',  # ì¢Œí‘œ ì—†ìŒ
                'price': 'ëŸ°ì¹˜ 2ë§Œì›, ë””ë„ˆ 3ë§Œì›',
                'raw_categories_diningcode': ['#ì´ˆë°¥ë·”í˜', '#ì¼ì‹', '#ë·”í˜'],
                'diningcode_place_id': 'test3',
                'menu_items': ['ì´ˆë°¥', 'ì‚¬ì‹œë¯¸', 'ìš°ë™']
            },
            {
                'name': 'ê³ ê¸°ì²œêµ­',
                'address': 'ì„œìš¸ ê°•ë‚¨êµ¬ ë…¼í˜„ë™',
                'price': '2ë§Œì›ëŒ€',
                'raw_categories_diningcode': ['#ì†Œê³ ê¸°ë¬´í•œë¦¬í•„', '#í•œìš°', '#êµ¬ì´'],
                'diningcode_place_id': 'test4',
                'position_lat': 37.5129,
                'position_lng': 127.0426
            }
        ]
        
        # ë°ì´í„° ê°•í™” ì‹¤í–‰
        from data_enhancement import DataEnhancer
        enhancer = DataEnhancer()
        
        logger.info(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_stores)}ê°œ ê°€ê²Œ")
        
        enhanced_stores, stats = enhancer.enhance_stores_data(test_stores)
        
        # ê²°ê³¼ ì¶œë ¥
        logger.info("=== ê°•í™” ê²°ê³¼ ===")
        logger.info(f"ì›ë³¸ ê°€ê²Œ ìˆ˜: {stats.total_stores}")
        logger.info(f"ìµœì¢… ê°€ê²Œ ìˆ˜: {len(enhanced_stores)}")
        logger.info(f"ì§€ì˜¤ì½”ë”© ì„±ê³µ: {stats.geocoding_success}")
        logger.info(f"ê°€ê²© ì •ê·œí™”: {stats.price_normalized}")
        logger.info(f"ì¹´í…Œê³ ë¦¬ ë§¤í•‘: {stats.categories_mapped}")
        logger.info(f"ì¤‘ë³µ ì œê±°: {stats.duplicates_removed}")
        
        # ê°œë³„ ê°€ê²Œ ê²°ê³¼ í™•ì¸
        for i, store in enumerate(enhanced_stores):
            logger.info(f"\n=== ê°€ê²Œ {i+1}: {store.get('name')} ===")
            logger.info(f"ì£¼ì†Œ: {store.get('address')}")
            logger.info(f"ì¢Œí‘œ: {store.get('position_lat')}, {store.get('position_lng')}")
            
            # ì§€ì˜¤ì½”ë”© ì •ë³´
            if store.get('geocoding_source'):
                logger.info(f"ì§€ì˜¤ì½”ë”© ì†ŒìŠ¤: {store.get('geocoding_source')}")
                logger.info(f"ì§€ì˜¤ì½”ë”© ì‹ ë¢°ë„: {store.get('geocoding_confidence', 0):.2f}")
            
            # ì •ê·œí™”ëœ ê°€ê²© ì •ë³´
            norm_price = store.get('normalized_price', {})
            if norm_price:
                logger.info(f"ê°€ê²© íƒ€ì…: {norm_price.get('price_type')}")
                logger.info(f"ê°€ê²© ë²”ìœ„: {norm_price.get('min_price')} ~ {norm_price.get('max_price')}")
                logger.info(f"ê°€ê²© ì‹ ë¢°ë„: {norm_price.get('confidence', 0):.2f}")
                if norm_price.get('time_based'):
                    logger.info(f"ì‹œê°„ëŒ€ë³„ ê°€ê²©: {norm_price.get('time_based')}")
                if norm_price.get('conditions'):
                    logger.info(f"ê°€ê²© ì¡°ê±´: {norm_price.get('conditions')}")
            
            # í‘œì¤€ ì¹´í…Œê³ ë¦¬
            std_categories = store.get('standard_categories', [])
            if std_categories:
                logger.info(f"í‘œì¤€ ì¹´í…Œê³ ë¦¬: {std_categories}")
        
        # ê°•í™” í†µê³„ ìƒì„¸ ì •ë³´
        summary = enhancer.get_enhancement_summary()
        logger.info("\n=== ê°•í™” í†µê³„ ìƒì„¸ ===")
        logger.info(f"ì¢Œí‘œ ì™„ì„±ë„: {summary.get('geocoding_rate', 0):.1f}%")
        logger.info(f"ê°€ê²© ì •ê·œí™”ìœ¨: {summary.get('price_normalization_rate', 0):.1f}%")
        logger.info(f"ì¹´í…Œê³ ë¦¬ ë§¤í•‘ìœ¨: {summary.get('category_mapping_rate', 0):.1f}%")
        logger.info(f"ì¤‘ë³µ ì œê±°ìœ¨: {summary.get('duplicate_removal_rate', 0):.1f}%")
        
        logger.info("=== 3ë‹¨ê³„ ê³ ë„í™” ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===")
        return enhanced_stores
        
    except Exception as e:
        logger.error(f"3ë‹¨ê³„ ê³ ë„í™” í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
        return []

def run_stage3_crawling():
    """3ë‹¨ê³„ ê³ ë„í™” í¬ë¡¤ë§ ì‹¤í–‰ (ì¹´ì¹´ì˜¤ API ì‚¬ìš©)"""
    logger.info("=== 3ë‹¨ê³„ ê³ ë„í™” í¬ë¡¤ë§ ì‹¤í–‰ ===")
    
    # API í‚¤ í™•ì¸
    if not config.KAKAO_API_KEY:
        logger.warning("ì¹´ì¹´ì˜¤ ì§€ì˜¤ì½”ë”© API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        logger.info("ì¹´ì¹´ì˜¤ API í‚¤ë¥¼ config.pyì— ì„¤ì •í•´ì£¼ì„¸ìš”.")
        logger.info("API í‚¤ ì—†ì´ë„ ê°€ê²© ì •ê·œí™”, ì¹´í…Œê³ ë¦¬ ë§¤í•‘, ì¤‘ë³µ ì œê±°ëŠ” ë™ì‘í•©ë‹ˆë‹¤.")
    
    # ê°•í™”ëœ í¬ë¡¤ë§ ì‹¤í–‰
    return run_enhanced_crawling()

def run_stage4_seoul_coverage():
    """4ë‹¨ê³„: ì„œìš¸ 25ê°œ êµ¬ ì™„ì „ ì»¤ë²„ë¦¬ì§€ í¬ë¡¤ë§"""
    logger.info("=== 4ë‹¨ê³„: ì„œìš¸ ì™„ì „ ì»¤ë²„ë¦¬ì§€ í¬ë¡¤ë§ ì‹œì‘ ===")
    
    try:
        from seoul_districts import SeoulDistrictManager
        from seoul_scheduler import SeoulCrawlingScheduler
        
        # ì„œìš¸ êµ¬ ê´€ë¦¬ì ì´ˆê¸°í™”
        district_manager = SeoulDistrictManager()
        
        # ì„œìš¸ ì»¤ë²„ë¦¬ì§€ í˜„í™© í™•ì¸
        stats = district_manager.get_seoul_coverage_stats()
        logger.info(f"ì„œìš¸ ì»¤ë²„ë¦¬ì§€ í˜„í™©:")
        logger.info(f"  ì´ êµ¬ ìˆ˜: {stats['total_districts']}")
        logger.info(f"  ì™„ë£Œìœ¨: {stats['completion_rate']:.1f}%")
        logger.info(f"  ì˜ˆìƒ ì´ ê°€ê²Œ ìˆ˜: {stats['total_expected_stores']:,}ê°œ")
        
        # ë¯¸ì™„ë£Œ êµ¬ ëª©ë¡
        incomplete_districts = district_manager.get_incomplete_districts()
        logger.info(f"ë¯¸ì™„ë£Œ êµ¬: {len(incomplete_districts)}ê°œ")
        
        if not incomplete_districts:
            logger.info("ëª¨ë“  êµ¬ì˜ í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            return
        
        # ìš°ì„ ìˆœìœ„ë³„ ì²˜ë¦¬
        for priority in range(1, 6):
            priority_districts = [d for d in incomplete_districts if d.priority == priority]
            if not priority_districts:
                continue
                
            logger.info(f"=== Tier {priority} êµ¬ ì²˜ë¦¬ ì‹œì‘ ===")
            
            for district in priority_districts:
                logger.info(f"{district.name} í¬ë¡¤ë§ ì‹œì‘...")
                
                # êµ¬ë³„ í¬ë¡¤ë§ ì‹¤í–‰
                result = run_district_crawling(district)
                
                if result['success']:
                    district_manager.update_district_status(
                        district.name, "ì™„ë£Œ", result['stores_processed']
                    )
                    logger.info(f"{district.name} ì™„ë£Œ: {result['stores_processed']}ê°œ ê°€ê²Œ")
                else:
                    district_manager.update_district_status(district.name, "ì˜¤ë¥˜")
                    logger.error(f"{district.name} ì‹¤íŒ¨: {result['error']}")
                
                # êµ¬ê°„ íœ´ì‹ (API ë¶€í•˜ ë°©ì§€)
                time.sleep(30)
        
        # ìµœì¢… í†µê³„
        final_stats = district_manager.get_seoul_coverage_stats()
        logger.info(f"=== 4ë‹¨ê³„ í¬ë¡¤ë§ ì™„ë£Œ ===")
        logger.info(f"ìµœì¢… ì™„ë£Œìœ¨: {final_stats['completion_rate']:.1f}%")
        logger.info(f"ì´ ì²˜ë¦¬ ê°€ê²Œ: {final_stats['total_expected_stores']:,}ê°œ")
        
    except Exception as e:
        logger.error(f"4ë‹¨ê³„ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        raise

def run_district_crawling(district_info) -> Dict:
    """ê°œë³„ êµ¬ í¬ë¡¤ë§ ì‹¤í–‰"""
    try:
        # ê¸°ì¡´ ì„¤ì • ë°±ì—…
        original_region = getattr(config, 'TEST_REGION', None)
        original_rect = getattr(config, 'TEST_RECT', None)
        original_keywords = getattr(config, 'TEST_KEYWORDS', None)
        
        # êµ¬ë³„ ì„¤ì •ìœ¼ë¡œ ë³€ê²½
        config.TEST_REGION = district_info.name
        config.TEST_RECT = district_info.rect
        config.TEST_KEYWORDS = district_info.keywords
        
        logger.info(f"{district_info.name} ì„¤ì •:")
        logger.info(f"  ê²€ìƒ‰ ì˜ì—­: {district_info.rect}")
        logger.info(f"  í‚¤ì›Œë“œ ìˆ˜: {len(district_info.keywords)}")
        logger.info(f"  ì˜ˆìƒ ê°€ê²Œ: {district_info.expected_stores}ê°œ")
        
        # í¬ë¡¤ë§ ì‹¤í–‰
        stores = run_enhanced_crawling()
        
        return {
            'success': True,
            'stores_found': len(stores) if stores else 0,
            'stores_processed': len(stores) if stores else 0,
            'district': district_info.name
        }
        
    except Exception as e:
        return {
            'success': False,
            'error': str(e),
            'district': district_info.name
        }
        
    finally:
        # ì„¤ì • ë³µì›
        if original_region:
            config.TEST_REGION = original_region
        if original_rect:
            config.TEST_RECT = original_rect
        if original_keywords:
            config.TEST_KEYWORDS = original_keywords

def start_seoul_scheduler():
    """ì„œìš¸ ìë™ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘"""
    logger.info("=== ì„œìš¸ ìë™ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ ===")
    
    try:
        from seoul_districts import SeoulDistrictManager
        from seoul_scheduler import SeoulCrawlingScheduler
        
        # êµ¬ ê´€ë¦¬ì ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™”
        district_manager = SeoulDistrictManager()
        scheduler = SeoulCrawlingScheduler(district_manager)
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ (ë¬´í•œ ë£¨í”„)
        scheduler.start_scheduler()
        
    except KeyboardInterrupt:
        logger.info("ì‚¬ìš©ìì— ì˜í•´ ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"ìŠ¤ì¼€ì¤„ëŸ¬ ì˜¤ë¥˜: {e}")
        raise

def test_stage4_system():
    """4ë‹¨ê³„ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    logger.info("=== 4ë‹¨ê³„ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ===")
    
    try:
        from seoul_districts import SeoulDistrictManager, test_seoul_district_system
        from seoul_scheduler import test_seoul_scheduler
        
        # 1. ì„œìš¸ êµ¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
        logger.info("1. ì„œìš¸ êµ¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
        test_seoul_district_system()
        
        # 2. ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
        logger.info("2. ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
        test_seoul_scheduler()
        
        # 3. ë‹¨ì¼ êµ¬ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸
        logger.info("3. ë‹¨ì¼ êµ¬ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸")
        district_manager = SeoulDistrictManager()
        test_district = district_manager.get_district_info("ê°•ë‚¨êµ¬")
        
        if test_district:
            result = run_district_crawling(test_district)
            logger.info(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼: {result}")
        
        logger.info("4ë‹¨ê³„ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"4ë‹¨ê³„ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")

def show_seoul_coverage_dashboard():
    """ì„œìš¸ ì»¤ë²„ë¦¬ì§€ ëŒ€ì‹œë³´ë“œ í‘œì‹œ"""
    try:
        from seoul_districts import SeoulDistrictManager
        
        district_manager = SeoulDistrictManager()
        stats = district_manager.get_seoul_coverage_stats()
        
        print("\n" + "="*60)
        print("ğŸ—ºï¸  ì„œìš¸ 25ê°œ êµ¬ ì»¤ë²„ë¦¬ì§€ ëŒ€ì‹œë³´ë“œ")
        print("="*60)
        
        print(f"ğŸ“Š ì „ì²´ í˜„í™©:")
        print(f"   ì´ êµ¬ ìˆ˜: {stats['total_districts']}ê°œ")
        print(f"   ì™„ë£Œ: {stats['completed']}ê°œ")
        print(f"   ì§„í–‰ì¤‘: {stats['in_progress']}ê°œ")
        print(f"   ëŒ€ê¸°: {stats['pending']}ê°œ")
        print(f"   ì˜¤ë¥˜: {stats['error']}ê°œ")
        print(f"   ì™„ë£Œìœ¨: {stats['completion_rate']:.1f}%")
        print(f"   ì˜ˆìƒ ì´ ê°€ê²Œ: {stats['total_expected_stores']:,}ê°œ")
        
        print(f"\nğŸ† í‹°ì–´ë³„ í˜„í™©:")
        for tier, info in stats['tier_breakdown'].items():
            tier_num = tier.split('_')[1]
            print(f"   Tier {tier_num}: {info['completed']}/{info['count']}ê°œ ì™„ë£Œ, ì˜ˆìƒ {info['expected_stores']}ê°œ ê°€ê²Œ")
            print(f"     êµ¬ ëª©ë¡: {', '.join(info['districts'])}")
        
        # ë¯¸ì™„ë£Œ êµ¬ ëª©ë¡
        incomplete = district_manager.get_incomplete_districts()
        if incomplete:
            print(f"\nâ³ ë¯¸ì™„ë£Œ êµ¬ ({len(incomplete)}ê°œ):")
            for district in incomplete[:10]:  # ìµœëŒ€ 10ê°œë§Œ
                print(f"   {district.name}: {district.status}, ì˜ˆìƒ {district.expected_stores}ê°œ")
        
        print("="*60)
        
    except Exception as e:
        logger.error(f"ëŒ€ì‹œë³´ë“œ í‘œì‹œ ì˜¤ë¥˜: {e}")

def main():
    """ë©”ì¸ í¬ë¡¤ë§ í•¨ìˆ˜"""
    logger.info("=== ë¦¬í•„ìŠ¤íŒŸ í¬ë¡¤ëŸ¬ ì‹œì‘ ===")
    try:
        # ê¸°ë³¸ì ìœ¼ë¡œ ì„œìš¸ ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰
        return run_stage4_seoul_coverage()
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        return False

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        if command == "mvp":
            run_mvp_crawling()
        elif command == "enhanced":
            run_enhanced_crawling()
        elif command == "expansion":
            run_region_expansion()
        elif command == "stage3":
            run_stage3_crawling()
        elif command == "stage4":
            run_stage4_seoul_coverage()
        elif command == "seoul-scheduler":
            start_seoul_scheduler()
        elif command == "test-stage4":
            test_stage4_system()
        elif command == "seoul-dashboard":
            show_seoul_coverage_dashboard()
        elif command == "stats":
            show_database_stats()
        elif command == "test-single":
            test_single_store_enhanced()
        elif command == "test-stage3":
            test_stage3_enhancement()
        else:
            print("ì‚¬ìš©ë²•:")
            print("  python main.py mvp              # MVP í¬ë¡¤ë§")
            print("  python main.py enhanced         # ê°•í™”ëœ í¬ë¡¤ë§")
            print("  python main.py expansion        # ì§€ì—­ í™•ì¥ í¬ë¡¤ë§")
            print("  python main.py stage3           # 3ë‹¨ê³„ ê³ ë„í™” í¬ë¡¤ë§")
            print("  python main.py stage4           # 4ë‹¨ê³„ ì„œìš¸ ì™„ì „ ì»¤ë²„ë¦¬ì§€")
            print("  python main.py seoul-scheduler  # ì„œìš¸ ìë™ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘")
            print("  python main.py test-stage4      # 4ë‹¨ê³„ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
            print("  python main.py seoul-dashboard  # ì„œìš¸ ì»¤ë²„ë¦¬ì§€ ëŒ€ì‹œë³´ë“œ")
            print("  python main.py stats            # ë°ì´í„°ë² ì´ìŠ¤ í†µê³„")
            print("  python main.py test-single      # ë‹¨ì¼ ê°€ê²Œ í…ŒìŠ¤íŠ¸")
            print("  python main.py test-stage3      # 3ë‹¨ê³„ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
    else:
        # ê¸°ë³¸ ì‹¤í–‰: ê°•í™”ëœ í¬ë¡¤ë§
        run_enhanced_crawling()