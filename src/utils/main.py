"""
ë¦¬í•„ìŠ¤íŒŸ í¬ë¡¤ëŸ¬ ë©”ì¸ ì‹¤í–‰ ëª¨ë“ˆ (ê°„ì†Œí™”ëœ ë²„ì „)
ì•ì„œ ê°œì„ í•œ ì£¼ì†Œ, ì˜ì—…ì‹œê°„, break_time, last_order ìˆ˜ì§‘ ê¸°ëŠ¥ ì ìš©
"""

import sys
import os
import logging
from datetime import datetime
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'logs/main_crawler_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def run_gangnam_test():
    """ê°•ë‚¨ ì§€ì—­ í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§ (ê°œì„ ëœ ê¸°ëŠ¥ í™•ì¸ìš©)"""
    try:
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs('logs', exist_ok=True)
        
        logger.info("ğŸš€ ê°•ë‚¨ ì§€ì—­ í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§ ì‹œì‘")
        logger.info("âœ¨ ê°œì„ ëœ ê¸°ëŠ¥: ì£¼ì†Œ ì¶”ì¶œ, ì˜ì—…ì‹œê°„, break_time, last_order ìˆ˜ì§‘")
        
        # í¬ë¡¤ëŸ¬ ì„í¬íŠ¸ ë° ì‹¤í–‰
        from src.core.crawler import DiningCodeCrawler
        from src.core.database import DatabaseManager
        
        # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        crawler = DiningCodeCrawler()
        db = DatabaseManager()
        
        # ê°•ë‚¨ ì§€ì—­ í¬ë¡¤ë§ ì„¤ì •
        keyword = "ì„œìš¸ ê°•ë‚¨ ë¬´í•œë¦¬í•„"
        rect = "37.4979,127.0276,37.5279,127.0576"  # ê°•ë‚¨ ì§€ì—­ ì¢Œí‘œ
        
        logger.info(f"ê²€ìƒ‰ í‚¤ì›Œë“œ: {keyword}")
        logger.info(f"ê²€ìƒ‰ ì˜ì—­: {rect}")
        
        # ê°€ê²Œ ëª©ë¡ ìˆ˜ì§‘
        stores = crawler.get_store_list(keyword, rect)
        logger.info(f"ìˆ˜ì§‘ëœ ê°€ê²Œ ìˆ˜: {len(stores)}ê°œ")
        
        if not stores:
            logger.warning("ìˆ˜ì§‘ëœ ê°€ê²Œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
        detailed_stores = []
        success_count = 0
        
        for i, store in enumerate(stores, 1):
            try:
                logger.info(f"ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘... ({i}/{len(stores)}) {store.get('name', 'Unknown')}")
                
                detail_info = crawler.get_store_detail(store)
                if detail_info:
                    detailed_stores.append(detail_info)
                    success_count += 1
                    
                    # ê°œì„ ëœ ê¸°ëŠ¥ í™•ì¸ ë¡œê·¸
                    logger.info(f"  âœ… ì£¼ì†Œ: {detail_info.get('address', 'N/A')[:50]}...")
                    logger.info(f"  âœ… ì˜ì—…ì‹œê°„: {detail_info.get('open_hours', 'N/A')[:50]}...")
                    logger.info(f"  âœ… ë¸Œë ˆì´í¬íƒ€ì„: {detail_info.get('break_time', 'N/A')}")
                    logger.info(f"  âœ… ë¼ìŠ¤íŠ¸ì˜¤ë”: {detail_info.get('last_order', 'N/A')}")
                    logger.info(f"  âœ… íœ´ë¬´ì¼: {detail_info.get('holiday', 'N/A')}")
                    
            except Exception as e:
                logger.error(f"ê°€ê²Œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                continue
        
        logger.info(f"ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {success_count}/{len(stores)}ê°œ")
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        if detailed_stores:
            try:
                inserted_count = db.insert_stores_batch(detailed_stores)
                logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ: {len(inserted_count)}ê°œ")
                
                # ì„±ê³µ í†µê³„
                success_rate = (success_count / len(stores)) * 100
                logger.info(f"ì „ì²´ ì„±ê³µë¥ : {success_rate:.1f}%")
                
                # ê°œì„ ëœ ê¸°ëŠ¥ í†µê³„
                check_improvement_stats(detailed_stores)
                
                return True
                
            except Exception as e:
                logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
                return False
        else:
            logger.warning("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
            
    except Exception as e:
        logger.error(f"ê°•ë‚¨ í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False
    
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        try:
            crawler.close()
            db.close()
        except:
            pass

def run_basic_test():
    """ê¸°ë³¸ í…ŒìŠ¤íŠ¸ (ë‹¨ì¼ ê°€ê²Œ ìƒì„¸ ì •ë³´ í™•ì¸)"""
    try:
        logger.info("ğŸ” ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì‹œì‘ (ë‹¨ì¼ ê°€ê²Œ ìƒì„¸ ì •ë³´ í™•ì¸)")
        
        from src.core.crawler import DiningCodeCrawler
        from src.core.database import DatabaseManager
        
        crawler = DiningCodeCrawler()
        db = DatabaseManager()
        
        # ê°•ë‚¨ ì§€ì—­ì—ì„œ ì²« ë²ˆì§¸ ê°€ê²Œ ì°¾ê¸°
        keyword = "ì„œìš¸ ê°•ë‚¨ ë¬´í•œë¦¬í•„"
        rect = "37.4979,127.0276,37.5279,127.0576"
        
        stores = crawler.get_store_list(keyword, rect)
        
        if not stores:
            logger.warning("í…ŒìŠ¤íŠ¸í•  ê°€ê²Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # ì²« ë²ˆì§¸ ê°€ê²Œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
        test_store = stores[0]
        logger.info(f"í…ŒìŠ¤íŠ¸ ëŒ€ìƒ: {test_store.get('name')}")
        
        detail_info = crawler.get_store_detail(test_store)
        
        if detail_info:
            logger.info("=== ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===")
            logger.info(f"ğŸ“ ê°€ê²Œëª…: {detail_info.get('name')}")
            logger.info(f"ğŸ“ ì£¼ì†Œ: {detail_info.get('address')}")
            logger.info(f"ğŸ“ ì „í™”ë²ˆí˜¸: {detail_info.get('phone_number')}")
            logger.info(f"â­ í‰ì : {detail_info.get('diningcode_rating')}")
            logger.info(f"ğŸ• ì˜ì—…ì‹œê°„: {detail_info.get('open_hours')}")
            logger.info(f"â˜• ë¸Œë ˆì´í¬íƒ€ì„: {detail_info.get('break_time')}")
            logger.info(f"ğŸ½ï¸ ë¼ìŠ¤íŠ¸ì˜¤ë”: {detail_info.get('last_order')}")
            logger.info(f"ğŸš« íœ´ë¬´ì¼: {detail_info.get('holiday')}")
            logger.info(f"ğŸ’° ê°€ê²©: {detail_info.get('price')}")
            logger.info(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ìˆ˜: {len(detail_info.get('image_urls', []))}")
            
            # ì¹´í…Œê³ ë¦¬ ì •ë³´ í™•ì¸
            raw_categories = detail_info.get('raw_categories_diningcode', [])
            logger.info(f"ğŸ·ï¸ ì¹´í…Œê³ ë¦¬: {raw_categories}")
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (ì¹´í…Œê³ ë¦¬ ì—°ê²° í¬í•¨)
            try:
                logger.info("ğŸ“Š save_crawled_data ë©”ì„œë“œë¡œ ì €ì¥ (ì¹´í…Œê³ ë¦¬ ì—°ê²° í¬í•¨)")
                db.save_crawled_data([detail_info], keyword="ê¸°ë³¸ í…ŒìŠ¤íŠ¸", rect_area="ê°•ë‚¨ ì§€ì—­")
                logger.info(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì„±ê³µ: 1ê°œ ê°€ê²Œ (ì¹´í…Œê³ ë¦¬ ì—°ê²° í¬í•¨)")
                
                # ì¹´í…Œê³ ë¦¬ ì €ì¥ í™•ì¸
                logger.info("ğŸ” ì¹´í…Œê³ ë¦¬ ì €ì¥ ìƒíƒœ í™•ì¸...")
                cursor = db.pg_conn.cursor()
                cursor.execute("SELECT COUNT(*) FROM categories")
                category_count = cursor.fetchone()[0]
                cursor.execute("SELECT COUNT(*) FROM store_categories")
                store_category_count = cursor.fetchone()[0]
                
                # ë°©ê¸ˆ ì €ì¥ëœ ê°€ê²Œì˜ ì¹´í…Œê³ ë¦¬ ì—°ê²° í™•ì¸
                cursor.execute("""
                    SELECT s.name, array_agg(c.name ORDER BY c.name) as categories
                    FROM stores s
                    LEFT JOIN store_categories sc ON s.id = sc.store_id
                    LEFT JOIN categories c ON sc.category_id = c.id
                    WHERE s.diningcode_place_id = %s
                    GROUP BY s.name
                """, (detail_info.get('diningcode_place_id'),))
                
                result = cursor.fetchone()
                if result:
                    store_name, linked_categories = result
                    logger.info(f"ğŸª ì €ì¥ëœ ê°€ê²Œ: {store_name}")
                    logger.info(f"ğŸ·ï¸ ì—°ê²°ëœ ì¹´í…Œê³ ë¦¬: {linked_categories if linked_categories[0] else 'ì—†ìŒ'}")
                
                cursor.close()
                
                logger.info(f"ğŸ“Š ì´ ì¹´í…Œê³ ë¦¬ ìˆ˜: {category_count}ê°œ")
                logger.info(f"ğŸ“Š ê°€ê²Œ-ì¹´í…Œê³ ë¦¬ ì—°ê²° ìˆ˜: {store_category_count}ê°œ")
                    
            except Exception as e:
                logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
                import traceback
                logger.error(traceback.format_exc())
                return False
            
            return True
        else:
            logger.error("ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨")
            return False
            
    except Exception as e:
        logger.error(f"ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return False
    
    finally:
        try:
            crawler.close()
            db.close()
        except:
            pass

def run_seoul_full_crawling():
    """ì„œìš¸ ì „ì§€ì—­ í¬ë¡¤ë§ (ê°œì„ ëœ ì—ëŸ¬ í•¸ë“¤ë§)"""
    try:
        logger.info("ğŸŒ ì„œìš¸ ì „ì§€ì—­ í¬ë¡¤ë§ ì‹œì‘")
        
        from src.core.crawler import DiningCodeCrawler
        from src.core.database import DatabaseManager
        
        crawler = DiningCodeCrawler()
        db = DatabaseManager()
        
        # ì„œìš¸ì‹œ 25ê°œ êµ¬ ì„¤ì • (ê°„ì†Œí™”)
        seoul_regions = [
            {"name": "ê°•ë‚¨êµ¬", "keyword": "ì„œìš¸ ê°•ë‚¨ ë¬´í•œë¦¬í•„", "rect": "37.4979,127.0276,37.5279,127.0576"},
            {"name": "ê°•ë¶êµ¬", "keyword": "ì„œìš¸ ê°•ë¶ ë¬´í•œë¦¬í•„", "rect": "37.6279,127.0076,37.6579,127.0376"},
            {"name": "ê°•ì„œêµ¬", "keyword": "ì„œìš¸ ê°•ì„œ ë¬´í•œë¦¬í•„", "rect": "37.5379,126.8176,37.5679,126.8476"},
            {"name": "ê´€ì•…êµ¬", "keyword": "ì„œìš¸ ê´€ì•… ë¬´í•œë¦¬í•„", "rect": "37.4579,126.9276,37.4879,126.9576"},
            {"name": "ê´‘ì§„êµ¬", "keyword": "ì„œìš¸ ê´‘ì§„ ë¬´í•œë¦¬í•„", "rect": "37.5279,127.0676,37.5579,127.0976"},
            {"name": "êµ¬ë¡œêµ¬", "keyword": "ì„œìš¸ êµ¬ë¡œ ë¬´í•œë¦¬í•„", "rect": "37.4879,126.8576,37.5179,126.8876"},
            {"name": "ê¸ˆì²œêµ¬", "keyword": "ì„œìš¸ ê¸ˆì²œ ë¬´í•œë¦¬í•„", "rect": "37.4479,126.8776,37.4779,126.9076"},
            {"name": "ë…¸ì›êµ¬", "keyword": "ì„œìš¸ ë…¸ì› ë¬´í•œë¦¬í•„", "rect": "37.6379,127.0576,37.6679,127.0876"},
            {"name": "ë„ë´‰êµ¬", "keyword": "ì„œìš¸ ë„ë´‰ ë¬´í•œë¦¬í•„", "rect": "37.6579,127.0276,37.6879,127.0576"},
            {"name": "ë™ëŒ€ë¬¸êµ¬", "keyword": "ì„œìš¸ ë™ëŒ€ë¬¸ ë¬´í•œë¦¬í•„", "rect": "37.5579,127.0376,37.5879,127.0676"},
            {"name": "ë™ì‘êµ¬", "keyword": "ì„œìš¸ ë™ì‘ ë¬´í•œë¦¬í•„", "rect": "37.4979,126.9376,37.5279,126.9676"},
            {"name": "ë§ˆí¬êµ¬", "keyword": "ì„œìš¸ ë§ˆí¬ ë¬´í•œë¦¬í•„", "rect": "37.5379,126.8976,37.5679,126.9276"},
            {"name": "ì„œëŒ€ë¬¸êµ¬", "keyword": "ì„œìš¸ ì„œëŒ€ë¬¸ ë¬´í•œë¦¬í•„", "rect": "37.5679,126.9176,37.5979,126.9476"},
            {"name": "ì„œì´ˆêµ¬", "keyword": "ì„œìš¸ ì„œì´ˆ ë¬´í•œë¦¬í•„", "rect": "37.4679,127.0076,37.4979,127.0376"},
            {"name": "ì„±ë™êµ¬", "keyword": "ì„œìš¸ ì„±ë™ ë¬´í•œë¦¬í•„", "rect": "37.5479,127.0176,37.5779,127.0476"},
            {"name": "ì„±ë¶êµ¬", "keyword": "ì„œìš¸ ì„±ë¶ ë¬´í•œë¦¬í•„", "rect": "37.5879,127.0076,37.6179,127.0376"},
            {"name": "ì†¡íŒŒêµ¬", "keyword": "ì„œìš¸ ì†¡íŒŒ ë¬´í•œë¦¬í•„", "rect": "37.4779,127.0876,37.5079,127.1176"},
            {"name": "ì–‘ì²œêµ¬", "keyword": "ì„œìš¸ ì–‘ì²œ ë¬´í•œë¦¬í•„", "rect": "37.5179,126.8476,37.5479,126.8776"},
            {"name": "ì˜ë“±í¬êµ¬", "keyword": "ì„œìš¸ ì˜ë“±í¬ ë¬´í•œë¦¬í•„", "rect": "37.5079,126.8876,37.5379,126.9176"},
            {"name": "ìš©ì‚°êµ¬", "keyword": "ì„œìš¸ ìš©ì‚° ë¬´í•œë¦¬í•„", "rect": "37.5179,126.9676,37.5479,126.9976"},
            {"name": "ì€í‰êµ¬", "keyword": "ì„œìš¸ ì€í‰ ë¬´í•œë¦¬í•„", "rect": "37.5879,126.9076,37.6179,126.9376"},
            {"name": "ì¢…ë¡œêµ¬", "keyword": "ì„œìš¸ ì¢…ë¡œ ë¬´í•œë¦¬í•„", "rect": "37.5679,126.9776,37.5979,127.0076"},
            {"name": "ì¤‘êµ¬", "keyword": "ì„œìš¸ ì¤‘êµ¬ ë¬´í•œë¦¬í•„", "rect": "37.5479,126.9776,37.5779,127.0076"},
            {"name": "ì¤‘ë‘êµ¬", "keyword": "ì„œìš¸ ì¤‘ë‘ ë¬´í•œë¦¬í•„", "rect": "37.5979,127.0676,37.6279,127.0976"},
            {"name": "ê°•ë™êµ¬", "keyword": "ì„œìš¸ ê°•ë™ ë¬´í•œë¦¬í•„", "rect": "37.5179,127.1076,37.5479,127.1376"}
        ]
        
        total_stores = 0
        successful_regions = 0
        failed_stores = []  # ì‹¤íŒ¨í•œ ê°€ê²Œ ì •ë³´ ì €ì¥
        region_results = {}  # ì§€ì—­ë³„ ê²°ê³¼ ì €ì¥
        
        for i, region in enumerate(seoul_regions, 1):
            logger.info(f"ğŸ“ [{i}/{len(seoul_regions)}] {region['name']} í¬ë¡¤ë§ ì‹œì‘")
            
            region_start_time = datetime.now()
            region_success_count = 0
            region_failed_count = 0
            region_failed_stores = []
            
            try:
                # ì§€ì—­ë³„ í¬ë¡¤ë§
                stores = crawler.get_store_list(region['keyword'], region['rect'])
                
                if stores:
                    # ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ (ê°œì„ ëœ ì—ëŸ¬ í•¸ë“¤ë§)
                    detailed_stores = []
                    for j, store in enumerate(stores, 1):
                        try:
                            logger.info(f"  ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘... ({j}/{len(stores)}) {store.get('name', 'Unknown')}")
                            
                            detail_info = crawler.get_store_detail(store)
                            if detail_info:
                                # ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
                                quality_score = detail_info.get('data_quality_score', 0)
                                
                                if quality_score >= 30:  # ìµœì†Œ í’ˆì§ˆ ê¸°ì¤€
                                    detailed_stores.append(detail_info)
                                    region_success_count += 1
                                    
                                    # ì„±ê³µì ìœ¼ë¡œ ìˆ˜ì§‘ëœ ì •ë³´ ë¡œê¹…
                                    logger.info(f"    âœ… ì„±ê³µ: í’ˆì§ˆì ìˆ˜ {quality_score}%")
                                else:
                                    # í’ˆì§ˆì´ ë‚®ì€ ê°€ê²Œ ì •ë³´
                                    failed_store = {
                                        'region': region['name'],
                                        'store_info': store,
                                        'detail_info': detail_info,
                                        'reason': f'ë‚®ì€ í’ˆì§ˆ ì ìˆ˜: {quality_score}%',
                                        'timestamp': datetime.now().isoformat()
                                    }
                                    region_failed_stores.append(failed_store)
                                    region_failed_count += 1
                                    logger.warning(f"    âš ï¸ í’ˆì§ˆ ë¶€ì¡±: {quality_score}% (ìµœì†Œ 30% í•„ìš”)")
                            else:
                                # ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨
                                failed_store = {
                                    'region': region['name'],
                                    'store_info': store,
                                    'reason': 'ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨',
                                    'timestamp': datetime.now().isoformat()
                                }
                                region_failed_stores.append(failed_store)
                                region_failed_count += 1
                                logger.warning(f"    âŒ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨")
                                
                        except Exception as e:
                            # ê°œë³„ ê°€ê²Œ í¬ë¡¤ë§ ì‹¤íŒ¨
                            failed_store = {
                                'region': region['name'],
                                'store_info': store,
                                'reason': f'í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}',
                                'timestamp': datetime.now().isoformat()
                            }
                            region_failed_stores.append(failed_store)
                            region_failed_count += 1
                            logger.error(f"    âŒ ê°€ê²Œ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                            continue
                    
                    # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
                    if detailed_stores:
                        try:
                            inserted_count = db.insert_stores_batch(detailed_stores)
                            total_stores += len(inserted_count)
                            successful_regions += 1
                            logger.info(f"âœ… {region['name']}: {len(inserted_count)}ê°œ ì €ì¥")
                        except Exception as e:
                            logger.error(f"âŒ {region['name']} ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
                            # ì €ì¥ ì‹¤íŒ¨í•œ ê°€ê²Œë“¤ë„ ì‹¤íŒ¨ ëª©ë¡ì— ì¶”ê°€
                            for store in detailed_stores:
                                failed_store = {
                                    'region': region['name'],
                                    'store_info': store,
                                    'reason': f'ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {str(e)}',
                                    'timestamp': datetime.now().isoformat()
                                }
                                region_failed_stores.append(failed_store)
                    else:
                        logger.warning(f"âŒ {region['name']}: ì €ì¥í•  ë°ì´í„° ì—†ìŒ")
                else:
                    logger.warning(f"âŒ {region['name']}: ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                
                # ì§€ì—­ë³„ ê²°ê³¼ ìš”ì•½
                region_end_time = datetime.now()
                region_duration = (region_end_time - region_start_time).total_seconds()
                
                region_results[region['name']] = {
                    'success_count': region_success_count,
                    'failed_count': region_failed_count,
                    'total_found': len(stores) if stores else 0,
                    'duration_seconds': region_duration,
                    'failed_stores': region_failed_stores
                }
                
                failed_stores.extend(region_failed_stores)
                
                logger.info(f"ğŸ“Š {region['name']} ì™„ë£Œ: ì„±ê³µ {region_success_count}ê°œ, ì‹¤íŒ¨ {region_failed_count}ê°œ, ì†Œìš”ì‹œê°„ {region_duration:.1f}ì´ˆ")
            
            except Exception as e:
                logger.error(f"âŒ {region['name']} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                # ì§€ì—­ ì „ì²´ ì‹¤íŒ¨ ê¸°ë¡
                region_results[region['name']] = {
                    'success_count': 0,
                    'failed_count': 0,
                    'total_found': 0,
                    'duration_seconds': 0,
                    'error': str(e)
                }
                continue
            
            # ì§€ì—­ ê°„ íœ´ì‹ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            if i < len(seoul_regions):
                logger.info("â° 5ì´ˆ íœ´ì‹...")
                import time
                time.sleep(5)
        
        # ì‹¤íŒ¨í•œ ê°€ê²Œ ì •ë³´ ì €ì¥
        if failed_stores:
            failed_stores_file = f"failed_stores_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(failed_stores_file, 'w', encoding='utf-8') as f:
                import json
                json.dump(failed_stores, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ“‹ ì‹¤íŒ¨í•œ ê°€ê²Œ ì •ë³´ ì €ì¥: {failed_stores_file} ({len(failed_stores)}ê°œ)")
        
        # ì§€ì—­ë³„ ê²°ê³¼ ì €ì¥
        results_file = f"seoul_crawling_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            import json
            json.dump(region_results, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ“Š ì§€ì—­ë³„ ê²°ê³¼ ì €ì¥: {results_file}")
        
        # ìµœì¢… ê²°ê³¼ ìš”ì•½
        total_failed = len(failed_stores)
        success_rate = (total_stores / (total_stores + total_failed) * 100) if (total_stores + total_failed) > 0 else 0
        
        logger.info(f"ğŸ‰ ì„œìš¸ ì „ì§€ì—­ í¬ë¡¤ë§ ì™„ë£Œ!")
        logger.info(f"ğŸ“Š ì„±ê³µí•œ êµ¬: {successful_regions}/{len(seoul_regions)}ê°œ")
        logger.info(f"ğŸ“Š ì´ ìˆ˜ì§‘ ê°€ê²Œ: {total_stores}ê°œ")
        logger.info(f"ğŸ“Š ì‹¤íŒ¨í•œ ê°€ê²Œ: {total_failed}ê°œ")
        logger.info(f"ğŸ“Š ì „ì²´ ì„±ê³µë¥ : {success_rate:.1f}%")
        
        # ì¬ì‹œë„ ê°€ëŠ¥í•œ ì‹¤íŒ¨ ê°€ê²Œ ë¶„ì„
        retry_candidates = [store for store in failed_stores 
                          if 'íƒ€ì„ì•„ì›ƒ' in store['reason'] or 'ë„¤íŠ¸ì›Œí¬' in store['reason'] or 'ì—°ê²°' in store['reason']]
        
        if retry_candidates:
            logger.info(f"ğŸ”„ ì¬ì‹œë„ ê°€ëŠ¥í•œ ê°€ê²Œ: {len(retry_candidates)}ê°œ")
            retry_file = f"retry_candidates_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(retry_file, 'w', encoding='utf-8') as f:
                import json
                json.dump(retry_candidates, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ“‹ ì¬ì‹œë„ ëŒ€ìƒ ì €ì¥: {retry_file}")
        
        return True
        
    except Exception as e:
        logger.error(f"ì„œìš¸ ì „ì§€ì—­ í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False
    
    finally:
        try:
            crawler.close()
            db.close()
        except:
            pass

def check_improvement_stats(stores):
    """ê°œì„ ëœ ê¸°ëŠ¥ í†µê³„ í™•ì¸"""
    logger.info("ğŸ” ê°œì„ ëœ ê¸°ëŠ¥ í†µê³„ í™•ì¸:")
    
    # ì£¼ì†Œ ìˆ˜ì§‘ë¥ 
    address_count = sum(1 for store in stores if store.get('address'))
    address_rate = (address_count / len(stores)) * 100
    logger.info(f"  ğŸ“ ì£¼ì†Œ ìˆ˜ì§‘ë¥ : {address_count}/{len(stores)} ({address_rate:.1f}%)")
    
    # ì˜ì—…ì‹œê°„ ìˆ˜ì§‘ë¥ 
    hours_count = sum(1 for store in stores if store.get('open_hours'))
    hours_rate = (hours_count / len(stores)) * 100
    logger.info(f"  ğŸ• ì˜ì—…ì‹œê°„ ìˆ˜ì§‘ë¥ : {hours_count}/{len(stores)} ({hours_rate:.1f}%)")
    
    # ë¸Œë ˆì´í¬íƒ€ì„ ìˆ˜ì§‘ë¥ 
    break_count = sum(1 for store in stores if store.get('break_time'))
    break_rate = (break_count / len(stores)) * 100
    logger.info(f"  â˜• ë¸Œë ˆì´í¬íƒ€ì„ ìˆ˜ì§‘ë¥ : {break_count}/{len(stores)} ({break_rate:.1f}%)")
    
    # ë¼ìŠ¤íŠ¸ì˜¤ë” ìˆ˜ì§‘ë¥ 
    last_order_count = sum(1 for store in stores if store.get('last_order'))
    last_order_rate = (last_order_count / len(stores)) * 100
    logger.info(f"  ğŸ½ï¸ ë¼ìŠ¤íŠ¸ì˜¤ë” ìˆ˜ì§‘ë¥ : {last_order_count}/{len(stores)} ({last_order_rate:.1f}%)")
    
    # íœ´ë¬´ì¼ ìˆ˜ì§‘ë¥ 
    holiday_count = sum(1 for store in stores if store.get('holiday'))
    holiday_rate = (holiday_count / len(stores)) * 100
    logger.info(f"  ğŸš« íœ´ë¬´ì¼ ìˆ˜ì§‘ë¥ : {holiday_count}/{len(stores)} ({holiday_rate:.1f}%)")
    
    # ìš”ì¼ë³„ ì˜ì—…ì‹œê°„ ìˆ˜ì§‘ë¥  (ì›”í™”ìˆ˜ëª©ê¸ˆí† ì¼ íŒ¨í„´ í™•ì¸)
    weekday_pattern_count = 0
    for store in stores:
        hours = store.get('open_hours', '')
        if any(day in hours for day in ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼']):
            weekday_pattern_count += 1
    
    weekday_rate = (weekday_pattern_count / len(stores)) * 100
    logger.info(f"  ğŸ“… ìš”ì¼ë³„ ì˜ì—…ì‹œê°„ ìˆ˜ì§‘ë¥ : {weekday_pattern_count}/{len(stores)} ({weekday_rate:.1f}%)")

def check_database_status():
    """ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸"""
    try:
        logger.info("ğŸ” ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸")
        
        from src.core.database import DatabaseManager
        db = DatabaseManager()
        
        # ê¸°ë³¸ ì—°ê²° í…ŒìŠ¤íŠ¸
        if not db.test_connection():
            logger.error("âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨")
            return False
        
        logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")
        
        # í…Œì´ë¸” í™•ì¸
        try:
            cursor = db.connection.cursor()
            cursor.execute("SELECT COUNT(*) FROM stores")
            store_count = cursor.fetchone()[0]
            logger.info(f"ğŸ“Š í˜„ì¬ ì €ì¥ëœ ê°€ê²Œ ìˆ˜: {store_count}ê°œ")
            
            if store_count > 0:
                # ê°œì„ ëœ í•„ë“œ í™•ì¸
                cursor.execute("SELECT COUNT(*) FROM stores WHERE address IS NOT NULL AND address != ''")
                address_count = cursor.fetchone()[0]
                address_rate = (address_count / store_count * 100)
                logger.info(f"ğŸ“ ì£¼ì†Œ ë³´ìœ  ê°€ê²Œ: {address_count}ê°œ ({address_rate:.1f}%)")
                
                cursor.execute("SELECT COUNT(*) FROM stores WHERE break_time IS NOT NULL AND break_time != ''")
                break_time_count = cursor.fetchone()[0]
                break_time_rate = (break_time_count / store_count * 100)
                logger.info(f"â˜• ë¸Œë ˆì´í¬íƒ€ì„ ë³´ìœ  ê°€ê²Œ: {break_time_count}ê°œ ({break_time_rate:.1f}%)")
                
                cursor.execute("SELECT COUNT(*) FROM stores WHERE last_order IS NOT NULL AND last_order != ''")
                last_order_count = cursor.fetchone()[0]
                last_order_rate = (last_order_count / store_count * 100)
                logger.info(f"ğŸ½ï¸ ë¼ìŠ¤íŠ¸ì˜¤ë” ë³´ìœ  ê°€ê²Œ: {last_order_count}ê°œ ({last_order_rate:.1f}%)")
            
            cursor.close()
            
        except Exception as e:
            logger.warning(f"í…Œì´ë¸” ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
        
        db.close()
        return True
        
    except Exception as e:
        logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
        return False

def retry_failed_stores(failed_stores_file: str):
    """ì‹¤íŒ¨í•œ ê°€ê²Œë“¤ ì¬ì‹œë„"""
    try:
        logger.info(f"ğŸ”„ ì‹¤íŒ¨í•œ ê°€ê²Œ ì¬ì‹œë„ ì‹œì‘: {failed_stores_file}")
        
        import json
        with open(failed_stores_file, 'r', encoding='utf-8') as f:
            failed_stores = json.load(f)
        
        if not failed_stores:
            logger.warning("ì¬ì‹œë„í•  ê°€ê²Œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        from src.core.crawler import DiningCodeCrawler
        from src.core.database import DatabaseManager
        
        crawler = DiningCodeCrawler()
        db = DatabaseManager()
        
        retry_success = 0
        retry_failed = 0
        new_failed_stores = []
        
        logger.info(f"ì´ {len(failed_stores)}ê°œ ê°€ê²Œ ì¬ì‹œë„ ì‹œì‘")
        
        for i, failed_store in enumerate(failed_stores, 1):
            try:
                store_info = failed_store['store_info']
                region = failed_store['region']
                original_reason = failed_store['reason']
                
                logger.info(f"[{i}/{len(failed_stores)}] ì¬ì‹œë„: {store_info.get('name', 'Unknown')} ({region})")
                logger.info(f"  ì›ë˜ ì‹¤íŒ¨ ì‚¬ìœ : {original_reason}")
                
                # ì¬ì‹œë„ ìˆ˜í–‰
                detail_info = crawler.get_store_detail(store_info)
                
                if detail_info:
                    quality_score = detail_info.get('data_quality_score', 0)
                    
                    if quality_score >= 30:
                        # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹œë„
                        try:
                            inserted_count = db.insert_stores_batch([detail_info])
                            if len(inserted_count) > 0:
                                retry_success += 1
                                logger.info(f"  âœ… ì¬ì‹œë„ ì„±ê³µ: í’ˆì§ˆì ìˆ˜ {quality_score}%")
                            else:
                                retry_failed += 1
                                new_failed_stores.append({
                                    **failed_store,
                                    'retry_reason': 'ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨ (ì¬ì‹œë„)',
                                    'retry_timestamp': datetime.now().isoformat()
                                })
                                logger.warning(f"  âŒ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨ (ì¬ì‹œë„)")
                        except Exception as e:
                            retry_failed += 1
                            new_failed_stores.append({
                                **failed_store,
                                'retry_reason': f'ì €ì¥ ì˜¤ë¥˜: {str(e)}',
                                'retry_timestamp': datetime.now().isoformat()
                            })
                            logger.error(f"  âŒ ì €ì¥ ì˜¤ë¥˜ (ì¬ì‹œë„): {e}")
                    else:
                        retry_failed += 1
                        new_failed_stores.append({
                            **failed_store,
                            'retry_reason': f'ë‚®ì€ í’ˆì§ˆ ì ìˆ˜: {quality_score}% (ì¬ì‹œë„)',
                            'retry_timestamp': datetime.now().isoformat()
                        })
                        logger.warning(f"  âš ï¸ í’ˆì§ˆ ë¶€ì¡± (ì¬ì‹œë„): {quality_score}%")
                else:
                    retry_failed += 1
                    new_failed_stores.append({
                        **failed_store,
                        'retry_reason': 'ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨ (ì¬ì‹œë„)',
                        'retry_timestamp': datetime.now().isoformat()
                    })
                    logger.warning(f"  âŒ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨ (ì¬ì‹œë„)")
                
                # ìš”ì²­ ê°„ ì§€ì—°
                if i % 10 == 0:  # 10ê°œë§ˆë‹¤ íœ´ì‹
                    logger.info("â° 3ì´ˆ íœ´ì‹...")
                    import time
                    time.sleep(3)
                
            except Exception as e:
                retry_failed += 1
                new_failed_stores.append({
                    **failed_store,
                    'retry_reason': f'ì¬ì‹œë„ ì˜¤ë¥˜: {str(e)}',
                    'retry_timestamp': datetime.now().isoformat()
                })
                logger.error(f"  âŒ ì¬ì‹œë„ ì˜¤ë¥˜: {e}")
                continue
        
        # ì¬ì‹œë„ ê²°ê³¼ ì €ì¥
        if new_failed_stores:
            retry_failed_file = f"retry_failed_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(retry_failed_file, 'w', encoding='utf-8') as f:
                json.dump(new_failed_stores, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ“‹ ì¬ì‹œë„ ì‹¤íŒ¨ ì •ë³´ ì €ì¥: {retry_failed_file} ({len(new_failed_stores)}ê°œ)")
        
        # ì¬ì‹œë„ ê²°ê³¼ ìš”ì•½
        total_retry = retry_success + retry_failed
        success_rate = (retry_success / total_retry * 100) if total_retry > 0 else 0
        
        logger.info(f"ğŸ‰ ì¬ì‹œë„ ì™„ë£Œ!")
        logger.info(f"ğŸ“Š ì¬ì‹œë„ ì„±ê³µ: {retry_success}ê°œ")
        logger.info(f"ğŸ“Š ì¬ì‹œë„ ì‹¤íŒ¨: {retry_failed}ê°œ")
        logger.info(f"ğŸ“Š ì¬ì‹œë„ ì„±ê³µë¥ : {success_rate:.1f}%")
        
        return True
        
    except Exception as e:
        logger.error(f"ì¬ì‹œë„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False
    
    finally:
        try:
            crawler.close()
            db.close()
        except:
            pass

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ë¦¬í•„ìŠ¤íŒŸ í¬ë¡¤ëŸ¬ (ê°œì„ ëœ ë²„ì „)')
    parser.add_argument('mode', choices=['gangnam', 'basic', 'seoul', 'check', 'stage4', 'retry'], 
                       help='ì‹¤í–‰ ëª¨ë“œ: gangnam(ê°•ë‚¨í…ŒìŠ¤íŠ¸), basic(ê¸°ë³¸í…ŒìŠ¤íŠ¸), seoul(ì„œìš¸ì „ì§€ì—­), check(DBìƒíƒœ), stage4(í˜¸í™˜ì„±), retry(ì¬ì‹œë„)')
    parser.add_argument('--retry-file', type=str, 
                       help='ì¬ì‹œë„í•  ì‹¤íŒ¨ ê°€ê²Œ JSON íŒŒì¼ ê²½ë¡œ (retry ëª¨ë“œì—ì„œ í•„ìˆ˜)')
    
    args = parser.parse_args()
    
    print("ğŸ½ï¸ ë¦¬í•„ìŠ¤íŒŸ í¬ë¡¤ëŸ¬ (ê°œì„ ëœ ë²„ì „)")
    print("=" * 50)
    print("âœ¨ ê°œì„ ëœ ê¸°ëŠ¥:")
    print("  - ì£¼ì†Œ ì •í™• ì¶”ì¶œ")
    print("  - ìš”ì¼ë³„ ì˜ì—…ì‹œê°„ ìˆ˜ì§‘ (ê°œì„ ëœ íŒŒì‹±)")
    print("  - ë¸Œë ˆì´í¬íƒ€ì„ ì •ë³´ ìˆ˜ì§‘")
    print("  - ë¼ìŠ¤íŠ¸ì˜¤ë” ì •ë³´ ìˆ˜ì§‘")
    print("  - íœ´ë¬´ì¼ ì •ë³´ ìˆ˜ì§‘")
    print("  - ê°€ê²© ì •ë³´ ì¶”ì¶œ (ê°œì„ ëœ ì…€ë ‰í„°)")
    print("  - ê°•í™”ëœ ì—ëŸ¬ í•¸ë“¤ë§")
    print("  - ë°ì´í„° í’ˆì§ˆ ê²€ì¦")
    print("  - ì‹¤íŒ¨ ê°€ê²Œ ì¬ì‹œë„ ê¸°ëŠ¥")
    print("=" * 50)
    
    if args.mode == 'gangnam':
        print("ğŸš€ ê°•ë‚¨ ì§€ì—­ í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§")
        success = run_gangnam_test()
    elif args.mode == 'basic':
        print("ğŸ” ê¸°ë³¸ í…ŒìŠ¤íŠ¸ (ë‹¨ì¼ ê°€ê²Œ)")
        success = run_basic_test()
    elif args.mode == 'seoul':
        print("ğŸŒ ì„œìš¸ ì „ì§€ì—­ í¬ë¡¤ë§")
        success = run_seoul_full_crawling()
    elif args.mode == 'check':
        print("ğŸ” ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸")
        success = check_database_status()
    elif args.mode == 'stage4':
        print("ğŸ”„ Stage4 í˜¸í™˜ ëª¨ë“œ (ê°•ë‚¨ ì§€ì—­)")
        success = run_gangnam_test()
    elif args.mode == 'retry':
        if not args.retry_file:
            print("âŒ ì¬ì‹œë„ ëª¨ë“œì—ì„œëŠ” --retry-file ì˜µì…˜ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            print("ì‚¬ìš©ë²•: python src/utils/main.py retry --retry-file failed_stores_20240629_123456.json")
            sys.exit(1)
        print(f"ğŸ”„ ì‹¤íŒ¨í•œ ê°€ê²Œ ì¬ì‹œë„ ëª¨ë“œ: {args.retry_file}")
        success = retry_failed_stores(args.retry_file)
    
    if success:
        print("âœ… ì‹¤í–‰ ì™„ë£Œ!")
        if args.mode == 'seoul':
            print("\nğŸ“‹ ìƒì„±ëœ íŒŒì¼ë“¤:")
            print("  - seoul_crawling_results_YYYYMMDD_HHMMSS.json: ì§€ì—­ë³„ ìƒì„¸ ê²°ê³¼")
            print("  - failed_stores_YYYYMMDD_HHMMSS.json: ì‹¤íŒ¨í•œ ê°€ê²Œ ëª©ë¡")
            print("  - retry_candidates_YYYYMMDD_HHMMSS.json: ì¬ì‹œë„ ê°€ëŠ¥í•œ ê°€ê²Œ ëª©ë¡")
            print("\nğŸ”„ ì¬ì‹œë„ ë°©ë²•:")
            print("  python src/utils/main.py retry --retry-file retry_candidates_YYYYMMDD_HHMMSS.json")
    else:
        print("âŒ ì‹¤í–‰ ì‹¤íŒ¨!")
        sys.exit(1)

if __name__ == "__main__":
    main()