import logging
import time
import random
import re
from typing import List, Dict, Optional, Any
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from selenium.webdriver.chrome.options import Options

# config import ìˆ˜ì •
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

try:
    from config.config import USER_AGENTS, MIN_DELAY, MAX_DELAY, IMAGE_STORAGE_CONFIG
except ImportError:
    # ê¸°ë³¸ê°’ ì„¤ì •
    USER_AGENTS = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    ]
    MIN_DELAY = 1
    MAX_DELAY = 3
    IMAGE_STORAGE_CONFIG = {}

# ì´ë¯¸ì§€ ë§¤ë‹ˆì € import
try:
    from src.core.image_manager import ImageManager
except ImportError:
    try:
        from .image_manager import ImageManager
    except ImportError:
        ImageManager = None


# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('crawler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# data í´ë” ìƒì„± (ì—†ìœ¼ë©´)
os.makedirs('data', exist_ok=True)

class DiningCodeCrawler:
    def __init__(self, enable_image_download: bool = True):
        """
        ë‹¤ì´ë‹ì½”ë“œ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        
        Args:
            enable_image_download: ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° Storage ì—…ë¡œë“œ í™œì„±í™” ì—¬ë¶€
        """
        self.driver = None
        self.current_url = ""
        self.session_start_time = time.time()
        
        # ì´ë¯¸ì§€ ë§¤ë‹ˆì € ì´ˆê¸°í™” (ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° Storage ì—…ë¡œë“œ)
        self.enable_image_download = enable_image_download
        self.image_manager = None
        
        if enable_image_download and ImageManager:
            try:
                # configì—ì„œ ì´ë¯¸ì§€ ìŠ¤í† ë¦¬ì§€ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
                self.image_manager = ImageManager(config=IMAGE_STORAGE_CONFIG)
                logger.info("ì´ë¯¸ì§€ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ (ìŠ¤í† ë¦¬ì§€ í™œì„±í™”)")
            except Exception as e:
                logger.warning(f"ì´ë¯¸ì§€ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.enable_image_download = False
        elif enable_image_download:
            logger.warning("ImageManager í´ë˜ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ. ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œê°€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
            self.enable_image_download = False
        
        # ì„±ëŠ¥ í†µê³„
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'images_processed': 0,
            'images_uploaded': 0
        }
        
        self.setup_driver()
        
    def setup_driver(self):
        """Selenium WebDriver ì„¤ì •"""
        chrome_options = Options()
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_argument('--disable-web-security')
        chrome_options.add_argument('--disable-features=VizDisplayCompositor')
        chrome_options.add_argument('--disable-extensions')
        chrome_options.add_argument('--disable-plugins')
        chrome_options.add_argument('--disable-images')  # ì´ë¯¸ì§€ ë¡œë”© ë¹„í™œì„±í™”ë¡œ ì†ë„ í–¥ìƒ
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # í˜ì´ì§€ ë¡œë“œ ì „ëµ ì„¤ì •
        chrome_options.add_argument('--page-load-strategy=normal')
        
        # User-Agent ëœë¤ ì„¤ì •
        user_agent = random.choice(USER_AGENTS)
        chrome_options.add_argument(f'--user-agent={user_agent}')
        
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            # íƒ€ì„ì•„ì›ƒ ì„¤ì • ìµœì í™”
            self.driver.set_page_load_timeout(20)  # 30ì´ˆì—ì„œ 20ì´ˆë¡œ ë‹¨ì¶•
            self.driver.implicitly_wait(5)  # 10ì´ˆì—ì„œ 5ì´ˆë¡œ ë‹¨ì¶•
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            self.wait = WebDriverWait(self.driver, 10)  # 20ì´ˆì—ì„œ 10ì´ˆë¡œ ë‹¨ì¶•
            logger.info("WebDriver ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"WebDriver ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def random_delay(self, min_delay=None, max_delay=None):
        """ëœë¤ ì§€ì—° (ì¬ì‹œë„ ë¡œì§ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ íŒŒë¼ë¯¸í„° ì¶”ê°€)"""
        min_d = min_delay or MIN_DELAY
        max_d = max_delay or MAX_DELAY
        delay = random.uniform(min_d, max_d)
        time.sleep(delay)
        
    def retry_on_failure(self, func, max_retries=3, delay_multiplier=1.5):
        """ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ ë¡œì§"""
        for attempt in range(max_retries):
            try:
                return func()
            except Exception as e:
                if attempt == max_retries - 1:
                    logger.error(f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {e}")
                    raise
                else:
                    wait_time = (attempt + 1) * delay_multiplier
                    logger.warning(f"ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {e}. {wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(wait_time)

    def get_store_list(self, keyword: str, rect: str) -> List[Dict]:
        """ë‹¤ì´ë‹ì½”ë“œì—ì„œ ê°€ê²Œ ëª©ë¡ ìˆ˜ì§‘ (ë‘ ë²ˆ ì‹œë„ ë°©ì‹)"""
        stores = []
        
        try:
            logger.info(f"ëª©ë¡ í˜ì´ì§€ í¬ë¡¤ë§ ì‹œì‘: {keyword}, {rect}")
            
            # ì²« ë²ˆì§¸ ì‹œë„
            stores = self._search_stores(keyword, rect, attempt=1)
            
            # ì²« ë²ˆì§¸ ì‹œë„ì—ì„œ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ë‘ ë²ˆì§¸ ì‹œë„
            if not stores:
                logger.info("ì²« ë²ˆì§¸ ê²€ìƒ‰ì—ì„œ ê²°ê³¼ê°€ ì—†ìŒ. ë‘ ë²ˆì§¸ ì‹œë„ ì§„í–‰...")
                time.sleep(2)  # 3ì´ˆì—ì„œ 2ì´ˆë¡œ ë‹¨ì¶•
                stores = self._search_stores(keyword, rect, attempt=2)
            
            if stores:
                logger.info(f"ì´ {len(stores)}ê°œ ê°€ê²Œ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ")
            else:
                logger.warning("ë‘ ë²ˆì˜ ì‹œë„ ëª¨ë‘ì—ì„œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
        except Exception as e:
            logger.error(f"ëª©ë¡ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
            try:
                logger.error(f"í˜„ì¬ URL: {self.driver.current_url}")
                logger.error(f"í˜ì´ì§€ ì œëª©: {self.driver.title}")
            except:
                pass
            
        return stores
    
    def _search_stores(self, keyword: str, rect: str, attempt: int) -> List[Dict]:
        """ì‹¤ì œ ê²€ìƒ‰ ìˆ˜í–‰ (ë‹¨ì¼ ì‹œë„)"""
        stores = []
        
        try:
            logger.info(f"=== {attempt}ë²ˆì§¸ ê²€ìƒ‰ ì‹œë„ ===")
            
            # 1. ë©”ì¸ í˜ì´ì§€ ë¨¼ì € ì ‘ì† (ì•ˆì •ì„±ì„ ìœ„í•´)
            logger.info("ë‹¤ì´ë‹ì½”ë“œ ë©”ì¸ í˜ì´ì§€ ì ‘ì† ì¤‘...")
            self.driver.get("https://www.diningcode.com")
            self.random_delay()
            
            # 2. ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì§ì ‘ ì´ë™
            if rect and rect != "":
                search_url = f"https://www.diningcode.com/list.dc?query={keyword}&rect={rect}"
                logger.info(f"ì§€ì—­ ì œí•œ ê²€ìƒ‰ URL ì ‘ì†: {search_url}")
            else:
                search_url = f"https://www.diningcode.com/list.dc?query={keyword}"
                logger.info(f"ì „êµ­ ê²€ìƒ‰ URL ì ‘ì†: {search_url}")
            
            # ì²« ë²ˆì§¸ ê²€ìƒ‰ ì‹œë„
            stores = self._try_search_url(search_url, keyword, rect, "ì²« ë²ˆì§¸")
            
            # ì²« ë²ˆì§¸ ì‹œë„ì—ì„œ ê²°ê³¼ê°€ ì—†ê³  ì§€ì—­ ì œí•œ ê²€ìƒ‰ì¸ ê²½ìš°, ê°™ì€ ê²€ìƒ‰ì„ í•œ ë²ˆ ë” ì‹œë„
            if not stores and rect and rect != "":
                logger.info("ì²« ë²ˆì§¸ ì§€ì—­ ê²€ìƒ‰ì—ì„œ ê²°ê³¼ê°€ ì—†ìŒ. ì ì‹œ ëŒ€ê¸° í›„ ê°™ì€ ê²€ìƒ‰ ì¬ì‹œë„...")
                time.sleep(3)  # 5ì´ˆì—ì„œ 3ì´ˆë¡œ ë‹¨ì¶•
                stores = self._try_search_url(search_url, keyword, rect, "ì¬ì‹œë„")
                
                # ì¬ì‹œë„ì—ì„œë„ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì „êµ­ ê²€ìƒ‰ìœ¼ë¡œ ì‹œë„
                if not stores:
                    logger.info("ì§€ì—­ ì œí•œ ê²€ìƒ‰ ì¬ì‹œë„ì—ì„œë„ ê²°ê³¼ê°€ ì—†ìŒ. ì „êµ­ ê²€ìƒ‰ìœ¼ë¡œ ì‹œë„...")
                    fallback_url = f"https://www.diningcode.com/list.dc?query={keyword}"
                    stores = self._try_search_url(fallback_url, keyword, "", "ì „êµ­ ê²€ìƒ‰")
            
        except Exception as e:
            logger.error(f"{attempt}ë²ˆì§¸ ê²€ìƒ‰ ì‹œë„ ì¤‘ ì˜¤ë¥˜: {e}")
            
        return stores
    
    def _try_search_url(self, search_url: str, keyword: str, rect: str, search_type: str) -> List[Dict]:
        """íŠ¹ì • URLë¡œ ê²€ìƒ‰ ì‹œë„"""
        stores = []
        
        try:
            logger.info(f"=== {search_type} ê²€ìƒ‰ ì‹œë„ ===")
            logger.info(f"URL: {search_url}")
            
            self.driver.get(search_url)
            self.random_delay()
            
            # 3. í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° - React ì•±ì´ ë¡œë“œë  ë•Œê¹Œì§€
            logger.info("React ì•± ë¡œë”© ëŒ€ê¸° ì¤‘...")
            try:
                # PoiBlock í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ìš”ì†Œë“¤ì´ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ëŒ€ê¸° (ì‹œê°„ ë‹¨ì¶•)
                WebDriverWait(self.driver, 5).until(EC.presence_of_element_located((By.CLASS_NAME, "PoiBlock")))
                logger.info("ê°€ê²Œ ëª©ë¡ ë¡œë”© ì™„ë£Œ")
            except TimeoutException:
                logger.warning("PoiBlock ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ. ì¶”ê°€ ëŒ€ê¸°...")
                # ì¶”ê°€ ëŒ€ê¸° ì‹œê°„ ë‹¨ì¶•
                time.sleep(3)  # 15ì´ˆì—ì„œ 3ì´ˆë¡œ ë‹¨ì¶•
                
                # ë‹¤ì‹œ í•œ ë²ˆ ì‹œë„
                try:
                    poi_elements = self.driver.find_elements(By.CLASS_NAME, "PoiBlock")
                    if poi_elements:
                        logger.info(f"ì¶”ê°€ ëŒ€ê¸° í›„ {len(poi_elements)}ê°œ PoiBlock ë°œê²¬")
                    else:
                        logger.warning(f"{search_type}ì—ì„œ PoiBlockì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                        return stores
                except:
                    logger.warning(f"{search_type}ì—ì„œ PoiBlock ê²€ìƒ‰ ì‹¤íŒ¨")
                    return stores
            
            # 4. í˜„ì¬ í˜ì´ì§€ ì •ë³´ í™•ì¸
            current_url = self.driver.current_url
            page_title = self.driver.title
            logger.info(f"í˜„ì¬ í˜ì´ì§€ URL: {current_url}")
            logger.info(f"í˜ì´ì§€ ì œëª©: {page_title}")
            
            # 5. JavaScriptì—ì„œ ë°ì´í„° ì¶”ì¶œ ì‹œë„
            try:
                # ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ìœ¼ë¡œ ì¶”ê°€ ê²°ê³¼ ë¡œë“œ
                self._load_more_results()
                
                # ë°©ë²• 1: localStorageì—ì„œ listData ì¶”ì¶œ
                list_data = self.driver.execute_script("return localStorage.getItem('listData');")
                if list_data:
                    import json
                    data = json.loads(list_data)
                    poi_list = data.get('poi_section', {}).get('list', [])
                    
                    logger.info(f"localStorageì—ì„œ {len(poi_list)}ê°œ ê°€ê²Œ ë°ì´í„° ì¶”ì¶œ")
                    
                    for poi in poi_list:
                        store_info = {
                            'diningcode_place_id': poi.get('v_rid', ''),
                            'detail_url': f"/profile.php?rid={poi.get('v_rid', '')}",
                            'name': poi.get('nm', ''),
                            'branch': poi.get('branch', ''),
                            'basic_address': poi.get('addr', ''),
                            'road_address': poi.get('road_addr', ''),
                            'phone_number': poi.get('phone', ''),
                            'distance': poi.get('distance', ''),
                            'category': poi.get('category', ''),
                            'keyword': keyword,
                            'rect_area': rect,
                            'position_lat': poi.get('lat'),
                            'position_lng': poi.get('lng'),
                            'diningcode_score': poi.get('score'),
                            'diningcode_rating': poi.get('user_score'),
                            'review_count': poi.get('review_cnt', 0),
                            'image_urls': poi.get('image_list', []),
                            'open_status': poi.get('open_status', ''),
                            'raw_categories_diningcode': poi.get('keyword', [])
                        }
                        
                        if store_info['diningcode_place_id'] and store_info['name']:
                            stores.append(store_info)
                            logger.info(f"ê°€ê²Œ ì¶”ê°€: {store_info['name']} {store_info['branch']} (ID: {store_info['diningcode_place_id']})")
                    
                    if stores:
                        logger.info(f"localStorageì—ì„œ ì´ {len(stores)}ê°œ ê°€ê²Œ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ")
                        return stores
                else:
                    logger.info("localStorageì— listDataê°€ ì—†ìŒ. ë‹¤ë¥¸ ë°©ë²• ì‹œë„...")
                
                # ë°©ë²• 2: ì „ì—­ JavaScript ë³€ìˆ˜ì—ì„œ ë°ì´í„° ì¶”ì¶œ
                js_check_script = """
                    // window ê°ì²´ì—ì„œ ë°ì´í„° ì°¾ê¸°
                    var data = null;
                    if (window.__INITIAL_STATE__) {
                        data = window.__INITIAL_STATE__;
                    } else if (window.__APP_DATA__) {
                        data = window.__APP_DATA__;
                    } else if (window.appData) {
                        data = window.appData;
                    }
                    return data;
                """
                
                js_data = self.driver.execute_script(js_check_script)
                if js_data:
                    logger.info("ì „ì—­ JavaScript ë³€ìˆ˜ì—ì„œ ë°ì´í„° ë°œê²¬")
                    # ë°ì´í„° êµ¬ì¡° ë¶„ì„ í›„ ì¶”ì¶œ
                
                # ë°©ë²• 3: í˜ì´ì§€ ë‚´ script íƒœê·¸ì—ì„œ JSON ë°ì´í„° ì¶”ì¶œ
                scripts = self.driver.find_all('script')
                for script in scripts:
                    script_text = script.get_attribute('innerHTML')
                    if script_text and ('poi_list' in script_text or 'store_list' in script_text):
                        logger.info("script íƒœê·¸ì—ì„œ ê°€ê²Œ ë°ì´í„° ë°œê²¬")
                        # JSON ë°ì´í„° íŒŒì‹± ì‹œë„
                        
            except Exception as e:
                logger.warning(f"JavaScript ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            # 6. HTML íŒŒì‹±ìœ¼ë¡œ ê°€ê²Œ ì •ë³´ ì¶”ì¶œ (ë°±ì—… ë°©ë²•)
            logger.info("HTML íŒŒì‹±ìœ¼ë¡œ ê°€ê²Œ ì •ë³´ ì¶”ì¶œ ì‹œë„...")
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # PoiBlock í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ë§í¬ë“¤ ì°¾ê¸°
            poi_blocks = soup.find_all('a', class_='PoiBlock')
            logger.info(f"HTMLì—ì„œ {len(poi_blocks)}ê°œ PoiBlock ë°œê²¬")
            
            for block in poi_blocks:
                try:
                    store_info = {
                        'diningcode_place_id': '',
                        'detail_url': '',
                        'name': '',
                        'branch': '',
                        'basic_address': '',
                        'keyword': keyword,
                        'rect_area': rect,
                        'position_lat': None,
                        'position_lng': None
                    }
                    
                    # URLì—ì„œ rid ì¶”ì¶œ
                    href = block.get('href', '')
                    if 'rid=' in href:
                        rid = href.split('rid=')[1].split('&')[0]
                        store_info['diningcode_place_id'] = rid
                        store_info['detail_url'] = href
                    
                    # ê°€ê²Œ ì´ë¦„ ì¶”ì¶œ
                    title_elem = block.find('h2')
                    if title_elem:
                        # ë²ˆí˜¸ ì œê±° (ì˜ˆ: "1. ìœ¡ë¯¸ì œë‹¹" -> "ìœ¡ë¯¸ì œë‹¹", "14.ê°•ë‚¨ ë¼ì§€ìƒíšŒ" -> "ê°•ë‚¨ ë¼ì§€ìƒíšŒ")
                        name_text = title_elem.get_text(strip=True)
                        
                        # ë‹¤ì–‘í•œ ë²ˆí˜¸ íŒ¨í„´ ì œê±°
                        # íŒ¨í„´ 1: "1. ê°€ê²Œëª…", "14. ê°€ê²Œëª…"
                        if re.match(r'^\d+\.\s*', name_text):
                            name_text = re.sub(r'^\d+\.\s*', '', name_text)
                        
                        # íŒ¨í„´ 2: "1.ê°€ê²Œëª…", "14.ê°€ê²Œëª…" (ê³µë°± ì—†ìŒ)
                        elif re.match(r'^\d+\.', name_text):
                            name_text = re.sub(r'^\d+\.', '', name_text)
                        
                        # íŒ¨í„´ 3: "1 ê°€ê²Œëª…", "14 ê°€ê²Œëª…" (ì  ì—†ìŒ)
                        elif re.match(r'^\d+\s+', name_text):
                            name_text = re.sub(r'^\d+\s+', '', name_text)
                        
                        # ì§€ì ëª… ë¶„ë¦¬ (í•˜ì§€ë§Œ ê°€ê²Œëª…ì€ ì›ë³¸ ê·¸ëŒ€ë¡œ ì €ì¥)
                        place_elem = title_elem.find('span', class_='Info__Title__Place')
                        if place_elem:
                            store_info['branch'] = place_elem.get_text(strip=True)
                            # ê°€ê²Œëª…ì€ ì›ë³¸ ê·¸ëŒ€ë¡œ ì €ì¥ (ì§€ì ëª… í¬í•¨)
                            store_info['name'] = name_text.strip()
                        else:
                            store_info['name'] = name_text.strip()
                            store_info['branch'] = ''
                    
                    # data ì†ì„±ì—ì„œ ìœ„ì¹˜ì •ë³´ ì¶”ì¶œ ì‹œë„
                    data_lat = block.get('data-lat') or block.get('data-latitude')
                    data_lng = block.get('data-lng') or block.get('data-longitude')
                    
                    if data_lat and data_lng:
                        try:
                            store_info['position_lat'] = float(data_lat)
                            store_info['position_lng'] = float(data_lng)
                            logger.info(f"HTML data ì†ì„±ì—ì„œ ì¢Œí‘œ ì¶”ì¶œ: {store_info['name']} ({data_lat}, {data_lng})")
                        except:
                            pass
                    
                    # í‰ì  ì •ë³´ ì¶”ì¶œ
                    score_elem = block.find('p', class_='Score')
                    if score_elem:
                        score_span = score_elem.find('span')
                        if score_span:
                            try:
                                store_info['diningcode_score'] = int(score_span.get_text(strip=True))
                            except:
                                pass
                    
                    # ì‚¬ìš©ì í‰ì  ì¶”ì¶œ
                    user_score_elem = block.find('span', class_='score-text')
                    if user_score_elem:
                        try:
                            store_info['diningcode_rating'] = float(user_score_elem.get_text(strip=True))
                        except:
                            pass
                    
                    # ì¹´í…Œê³ ë¦¬ ì •ë³´ ì¶”ì¶œ
                    categories = []
                    category_elems = block.find_all('span', class_='Category')
                    for cat_elem in category_elems:
                        cat_text = cat_elem.get_text(strip=True)
                        if cat_text:
                            categories.append(cat_text)
                    store_info['raw_categories_diningcode'] = categories
                    
                    # ìœ íš¨í•œ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
                    if store_info['diningcode_place_id'] and store_info['name']:
                        stores.append(store_info)
                        logger.info(f"ê°€ê²Œ ì¶”ê°€: {store_info['name']} {store_info['branch']} (ID: {store_info['diningcode_place_id']})")
                        
                except Exception as e:
                    logger.warning(f"ê°€ê²Œ ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
            
            logger.info(f"{search_type}ì—ì„œ ì´ {len(stores)}ê°œ ê°€ê²Œ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"{search_type} ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            
        return stores
    
    def _load_more_results(self):
        """ë”ë³´ê¸° ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ì¶”ê°€ ê²°ê³¼ ë¡œë“œ"""
        try:
            max_attempts = 3  # ìµœëŒ€ 3ë²ˆê¹Œì§€ ë”ë³´ê¸° í´ë¦­
            
            for attempt in range(max_attempts):
                try:
                    # ë”ë³´ê¸° ë²„íŠ¼ ì°¾ê¸° (ì—¬ëŸ¬ ê°€ëŠ¥í•œ ì…€ë ‰í„° ì‹œë„)
                    more_button_selectors = [
                        "button[class*='more']",
                        "button[class*='More']", 
                        "a[class*='more']",
                        "div[class*='more']",
                        ".btn-more",
                        ".more-btn",
                        "[data-testid*='more']"
                    ]
                    
                    more_button = None
                    for selector in more_button_selectors:
                        try:
                            elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                            for element in elements:
                                if element.is_displayed() and element.is_enabled():
                                    text = element.text.lower()
                                    if any(keyword in text for keyword in ['ë”ë³´ê¸°', 'more', 'ë” ë³´ê¸°', 'ì¶”ê°€']):
                                        more_button = element
                                        break
                            if more_button:
                                break
                        except:
                            continue
                    
                    if more_button:
                        logger.info(f"ë”ë³´ê¸° ë²„íŠ¼ ë°œê²¬ (ì‹œë„ {attempt + 1}): {more_button.text}")
                        
                        # ë²„íŠ¼ì´ ë³´ì´ë„ë¡ ìŠ¤í¬ë¡¤
                        self.driver.execute_script("arguments[0].scrollIntoView(true);", more_button)
                        time.sleep(1)
                        
                        # í´ë¦­
                        more_button.click()
                        logger.info("ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì™„ë£Œ")
                        
                        # ë¡œë”© ëŒ€ê¸°
                        time.sleep(2)
                        
                        # ìƒˆë¡œìš´ ê²°ê³¼ê°€ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
                        new_poi_count = len(self.driver.find_elements(By.CLASS_NAME, "PoiBlock"))
                        logger.info(f"ë”ë³´ê¸° í›„ ì´ {new_poi_count}ê°œ ê°€ê²Œ ë°œê²¬")
                        
                    else:
                        logger.info(f"ë”ë³´ê¸° ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ (ì‹œë„ {attempt + 1})")
                        break
                        
                except Exception as e:
                    logger.warning(f"ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {e}")
                    continue
                    
        except Exception as e:
            logger.warning(f"ë”ë³´ê¸° ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {e}")

    def get_store_detail(self, store_info: Dict) -> Dict:
        """ê°€ê²Œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ (ê°•í™”ëœ íŒŒì‹±)"""
        def _get_detail():
            return self._extract_store_detail(store_info)
        
        return self.retry_on_failure(_get_detail, max_retries=3)

    def _extract_store_detail(self, store_info: Dict) -> Dict:
        """ì‹¤ì œ ìƒì„¸ ì •ë³´ ì¶”ì¶œ ë¡œì§ (ê°•í™”ëœ ì—ëŸ¬ í•¸ë“¤ë§)"""
        detail_info = store_info.copy()
        extraction_errors = []
        
        try:
            place_id = store_info.get('diningcode_place_id', '')
            if not place_id:
                logger.warning("place_idê°€ ì—†ì–´ ìƒì„¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return detail_info
            
            # ìƒì„¸ í˜ì´ì§€ URL ìƒì„±
            detail_url = f"https://www.diningcode.com/profile.php?rid={place_id}"
            logger.info(f"ìƒì„¸ í˜ì´ì§€ ì ‘ì†: {detail_url}")
            
            self.driver.get(detail_url)
            self.random_delay()
            
            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° (ë” ìœ ì—°í•œ ì¡°ê±´)
            try:
                WebDriverWait(self.driver, 15).until(
                    lambda driver: driver.find_elements(By.TAG_NAME, "body") and 
                    len(driver.page_source) > 1000 and
                    "diningcode" in driver.current_url.lower()
                )
                time.sleep(2)
            except TimeoutException:
                logger.warning("ìƒì„¸ í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ")
                time.sleep(3)
            
            # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # 1. ë©”ë‰´ ì •ë³´ ì¶”ì¶œ (ì—ëŸ¬ í•¸ë“¤ë§ ê°•í™”)
            try:
                menu_info = self._extract_menu_info(soup)
                detail_info.update(menu_info)
                logger.debug("ë©”ë‰´ ì •ë³´ ì¶”ì¶œ ì„±ê³µ")
            except Exception as e:
                extraction_errors.append(f"ë©”ë‰´ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                logger.warning(f"ë©”ë‰´ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            # 2. ê°€ê²© ì •ë³´ ì¶”ì¶œ (ì—ëŸ¬ í•¸ë“¤ë§ ê°•í™”)
            try:
                price_info = self._extract_price_info(soup)
                detail_info.update(price_info)
                
                # êµ¬ì¡°í™”ëœ ë©”ë‰´ ì •ë³´ê°€ ìˆìœ¼ë©´ menu_itemsì— ì¶”ê°€
                if 'structured_menu_items' in price_info and price_info['structured_menu_items']:
                    if 'menu_items' not in detail_info:
                        detail_info['menu_items'] = []
                    
                    # ê¸°ì¡´ menu_itemsì™€ ë³‘í•© (ì¤‘ë³µ ì œê±°)
                    existing_names = {item.get('name') for item in detail_info['menu_items'] if isinstance(item, dict)}
                    
                    for structured_item in price_info['structured_menu_items']:
                        if structured_item['name'] not in existing_names:
                            detail_info['menu_items'].append(structured_item)
                
                logger.debug("ê°€ê²© ì •ë³´ ì¶”ì¶œ ì„±ê³µ")
            except Exception as e:
                extraction_errors.append(f"ê°€ê²© ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                logger.warning(f"ê°€ê²© ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            # 3. ì˜ì—…ì‹œê°„ ì •ë³´ ì¶”ì¶œ (ì—ëŸ¬ í•¸ë“¤ë§ ê°•í™”)
            try:
                hours_info = self._extract_hours_info(soup)
                detail_info.update(hours_info)
                logger.debug("ì˜ì—…ì‹œê°„ ì •ë³´ ì¶”ì¶œ ì„±ê³µ")
            except Exception as e:
                extraction_errors.append(f"ì˜ì—…ì‹œê°„ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                logger.warning(f"ì˜ì—…ì‹œê°„ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            # 4. ì´ë¯¸ì§€ ì •ë³´ ìˆ˜ì§‘ ë° ë‹¤ìš´ë¡œë“œ (ì—ëŸ¬ í•¸ë“¤ë§ ê°•í™”)
            try:
                image_info = self._extract_image_info(soup)
                detail_info.update(image_info)
                
                # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš°
                if self.enable_image_download and self.image_manager:
                    logger.info(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {detail_info.get('name', 'Unknown')}")
                    download_result = self.image_manager.download_store_images(detail_info)
                    
                    # ë‹¤ìš´ë¡œë“œëœ ë¡œì»¬ ê²½ë¡œë¡œ ì—…ë°ì´íŠ¸ (ëŒ€í‘œ ì´ë¯¸ì§€ë§Œ)
                    if download_result.get('main_image'):
                        detail_info['main_image_local'] = download_result['main_image']
                        logger.info(f"ëŒ€í‘œ ì´ë¯¸ì§€ ë¡œì»¬ ì €ì¥: {os.path.basename(download_result['main_image'])}")
                        
                        # Supabase Storageì— ì—…ë¡œë“œ
                        try:
                            storage_url = self.image_manager.upload_to_supabase(
                                download_result['main_image'],
                                detail_info.get('name', 'unknown')
                            )
                            if storage_url:
                                detail_info['main_image_storage_url'] = storage_url
                                logger.info(f"âœ… Supabase Storage ì—…ë¡œë“œ ì„±ê³µ: {storage_url}")
                            else:
                                logger.warning("âŒ Supabase Storage ì—…ë¡œë“œ ì‹¤íŒ¨")
                        except Exception as upload_error:
                            logger.error(f"Supabase ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {upload_error}")
                    
                    # ë‹¤ìš´ë¡œë“œ í†µê³„
                    stats = download_result.get('download_stats', {})
                    if stats.get('successful', 0) > 0:
                        logger.info(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {stats['successful']}/{stats['total_attempted']}")
                    else:
                        logger.warning(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
                
                logger.debug("ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ ì„±ê³µ")
            except Exception as e:
                extraction_errors.append(f"ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                logger.warning(f"ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            # 5. ë¦¬ë·° ë° ì„¤ëª… ì •ë³´ ì¶”ì¶œ (ì—ëŸ¬ í•¸ë“¤ë§ ê°•í™”)
            try:
                review_info = self._extract_review_info(soup)
                detail_info.update(review_info)
                logger.debug("ë¦¬ë·° ì •ë³´ ì¶”ì¶œ ì„±ê³µ")
            except Exception as e:
                extraction_errors.append(f"ë¦¬ë·° ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                logger.warning(f"ë¦¬ë·° ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            # 6. ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ (ì—ëŸ¬ í•¸ë“¤ë§ ê°•í™”)
            try:
                contact_info = self._extract_contact_info(soup)
                detail_info.update(contact_info)
                logger.debug("ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ ì„±ê³µ")
            except Exception as e:
                extraction_errors.append(f"ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                logger.warning(f"ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            # 7. ì¢Œí‘œ ë° ì£¼ì†Œ ì •ë³´ ì¶”ì¶œ (ì—ëŸ¬ í•¸ë“¤ë§ ê°•í™”)
            try:
                coordinate_info = self._extract_coordinate_info(soup)
                detail_info.update(coordinate_info)
                logger.debug("ì¢Œí‘œ ì •ë³´ ì¶”ì¶œ ì„±ê³µ")
            except Exception as e:
                extraction_errors.append(f"ì¢Œí‘œ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                logger.warning(f"ì¢Œí‘œ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            # 8. ì£¼ì†Œ ì •ë³´ ì¶”ê°€ ì¶”ì¶œ (ì—ëŸ¬ í•¸ë“¤ë§ ê°•í™”)
            try:
                address_info = self._extract_address_info(soup)
                # ì£¼ì†Œê°€ ì—†ê±°ë‚˜ coordinate_infoì˜ ì£¼ì†Œê°€ ë” ìƒì„¸í•œ ê²½ìš° ì—…ë°ì´íŠ¸
                if address_info.get('address') and (not detail_info.get('address') or len(address_info['address']) > len(detail_info.get('address', ''))):
                    detail_info['address'] = address_info['address']
                if address_info.get('basic_address'):
                    detail_info['basic_address'] = address_info['basic_address']
                if address_info.get('road_address'):
                    detail_info['road_address'] = address_info['road_address']
                logger.debug("ì£¼ì†Œ ì •ë³´ ì¶”ì¶œ ì„±ê³µ")
            except Exception as e:
                extraction_errors.append(f"ì£¼ì†Œ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                logger.warning(f"ì£¼ì†Œ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            # 9. ë¬´í•œë¦¬í•„ ê´€ë ¨ ì •ë³´ ì¶”ì¶œ (ì—ëŸ¬ í•¸ë“¤ë§ ê°•í™”)
            try:
                refill_info = self._extract_refill_info(soup)
                detail_info.update(refill_info)
                logger.debug("ë¬´í•œë¦¬í•„ ì •ë³´ ì¶”ì¶œ ì„±ê³µ")
            except Exception as e:
                extraction_errors.append(f"ë¬´í•œë¦¬í•„ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                logger.warning(f"ë¬´í•œë¦¬í•„ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            # ì¶”ì¶œ ì˜¤ë¥˜ ìš”ì•½
            if extraction_errors:
                detail_info['extraction_errors'] = extraction_errors
                logger.warning(f"ë¶€ë¶„ì  ì¶”ì¶œ ì˜¤ë¥˜ ({len(extraction_errors)}ê°œ): {'; '.join(extraction_errors[:3])}")
            
            # ë°ì´í„° í’ˆì§ˆ ê²€ì¦
            quality_score = self._calculate_data_quality(detail_info)
            detail_info['data_quality_score'] = quality_score
            
            logger.info(f"ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {detail_info.get('name', 'Unknown')} - ì£¼ì†Œ: {detail_info.get('address', 'N/A')[:50]}... (í’ˆì§ˆì ìˆ˜: {quality_score}%)")
            
        except Exception as e:
            logger.error(f"ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
            detail_info['fatal_error'] = str(e)
            # ê¸°ë³¸ ì •ë³´ë¼ë„ ë°˜í™˜
            
        return detail_info
    
    def _calculate_data_quality(self, store_info: Dict) -> int:
        """ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-100ì )"""
        try:
            score = 0
            max_score = 100
            
            # í•„ìˆ˜ ì •ë³´ (40ì )
            if store_info.get('name'):
                score += 10
            if store_info.get('address'):
                score += 15
            if store_info.get('position_lat') and store_info.get('position_lng'):
                score += 15
            
            # ì—°ë½ì²˜ ì •ë³´ (20ì )
            if store_info.get('phone_number'):
                score += 20
            
            # ì˜ì—… ì •ë³´ (20ì )
            if store_info.get('open_hours'):
                score += 10
            if store_info.get('last_order') or store_info.get('break_time'):
                score += 5
            if store_info.get('holiday'):
                score += 5
            
            # ì¶”ê°€ ì •ë³´ (20ì )
            if store_info.get('price_range') or store_info.get('average_price'):
                score += 5
            if store_info.get('image_urls') and len(store_info.get('image_urls', [])) > 0:
                score += 5
            if store_info.get('refill_items') and len(store_info.get('refill_items', [])) > 0:
                score += 5
            if store_info.get('keywords') and len(store_info.get('keywords', [])) > 0:
                score += 5
            
            return min(score, max_score)
            
        except Exception as e:
            logger.debug(f"í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0

    def _extract_menu_info(self, soup: BeautifulSoup) -> Dict:
        """ë©”ë‰´ ì •ë³´ ì¶”ì¶œ (mcp-browserbase ê¸°ë°˜ ê°œì„ )"""
        menu_info = {
            'menu_items': [],
            'menu_categories': [],
            'signature_menu': [],
            'refill_menu_items': []  # ë¬´í•œë¦¬í•„ ë©”ë‰´ ì „ìš©
        }
        
        try:
            logger.info("ğŸ½ï¸ ë©”ë‰´ ì •ë³´ ì¶”ì¶œ ì‹œì‘...")
            
            # 1. ë¨¼ì € ë¸Œë¼ìš°ì € ê¸°ë°˜ ì •í™•í•œ ë©”ë‰´ ì¶”ì¶œ ì‹œë„
            browser_menu_items = self._extract_menu_with_browser_verification(soup)
            if browser_menu_items:
                menu_info['refill_menu_items'] = browser_menu_items
                logger.info(f"ë¸Œë¼ìš°ì € ê¸°ë°˜ ë©”ë‰´ ì¶”ì¶œ ì„±ê³µ: {len(browser_menu_items)}ê°œ")
            
            # 2. ê¸°ì¡´ ë¡œì§ìœ¼ë¡œ í´ë°±
            if not browser_menu_items:
                # ë¬´í•œë¦¬í•„ ë©”ë‰´ ì •ë³´ ì¶”ì¶œ (ìš´ì˜ ì •ë³´ ì„¹ì…˜ì—ì„œ)
                refill_menu_items = self._extract_refill_menu_from_operation_info(soup)
                menu_info['refill_menu_items'] = refill_menu_items
                
                # ì¼ë°˜ ë©”ë‰´ ì„¹ì…˜ì—ì„œ ë©”ë‰´ ì¶”ì¶œ
                general_menu_items = self._extract_general_menu_items(soup)
                menu_info['menu_items'] = general_menu_items
            else:
                # ë¸Œë¼ìš°ì € ê¸°ë°˜ ë©”ë‰´ë¥¼ ë¬´í•œë¦¬í•„ ë©”ë‰´ë¡œ ì„¤ì •
                menu_info['refill_menu_items'] = browser_menu_items
                menu_info['menu_items'] = []
                
                # êµ¬ì¡°í™”ëœ ë©”ë‰´ ì •ë³´ë¥¼ menu_items í•„ë“œì— ì €ì¥
                if browser_menu_items:
                    # ê¸°ì¡´ menu_itemsê°€ ì—†ìœ¼ë©´ ì´ˆê¸°í™”
                    if 'menu_items' not in detail_info:
                        detail_info['menu_items'] = []
                    
                    # ë¸Œë¼ìš°ì € ê¸°ë°˜ ë©”ë‰´ë¥¼ menu_itemsì— ì¶”ê°€
                    for menu_item in browser_menu_items:
                        structured_item = {
                            'name': menu_item['name'],
                            'price': menu_item['price'],
                            'price_numeric': menu_item.get('price_numeric', 0),
                            'is_recommended': menu_item.get('is_recommended', False),
                            'type': 'browser_verified',
                            'order': menu_item.get('order', 0)
                        }
                        detail_info['menu_items'].append(structured_item)
            
            # 3. ëŒ€í‘œ ë©”ë‰´ ì¶”ì¶œ
            signature_menu = self._extract_signature_menu(soup)
            menu_info['signature_menu'] = signature_menu
            
            # 4. ë¬´í•œë¦¬í•„ ê´€ë ¨ í‚¤ì›Œë“œ ê²€ìƒ‰
            keywords = self._extract_menu_keywords(soup)
            menu_info['keywords'] = keywords
            
            total_menu_count = len(menu_info['menu_items']) + len(menu_info['refill_menu_items'])
            logger.info(f"ë©”ë‰´ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ: ì´ {total_menu_count}ê°œ ë©”ë‰´")
            
        except Exception as e:
            logger.error(f"ë©”ë‰´ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return menu_info
    
    def _clean_menu_name(self, menu_name):
        """ë©”ë‰´ëª…ì—ì„œ ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°"""
        if not menu_name:
            return ""
        
        # ê´‘ê³ , í—¤ë” í…ìŠ¤íŠ¸ ì œê±°
        unwanted_keywords = [
            'Advertisement', 'advertisement', 'ë©”ë‰´ì •ë³´', 'ìš´ì˜ì •ë³´',
            'ì˜ì—…ì‹œê°„', 'ì „í™”ë²ˆí˜¸', 'ì£¼ì†Œ', 'í‰ì ', 'ë¦¬ë·°', 'ì‚¬ì§„'
        ]
        
        # ì¤„ë°”ê¿ˆ ì œê±° ë° ì •ë¦¬
        cleaned = menu_name.replace('\n', ' ').strip()
        
        # ë¶ˆí•„ìš”í•œ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš° ì œê±° ì‹œë„
        for keyword in unwanted_keywords:
            if keyword in cleaned:
                # í‚¤ì›Œë“œ ì´í›„ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                parts = cleaned.split(keyword)
                if len(parts) > 1:
                    cleaned = parts[-1].strip()
        
        # ì—°ì†ëœ ê³µë°± ì œê±°
        import re
        cleaned = re.sub(r'\s+', ' ', cleaned)
        
        return cleaned.strip()

    def _extract_menu_with_browser_verification(self, soup: BeautifulSoup) -> List[Dict]:
        """ë¸Œë¼ìš°ì €ì—ì„œ í™•ì¸í•œ êµ¬ì¡°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•œ ë©”ë‰´ ì •ë³´ ì¶”ì¶œ"""
        menu_items = []
        
        try:
            logger.info("ğŸ” ë¸Œë¼ìš°ì € ê¸°ë°˜ ë©”ë‰´ ì •ë³´ ì¶”ì¶œ ì‹œì‘...")
            
            # ë‹¤ì´ë‹ì½”ë“œ ë©”ë‰´ ì •ë³´ êµ¬ì¡°:
            # 1. "ë©”ë‰´ì •ë³´" í…ìŠ¤íŠ¸ë¥¼ ê°€ì§„ paragraph
            # 2. ê·¸ ë‹¤ìŒì— ì˜¤ëŠ” ul ë¦¬ìŠ¤íŠ¸
            # 3. ê° li ì•ˆì— ë‘ ê°œì˜ p íƒœê·¸ (ë©”ë‰´ëª…, ê°€ê²©)
            
            # Step 1: ë©”ë‰´ì •ë³´ í—¤ë” ì°¾ê¸°
            menu_header_elem = soup.find('p', string=lambda text: text and 'ë©”ë‰´ì •ë³´' in text.strip())
            
            if not menu_header_elem:
                # ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ë©”ë‰´ì •ë³´ í—¤ë” ì°¾ê¸°
                menu_header_elem = soup.find(text=lambda text: text and 'ë©”ë‰´ì •ë³´' in text.strip())
                if menu_header_elem:
                    menu_header_elem = menu_header_elem.parent
            
            if menu_header_elem:
                logger.info("âœ… ë©”ë‰´ì •ë³´ í—¤ë” ë°œê²¬")
                
                # Step 2: ë©”ë‰´ì •ë³´ í—¤ë” ë‹¤ìŒì— ì˜¤ëŠ” ul ë¦¬ìŠ¤íŠ¸ ì°¾ê¸°
                menu_list = menu_header_elem.find_next_sibling('ul')
                
                if menu_list:
                    logger.info("âœ… ë©”ë‰´ ë¦¬ìŠ¤íŠ¸ ë°œê²¬")
                    
                    # Step 3: ê° li ì•„ì´í…œì—ì„œ ë©”ë‰´ ì •ë³´ ì¶”ì¶œ
                    list_items = menu_list.find_all('li', recursive=False)  # ì§ì ‘ ìì‹ë§Œ
                    
                    for i, item in enumerate(list_items):
                        try:
                            # ê° li ì•ˆì˜ p íƒœê·¸ë“¤ ì°¾ê¸°
                            paragraphs = item.find_all('p')
                            
                            if len(paragraphs) >= 2:
                                # ì²« ë²ˆì§¸ p: ë©”ë‰´ëª…
                                menu_name_elem = paragraphs[0]
                                menu_name = menu_name_elem.get_text(strip=True)
                                # ê´‘ê³  í…ìŠ¤íŠ¸ë‚˜ ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
                                menu_name = self._clean_menu_name(menu_name)
                                
                                # ë‘ ë²ˆì§¸ p: ê°€ê²©
                                price_elem = paragraphs[1]
                                price_text = price_elem.get_text(strip=True)
                                
                                # ìœ íš¨ì„± ê²€ì‚¬
                                if (menu_name and price_text and 
                                    len(menu_name) > 0 and len(menu_name) < 100 and
                                    ('ì›' in price_text or price_text == '0ì›')):
                                    
                                    # ê°€ê²©ì—ì„œ ìˆ«ì ì¶”ì¶œ
                                    price_match = re.search(r'([\d,]+)\s*ì›', price_text)
                                    price_numeric = 0
                                    if price_match:
                                        price_numeric = int(price_match.group(1).replace(',', ''))
                                    
                                    # ì¶”ì²œ ì—¬ë¶€ í™•ì¸
                                    is_recommended = 'ì¶”ì²œ' in menu_name
                                    
                                    menu_item = {
                                        'name': menu_name,
                                        'price': price_text,
                                        'price_numeric': price_numeric,
                                        'is_recommended': is_recommended,
                                        'source': 'browser_verified',
                                        'order': i + 1
                                    }
                                    
                                    menu_items.append(menu_item)
                                    logger.debug(f"ğŸ“‹ ë©”ë‰´ ì¶”ì¶œ: {menu_name} - {price_text}")
                                    
                        except Exception as e:
                            logger.warning(f"ë©”ë‰´ ì•„ì´í…œ {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                            continue
                    
                    # ê°€ê²©ìˆœ ì •ë ¬ (0ì› ì œì™¸í•˜ê³ )
                    paid_items = [item for item in menu_items if item['price_numeric'] > 0]
                    free_items = [item for item in menu_items if item['price_numeric'] == 0]
                    
                    paid_items.sort(key=lambda x: x['price_numeric'])
                    menu_items = paid_items + free_items
                    
                    logger.info(f"âœ… ë¸Œë¼ìš°ì € ê¸°ë°˜ ë©”ë‰´ ì¶”ì¶œ ì™„ë£Œ: {len(menu_items)}ê°œ")
                    
                else:
                    logger.warning("ë©”ë‰´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            else:
                logger.warning("ë©”ë‰´ì •ë³´ í—¤ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                
        except Exception as e:
            logger.error(f"ë¸Œë¼ìš°ì € ê¸°ë°˜ ë©”ë‰´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return menu_items
    
    def _extract_general_menu_items(self, soup: BeautifulSoup) -> List[Dict]:
        """ì¼ë°˜ ë©”ë‰´ ì•„ì´í…œ ì¶”ì¶œ"""
        menu_items = []
        
        try:
            # ë©”ë‰´ ì„¹ì…˜ ì°¾ê¸°
            menu_sections = soup.find_all(['div', 'section', 'ul'], class_=re.compile(r'menu|Menu'))
            
            for section in menu_sections:
                # ë¦¬ë·° ì„¹ì…˜ ì œì™¸
                section_text = section.get_text().lower()
                if any(exclude in section_text for exclude in ['ë¦¬ë·°', 'í›„ê¸°', 'í‰ì ', 'ë³„ì ', 'ë°©ë¬¸ê¸°', 'ë¸”ë¡œê·¸']):
                    continue
                
                # ë©”ë‰´ ì•„ì´í…œ ì¶”ì¶œ
                items = section.find_all(['div', 'li', 'tr'], class_=re.compile(r'menu-item|menuitem|item'))
                
                for item in items:
                    item_text = item.get_text()
                    
                    # ë¦¬ë·° ê´€ë ¨ í…ìŠ¤íŠ¸ ì œì™¸
                    if any(exclude in item_text for exclude in ['ë¦¬ë·°', 'í›„ê¸°', 'í‰ì ', 'ë³„ì ', 'ë°©ë¬¸', 'ì˜ˆì•½', 'ë¸”ë¡œê·¸']):
                        continue
                    
                    menu_name = item.find(['span', 'div', 'h3', 'h4'], class_=re.compile(r'name|title'))
                    menu_price = item.find(['span', 'div'], class_=re.compile(r'price|cost|amount'))
                    
                    if menu_name:
                        menu_data = {
                            'name': menu_name.get_text(strip=True),
                            'price': menu_price.get_text(strip=True) if menu_price else '',
                        }
                        
                        # ê°€ê²© ì •ë³´ê°€ ìˆê³  ìœ íš¨í•œ ë©”ë‰´ì¸ ê²½ìš°ë§Œ ì¶”ê°€
                        if menu_data['name'] and len(menu_data['name']) > 2:
                            menu_items.append(menu_data)
            
            # í…Œì´ë¸” í˜•íƒœì˜ ë©”ë‰´ ì¶”ì¶œ
            menu_tables = soup.find_all('table')
            for table in menu_tables:
                rows = table.find_all('tr')
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 2:
                        name_cell = cells[0].get_text(strip=True)
                        price_cell = cells[1].get_text(strip=True)
                        
                        # ë¦¬ë·° ê´€ë ¨ í…ìŠ¤íŠ¸ ì œì™¸
                        if any(exclude in name_cell for exclude in ['ë¦¬ë·°', 'í›„ê¸°', 'í‰ì ', 'ë³„ì ']):
                            continue
                        
                        if name_cell and price_cell and 'ì›' in price_cell:
                            menu_items.append({
                                'name': name_cell,
                                'price': price_cell
                            })
            
        except Exception as e:
            logger.error(f"ì¼ë°˜ ë©”ë‰´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return menu_items
    
    def _extract_signature_menu(self, soup: BeautifulSoup) -> List[str]:
        """ëŒ€í‘œ ë©”ë‰´ ì¶”ì¶œ"""
        signature_menu = []
        
        try:
            signature_elements = soup.find_all(['div', 'span'], class_=re.compile(r'signature|recommend|popular'))
            for elem in signature_elements:
                elem_text = elem.get_text()
                
                # ë¦¬ë·° ê´€ë ¨ í…ìŠ¤íŠ¸ ì œì™¸
                if any(exclude in elem_text for exclude in ['ë¦¬ë·°', 'í›„ê¸°', 'í‰ì ', 'ë³„ì ', 'ë°©ë¬¸ê¸°']):
                    continue
                
                text = elem.get_text(strip=True)
                if text and len(text) > 2 and len(text) < 50:
                    signature_menu.append(text)
            
        except Exception as e:
            logger.error(f"ëŒ€í‘œ ë©”ë‰´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return signature_menu
    
    def _extract_menu_keywords(self, soup: BeautifulSoup) -> List[str]:
        """ë©”ë‰´ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []
        
        try:
            refill_keywords = ['ë¬´í•œë¦¬í•„', 'ë¬´ì œí•œ', 'ì…€í”„ë°”']
            food_keywords = ['ê³ ê¸°', 'ì‚¼ê²¹ì‚´', 'ì†Œê³ ê¸°', 'ë¼ì§€ê³ ê¸°', 'ì´ˆë°¥', 'íšŒ', 'í•´ì‚°ë¬¼', 'ì•¼ì±„']
            
            all_text = soup.get_text().lower()
            
            for keyword in refill_keywords + food_keywords:
                if keyword in all_text:
                    keywords.append(keyword)
            
            keywords = keywords[:10]  # ìµœëŒ€ 10ê°œ
            
        except Exception as e:
            logger.error(f"ë©”ë‰´ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return keywords
    
    def _extract_refill_menu_from_operation_info(self, soup: BeautifulSoup) -> List[Dict]:
        """ìš´ì˜ ì •ë³´ ì„¹ì…˜ì—ì„œ ë¬´í•œë¦¬í•„ ë©”ë‰´ ì¶”ì¶œ (JSON íŒŒì‹± + HTML íŒŒì‹±)"""
        menu_items = []
        
        try:
            logger.info("ğŸ” ìš´ì˜ ì •ë³´ì—ì„œ ë¬´í•œë¦¬í•„ ë©”ë‰´ ì¶”ì¶œ ì‹œì‘...")
            
            # 1. ìš°ì„  JSON ë°ì´í„°ì—ì„œ ë©”ë‰´ ì •ë³´ ì¶”ì¶œ
            menu_items = self._extract_menu_from_json_data(soup)
            
            if len(menu_items) > 0:
                logger.info(f"JSON ë°ì´í„°ì—ì„œ {len(menu_items)}ê°œ ë©”ë‰´ ì¶”ì¶œ ì„±ê³µ")
                return menu_items
            
            # 2. JSON ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ HTMLì—ì„œ ì¶”ì¶œ (ë¸Œë¼ìš°ì € ê²€ì¦ ê¸°ë°˜ ê°œì„ )
            logger.info("JSON ì¶”ì¶œ ì‹¤íŒ¨, HTML íŒŒì‹±ìœ¼ë¡œ ì „í™˜...")
            
            # ë©”ë‰´ì •ë³´ í—¤ë” ì •í™•íˆ ì°¾ê¸°
            menu_header = soup.find(text=lambda text: text and 'ë©”ë‰´ì •ë³´' in text.strip())
            
            if menu_header:
                logger.info(f"âœ… ë©”ë‰´ì •ë³´ í—¤ë” ë°œê²¬: {menu_header.strip()}")
                
                # í—¤ë”ì˜ ë¶€ëª¨ ìš”ì†Œì—ì„œ ë‹¤ìŒ í˜•ì œ ìš”ì†Œ ì°¾ê¸° (ë©”ë‰´ ë¦¬ìŠ¤íŠ¸)
                header_parent = menu_header.parent
                if header_parent:
                    menu_list = header_parent.find_next_sibling(['ul', 'ol', 'div', 'section'])
                    
                    if menu_list:
                        logger.info(f"âœ… ë©”ë‰´ ë¦¬ìŠ¤íŠ¸ ì»¨í…Œì´ë„ˆ ë°œê²¬: {menu_list.name}")
                        
                        # ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œë“¤ì—ì„œ ë©”ë‰´ ì¶”ì¶œ
                        list_items = menu_list.find_all('li', recursive=True)
                        
                        for item in list_items:
                            paragraphs = item.find_all('p')
                            if len(paragraphs) >= 2:
                                # ì²« ë²ˆì§¸ p: ë©”ë‰´ëª…, ë‘ ë²ˆì§¸ p: ê°€ê²©
                                menu_name = paragraphs[0].get_text(strip=True)
                                price_text = paragraphs[1].get_text(strip=True)
                                
                                # ë¦¬ë·° ë‚´ìš© í•„í„°ë§ (ì§§ê³  ëª…í™•í•œ ë©”ë‰´ ì •ë³´ë§Œ)
                                if (menu_name and price_text and 
                                    len(menu_name) < 50 and  # ë©”ë‰´ëª…ì´ ë„ˆë¬´ ê¸¸ì§€ ì•ŠìŒ
                                    'ì›' in price_text and  # ê°€ê²© ì •ë³´ í¬í•¨
                                    len(price_text) < 20):  # ê°€ê²© í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ì§€ ì•ŠìŒ
                                    
                                    # ê°€ê²©ì—ì„œ ìˆ«ì ì¶”ì¶œ
                                    price_match = re.search(r'([\d,]+)\s*ì›', price_text)
                                    price = price_match.group(1).replace(',', '') if price_match else ''
                                    
                                    menu_item = {
                                        'name': menu_name,
                                        'price': price_text,
                                        'price_numeric': int(price) if price.isdigit() else 0,
                                        'is_recommended': 'ì¶”ì²œ' in menu_name,
                                        'source': 'html_parsing'
                                    }
                                    menu_items.append(menu_item)
                                    logger.info(f"ğŸ“‹ ë©”ë‰´ ì¶”ì¶œ: {menu_name} - {price_text}")
                        
                        if len(menu_items) > 0:
                            logger.info(f"âœ… HTMLì—ì„œ {len(menu_items)}ê°œ ë©”ë‰´ ì¶”ì¶œ ì™„ë£Œ")
                            return menu_items
            
            # ê¸°ì¡´ ë¡œì§ìœ¼ë¡œ í´ë°±
            menu_section = None
            
            # ë©”ë‰´ì •ë³´ ë˜ëŠ” ê°€ê²©ì •ë³´ í—¤ë” ì°¾ê¸° (í´ë°±)
            menu_headers = soup.find_all(text=re.compile(r'(ë©”ë‰´\s*ì •ë³´|ê°€ê²©\s*ì •ë³´|ìš´ì˜\s*ì •ë³´)', re.IGNORECASE))
            for header in menu_headers:
                parent = header.parent
                if parent:
                    # ë©”ë‰´ì •ë³´ ì„¹ì…˜ì˜ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
                    section_container = parent.find_parent(['div', 'section', 'article'])
                    if section_container:
                        # ë¦¬ë·° ì„¹ì…˜ì´ ì•„ë‹Œì§€ í™•ì¸
                        section_text = section_container.get_text()[:200]
                        if not any(keyword in section_text for keyword in ['ë¦¬ë·°', 'í›„ê¸°', 'í‰ì ', 'ë³„ì ', 'ë§›ìˆ', 'ì¢‹ì•˜', 'ì¶”ì²œí•´ìš”']):
                            menu_section = section_container
                            break
            
            # ë©”ë‰´ì •ë³´ ì„¹ì…˜ì´ ì—†ìœ¼ë©´ ë” êµ¬ì²´ì ìœ¼ë¡œ ì°¾ê¸°
            if not menu_section:
                # ê°€ê²©ì´ í¬í•¨ëœ ìš”ì†Œë“¤ ì¤‘ì—ì„œ ë©”ë‰´ ì„¹ì…˜ ì°¾ê¸°
                price_elements = soup.find_all(text=re.compile(r'\d{1,3}(?:,\d{3})*\s*ì›'))
                for price_elem in price_elements:
                    container = price_elem.parent.find_parent(['div', 'section'])
                    if container:
                        container_text = container.get_text()
                        # ë©”ë‰´ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆê³  ë¦¬ë·° í‚¤ì›Œë“œê°€ ì—†ëŠ” ì„¹ì…˜
                        if (any(keyword in container_text for keyword in ['ë¬´í•œë¦¬í•„', 'ì¶”ì²œ', 'í• ì¸', 'ì„¸íŠ¸']) and
                            not any(keyword in container_text for keyword in ['ë¦¬ë·°', 'í›„ê¸°', 'í‰ì ', 'ë³„ì ', 'ë§›ìˆë‹¤', 'ì¢‹ì•˜', 'ë°©ë¬¸í–ˆ'])):
                            menu_section = container
                            break
            
            if not menu_section:
                logger.warning("ë©”ë‰´ ì •ë³´ ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return menu_items
            
            # ë©”ë‰´ ì •ë³´ë§Œ ì •í™•íˆ ì¶”ì¶œ
            section_text = menu_section.get_text()
            logger.debug(f"ë©”ë‰´ ì„¹ì…˜ í…ìŠ¤íŠ¸ (ì¼ë¶€): {section_text[:300]}...")
            
            # ë¦¬ë·° ë‚´ìš© ì œê±° - ë” ê°•ë ¥í•œ í•„í„°ë§
            cleaned_text = self._clean_text_for_menu_extraction(section_text)
            
            # ë©”ë‰´ ì •ë³´ íŒŒì‹±
            menu_items = self._extract_clean_menu_from_text(cleaned_text)
            
            # HTML êµ¬ì¡°ì—ì„œ ì§ì ‘ ì¶”ì¶œ (ë°±ì—…)
            if len(menu_items) == 0:
                menu_items = self._extract_menu_from_html_structure(menu_section)
            
            logger.info(f"HTMLì—ì„œ {len(menu_items)}ê°œ ë¬´í•œë¦¬í•„ ë©”ë‰´ ì¶”ì¶œ")
            
            # ì¶”ì¶œëœ ë©”ë‰´ ì •ë³´ ë¡œê¹…
            for item in menu_items:
                logger.debug(f"ë©”ë‰´: {item['name']} - {item['price']}")
            
        except Exception as e:
            logger.error(f"ìš´ì˜ ì •ë³´ì—ì„œ ë¬´í•œë¦¬í•„ ë©”ë‰´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return menu_items
    
    def _extract_menu_from_json_data(self, soup: BeautifulSoup) -> List[Dict]:
        """JavaScript aData ë³€ìˆ˜ì—ì„œ JSON ë©”ë‰´ ë°ì´í„° ì¶”ì¶œ"""
        menu_items = []
        
        try:
            # script íƒœê·¸ì—ì„œ aData ë³€ìˆ˜ ì°¾ê¸°
            script_tags = soup.find_all('script', string=re.compile(r'aData\s*='))
            
            for script in script_tags:
                script_content = script.string
                if not script_content:
                    continue
                
                # aData ë³€ìˆ˜ ê°’ ì¶”ì¶œ
                aData_match = re.search(r'aData\s*=\s*(\[.*?\]);', script_content, re.DOTALL)
                
                if aData_match:
                    import json
                    json_str = aData_match.group(1)
                    
                    try:
                        # JSON íŒŒì‹±
                        aData = json.loads(json_str)
                        logger.debug(f"aData íŒŒì‹± ì„±ê³µ: {len(aData)}ê°œ ìš”ì†Œ")
                        
                        # ì²« ë²ˆì§¸ ìš”ì†Œì—ì„œ ë©”ë‰´ ì •ë³´ ì°¾ê¸°
                        if len(aData) > 0 and isinstance(aData[0], dict):
                            first_item = aData[0]
                            
                            # ë©”ë‰´ ì •ë³´ ì°¾ê¸°
                            if 'menu' in first_item and isinstance(first_item['menu'], dict):
                                menu_data = first_item['menu']
                                
                                if 'list' in menu_data and isinstance(menu_data['list'], list):
                                    menu_list = menu_data['list']
                                    logger.info(f"JSONì—ì„œ {len(menu_list)}ê°œ ë©”ë‰´ ë°œê²¬")
                                    
                                    for menu_item in menu_list:
                                        if isinstance(menu_item, dict):
                                            menu_name = menu_item.get('menu', '').strip()
                                            price_str = menu_item.get('price', '').strip()
                                            is_best = menu_item.get('best', 0) == 1
                                            
                                            if menu_name and price_str:
                                                # ê°€ê²©ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ
                                                price_match = re.search(r'(\d{1,3}(?:,\d{3})*)', price_str)
                                                if price_match:
                                                    price_numeric = int(price_match.group(1).replace(',', ''))
                                                    
                                                    # ê°€ê²© ë²”ìœ„ ê²€ì¦ (5,000ì› ~ 100,000ì›)
                                                    if 5000 <= price_numeric <= 100000:
                                                        menu_items.append({
                                                            'name': menu_name,
                                                            'price': price_str,
                                                            'price_numeric': price_numeric,
                                                            'is_recommended': is_best,
                                                            'description': 'ì¶”ì²œ' if is_best else ''
                                                        })
                                                        
                                                        logger.debug(f"JSON ë©”ë‰´ ì¶”ê°€: {menu_name} - {price_str}")
                                    
                                    # ê°€ê²© ìˆœ ì •ë ¬
                                    menu_items.sort(key=lambda x: x['price_numeric'])
                                    break
                                    
                    except json.JSONDecodeError as e:
                        logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                        continue
                        
        except Exception as e:
            logger.error(f"JSON ë©”ë‰´ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return menu_items
    
    def _clean_text_for_menu_extraction(self, text: str) -> str:
        """ë©”ë‰´ ì¶”ì¶œì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì •ë¦¬ - ë¦¬ë·° ë‚´ìš© ì œê±°"""
        lines = text.split('\n')
        cleaned_lines = []
        
        # ë¦¬ë·°ì„± í‚¤ì›Œë“œ íŒ¨í„´
        review_patterns = [
            r'.*[ë§›ìˆ|ì¢‹ì•˜|ë³„ë¡œ|ì¶”ì²œí•´|í›„ê¸°|ë¦¬ë·°|í‰ì |ë³„ì |ë‹¤ì‹œ|ì¬ë°©ë¬¸|ë§›ì§‘|ë¸”ë¡œê·¸].*',
            r'.*ì•ˆë…•í•˜ì„¸ìš”.*',
            r'.*ì†Œê°œí•´.*ë“œë¦´ê²Œìš”.*',
            r'.*ë¨¹ì–´ë³´ë‹ˆ.*',
            r'.*ì°¾ê³  ê³„ì‹ ë‹¤ë©´.*',
            r'.*~\s*ì˜¤ëŠ˜.*',
            r'.*ì œë¡œì—ìš”.*',
            r'.*ì‡ë‹˜ë“¤.*',
            r'.*ã„·ã„·.*',
            r'.*ë³´ì´ì‹œì£ .*',
            r'.*íì—…ì‹ ê³ .*',
            r'.*ì •ë³´ìˆ˜ì •.*',
            r'.*ìœ ëª…í•˜ë‹¤ëŠ”.*',
            r'.*ëŠë‚Œì€ ì•„ë‹ˆì—ˆì§€ë§Œ.*',
            r'.*ê°„ì ˆí• ë•Œ.*',
        ]
        
        for line in lines:
            line = line.strip()
            if len(line) < 3:  # ë„ˆë¬´ ì§§ì€ ë¼ì¸ ì œì™¸
                continue
                
            # ë¦¬ë·°ì„± ë‚´ìš© ì œì™¸
            is_review = False
            for pattern in review_patterns:
                if re.match(pattern, line, re.IGNORECASE):
                    is_review = True
                    break
            
            if not is_review:
                # ë©”ë‰´ë‚˜ ê°€ê²© ì •ë³´ê°€ í¬í•¨ëœ ë¼ì¸ë§Œ í¬í•¨
                if (re.search(r'\d{1,3}(?:,\d{3})*\s*ì›', line) or
                    any(keyword in line for keyword in ['ë¬´í•œë¦¬í•„', 'ì¶”ì²œ', 'í• ì¸', 'ì„¸íŠ¸', 'ì½”ìŠ¤'])):
                    cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines)
    
    def _extract_clean_menu_from_text(self, text: str) -> List[Dict]:
        """ì •ë¦¬ëœ í…ìŠ¤íŠ¸ì—ì„œ ë©”ë‰´ ì •ë³´ë§Œ ì •í™•íˆ ì¶”ì¶œ"""
        menu_items = []
        
        try:
            # ë©”ë‰´ + ê°€ê²© íŒ¨í„´ (ë” ì •í™•í•œ íŒ¨í„´)
            patterns = [
                # íŒ¨í„´ 1: "a ë¼ì§€ëª¨ë“¬ ë¬´í•œë¦¬í•„ ì¶”ì²œ 17,900ì›" (ë‹¤ì´ë‹ì½”ë“œ í‘œì¤€)
                r'([a-z])\s+([ê°€-í£\s\w()]+?(?:ë¬´í•œë¦¬í•„|ë·”í˜|ì…€í”„ë°”)[ê°€-í£\s\w()]*?(?:ì¶”ì²œ|í• ì¸)?)\s*(\d{1,3}(?:,\d{3})*)\s*ì›',
                
                # íŒ¨í„´ 2: "ë¼ì§€ëª¨ë“¬ ë¬´í•œë¦¬í•„ ì¶”ì²œ 17,900ì›"
                r'([ê°€-í£\s\w()]+?(?:ë¬´í•œë¦¬í•„|ë·”í˜|ì…€í”„ë°”)[ê°€-í£\s\w()]*?(?:ì¶”ì²œ|í• ì¸)?)\s+(\d{1,3}(?:,\d{3})*)\s*ì›',
                
                # íŒ¨í„´ 3: "ë¬´í•œë¦¬í•„ ì¶”ì²œ\n17,900ì›" (ì¤„ë°”ê¿ˆ êµ¬ë¶„)
                r'([ê°€-í£\s\w()]*?(?:ë¬´í•œë¦¬í•„|ë·”í˜|ì…€í”„ë°”)[ê°€-í£\s\w()]*?(?:ì¶”ì²œ|í• ì¸)?)\s*\n\s*(\d{1,3}(?:,\d{3})*)\s*ì›',
                
                # íŒ¨í„´ 4: "í‰ì¼ ì ì‹¬ ë¬´í•œë¦¬í•„ 15,900ì›" (ì‹œê°„ëŒ€ë³„)
                r'([ê°€-í£\s\w()]*?(?:í‰ì¼|ì£¼ë§|ì ì‹¬|ì €ë…|ì˜¤ì „|ì˜¤í›„)[ê°€-í£\s\w()]*?(?:ë¬´í•œë¦¬í•„|ë·”í˜|ì…€í”„ë°”)[ê°€-í£\s\w()]*?)\s*(\d{1,3}(?:,\d{3})*)\s*ì›',
                
                # íŒ¨í„´ 5: "ì„±ì¸ ë¬´í•œë¦¬í•„ 19,900ì›" (ì—°ë ¹ëŒ€ë³„)
                r'([ê°€-í£\s\w()]*?(?:ì„±ì¸|ì–´ë¦°ì´|ì´ˆë“±|ì¤‘ë“±|ê³ ë“±|í•™ìƒ)[ê°€-í£\s\w()]*?(?:ë¬´í•œë¦¬í•„|ë·”í˜|ì…€í”„ë°”)[ê°€-í£\s\w()]*?)\s*(\d{1,3}(?:,\d{3})*)\s*ì›'
            ]
            
            for pattern in patterns:
                matches = re.findall(pattern, text, re.MULTILINE | re.IGNORECASE)
                
                for match in matches:
                    if len(match) == 3:  # íŒ¨í„´ 1
                        menu_name = match[1].strip()
                        price = match[2].strip()
                    else:  # ë‚˜ë¨¸ì§€ íŒ¨í„´
                        menu_name = match[0].strip()
                        price = match[1].strip()
                    
                    # ë©”ë‰´ëª… ì •ë¦¬
                    menu_name = re.sub(r'^[a-z]\s+', '', menu_name)  # ì•ì˜ ì•ŒíŒŒë²³ ì œê±°
                    menu_name = re.sub(r'\s+', ' ', menu_name)  # ê³µë°± ì •ë¦¬
                    
                    # ìœ íš¨ì„± ê²€ì‚¬
                    if (len(menu_name) >= 3 and len(menu_name) <= 30 and 
                        price.replace(',', '').isdigit()):
                        
                        # ê°€ê²© ë²”ìœ„ ê²€ì¦ (5,000ì› ~ 50,000ì›)
                        price_num = int(price.replace(',', ''))
                        if 5000 <= price_num <= 50000:
                            # ì¤‘ë³µ ì²´í¬
                            if not any(item['name'] == menu_name for item in menu_items):
                                menu_items.append({
                                    'name': menu_name,
                                    'price': f"{price}ì›",
                                    'price_numeric': price_num,
                                    'description': ''
                                })
            
            # ê°€ê²© ìˆœ ì •ë ¬
            menu_items.sort(key=lambda x: x['price_numeric'])
            
        except Exception as e:
            logger.error(f"ì •ë¦¬ëœ í…ìŠ¤íŠ¸ì—ì„œ ë©”ë‰´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return menu_items
    
    def _extract_menu_from_html_structure(self, menu_section: BeautifulSoup) -> List[Dict]:
        """HTML êµ¬ì¡°ì—ì„œ ì§ì ‘ ë©”ë‰´ ì •ë³´ ì¶”ì¶œ"""
        menu_items = []
        
        try:
            # ê°€ê²©ì´ í¬í•¨ëœ ìš”ì†Œë“¤ ì°¾ê¸°
            price_elements = menu_section.find_all(text=re.compile(r'\d{1,3}(?:,\d{3})*\s*ì›'))
            
            for price_elem in price_elements:
                price_text = price_elem.strip()
                price_match = re.search(r'(\d{1,3}(?:,\d{3})*)\s*ì›', price_text)
                
                if price_match:
                    price = price_match.group(1)
                    price_num = int(price.replace(',', ''))
                    
                    # ê°€ê²© ë²”ìœ„ ê²€ì¦
                    if 5000 <= price_num <= 50000:
                        # ê°€ê²© ìš”ì†Œì˜ ë¶€ëª¨ë‚˜ í˜•ì œì—ì„œ ë©”ë‰´ëª… ì°¾ê¸°
                        parent = price_elem.parent
                        if parent:
                            parent_text = parent.get_text()
                            
                            # ë©”ë‰´ëª… ì¶”ì¶œ (ê°€ê²© ì œì™¸)
                            menu_text = re.sub(r'\d{1,3}(?:,\d{3})*\s*ì›', '', parent_text).strip()
                            
                            # ë¬´í•œë¦¬í•„ ê´€ë ¨ ë©”ë‰´ì¸ì§€ í™•ì¸
                            if (any(keyword in menu_text for keyword in ['ë¬´í•œë¦¬í•„', 'ë·”í˜', 'ì…€í”„ë°”', 'ì¶”ì²œ', 'í• ì¸']) and
                                len(menu_text) >= 3 and len(menu_text) <= 30 and
                                not any(keyword in menu_text for keyword in ['ë¦¬ë·°', 'í›„ê¸°', 'ë§›ìˆ', 'ì¢‹ì•˜'])):
                                
                                # ì¤‘ë³µ ì²´í¬
                                if not any(item['name'] == menu_text for item in menu_items):
                                    menu_items.append({
                                        'name': menu_text,
                                        'price': f"{price}ì›",
                                        'price_numeric': price_num,
                                        'description': ''
                                    })
            
        except Exception as e:
            logger.error(f"HTML êµ¬ì¡°ì—ì„œ ë©”ë‰´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return menu_items
    
    def _extract_price_info(self, soup: BeautifulSoup) -> Dict:
        """ê°€ê²© ì •ë³´ ì¶”ì¶œ (ë¬´í•œë¦¬í•„ ë©”ë‰´ íŠ¹í™”)"""
        price_info = {
            'price_range': '',
            'average_price': '',
            'price_details': [],
            'refill_prices': []  # ë¬´í•œë¦¬í•„ ê°€ê²© ì •ë³´ ì „ìš©
        }
        
        try:
            logger.info("ğŸ’° ê°€ê²© ì •ë³´ ì¶”ì¶œ ì‹œì‘...")
            
            # 1. ë¬´í•œë¦¬í•„ ë©”ë‰´ ê°€ê²© ì •ë³´ ì¶”ì¶œ (ìš´ì˜ ì •ë³´ì—ì„œ)
            refill_prices = self._extract_refill_prices_from_operation_info(soup)
            price_info['refill_prices'] = refill_prices
            
            # 2. ì¼ë°˜ ê°€ê²© ì •ë³´ ì¶”ì¶œ
            found_prices = []
            menu_prices = []
            
            # ê°€ê²© íŒ¨í„´ (ë¦¬ë·° ì œì™¸)
            price_patterns = [
                # ê¸°ë³¸ ê°€ê²© íŒ¨í„´
                r'(\d{1,3}(?:,\d{3})*)\s*ì›',
                # ë©”ë‰´ëª…ê³¼ ê°€ê²©
                r'([ê°€-í£\s\w()]+?)\s*[:ï¼š]\s*(\d{1,3}(?:,\d{3})*)\s*ì›',
                # ë²”ìœ„ ê°€ê²©
                r'(\d{1,3}(?:,\d{3})*)\s*ì›?\s*[-~]\s*(\d{1,3}(?:,\d{3})*)\s*ì›',
                # ë§Œì› ë‹¨ìœ„
                r'(\d{1,2})\s*ë§Œ\s*(\d{1,3}(?:,\d{3})*)\s*ì›',
                r'(\d{1,2})\s*ë§Œì›'
            ]
            
            # 3. ê°€ê²© ê´€ë ¨ ìš”ì†Œ ì°¾ê¸° (ë¦¬ë·° ì„¹ì…˜ ì œì™¸)
            price_selectors = [
                '.menu-price', '.price', '.cost', '.amount',
                '[class*="price"]', '[class*="Price"]', '[class*="cost"]',
                '[class*="menu"]', '[class*="Menu"]'
            ]
            
            for selector in price_selectors:
                elements = soup.select(selector)
                
                for elem in elements:
                    # ë¦¬ë·° ì„¹ì…˜ì¸ì§€ í™•ì¸
                    elem_text = elem.get_text()
                    if any(exclude in elem_text for exclude in ['ë¦¬ë·°', 'í›„ê¸°', 'í‰ì ', 'ë³„ì ', 'ë°©ë¬¸', 'ë¸”ë¡œê·¸']):
                        continue
                    
                    # ë¶€ëª¨ ìš”ì†Œë„ ì²´í¬
                    parent = elem.parent
                    if parent:
                        parent_text = parent.get_text()
                        if any(exclude in parent_text for exclude in ['ë¦¬ë·°', 'í›„ê¸°', 'í‰ì ', 'ë³„ì ', 'ë°©ë¬¸ê¸°']):
                            continue
                    
                    text = elem.get_text(strip=True)
                    
                    # ê°€ê²©ê³¼ ê´€ë ¨ ì—†ëŠ” í…ìŠ¤íŠ¸ ì œì™¸
                    if any(exclude in text for exclude in ['í›„ê¸°', 'ë¦¬ë·°', 'í‰ì ', 'ë³„ì ', 'ì¶”ì²œ', 'ë°©ë¬¸', 'ì˜ˆì•½']):
                        continue
                    
                    for pattern in price_patterns:
                        matches = re.findall(pattern, text)
                        for match in matches:
                            if isinstance(match, tuple):
                                if len(match) == 2:
                                    # ë©”ë‰´ëª…ê³¼ ê°€ê²© ë˜ëŠ” ë²”ìœ„ ê°€ê²©
                                    if match[0].replace(',', '').isdigit() and match[1].replace(',', '').isdigit():
                                        # ë²”ìœ„ ê°€ê²©
                                        found_prices.extend([match[0], match[1]])
                                    else:
                                        # ë©”ë‰´ëª…ê³¼ ê°€ê²©
                                        menu_name, price = match
                                        if price.replace(',', '').isdigit():
                                            found_prices.append(price)
                                            menu_prices.append(f"{menu_name}: {price}ì›")
                                else:
                                    found_prices.extend([m for m in match if m.replace(',', '').isdigit()])
                            else:
                                if match.replace(',', '').isdigit():
                                    found_prices.append(match)
            
            # 4. í…Œì´ë¸”ì—ì„œ ê°€ê²© ì •ë³´ ì¶”ì¶œ
            tables = soup.find_all('table')
            for table in tables:
                table_text = table.get_text()
                
                # ë¦¬ë·° í…Œì´ë¸” ì œì™¸
                if any(exclude in table_text for exclude in ['ë¦¬ë·°', 'í›„ê¸°', 'í‰ì ', 'ë³„ì ']):
                    continue
                
                rows = table.find_all('tr')
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    for cell in cells:
                        cell_text = cell.get_text(strip=True)
                        
                        # ê°€ê²© íŒ¨í„´ ì°¾ê¸°
                        price_matches = re.findall(r'(\d{1,3}(?:,\d{3})*)\s*ì›', cell_text)
                        for price in price_matches:
                            found_prices.append(price)
            
            # 5. ë¬´í•œë¦¬í•„ ê°€ê²©ì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
            if refill_prices:
                # ë¬´í•œë¦¬í•„ ê°€ê²©ìœ¼ë¡œ price_details êµ¬ì„± (êµ¬ì¡°í™”ëœ JSON)
                structured_menu_items = []
                for refill_price in refill_prices:
                    menu_item = {
                        'name': refill_price['name'],
                        'price': refill_price['price'],
                        'price_numeric': refill_price.get('price_numeric', 0),
                        'is_recommended': refill_price.get('is_recommended', False),
                        'type': 'refill'
                    }
                    structured_menu_items.append(menu_item)
                    
                    # ìˆ«ì ê°€ê²© ì¶”ì¶œ
                    price_num = refill_price.get('price_numeric', 0)
                    if price_num > 0:
                        found_prices.append(str(price_num))
                
                # êµ¬ì¡°í™”ëœ ë©”ë‰´ ì •ë³´ë¥¼ menu_itemsì— ì €ì¥ (detail_infoê°€ ìˆëŠ” ê²½ìš°)
                # price_detailsëŠ” ê¸°ì¡´ ë°°ì—´ í˜•íƒœ ìœ ì§€
                price_info['structured_menu_items'] = structured_menu_items
                
                logger.info(f"ë¬´í•œë¦¬í•„ ê°€ê²© ì •ë³´ {len(refill_prices)}ê°œ ì‚¬ìš©")
            
            # 6. ê°€ê²© ì •ë³´ ì •ë¦¬ ë° ê²€ì¦
            if found_prices:
                # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
                unique_prices = list(set(found_prices))
                
                # ìˆ«ìë¡œ ë³€í™˜ ê°€ëŠ¥í•œ ê°€ê²©ë§Œ í•„í„°ë§
                numeric_prices = []
                for price in unique_prices:
                    try:
                        # ë§Œì› ë‹¨ìœ„ ì²˜ë¦¬
                        if 'ë§Œ' in price:
                            num = float(price.replace('ë§Œ', '').replace(',', ''))
                            numeric_prices.append(int(num * 10000))
                        else:
                            num = int(price.replace(',', ''))
                            # í•©ë¦¬ì ì¸ ê°€ê²© ë²”ìœ„ í•„í„°ë§ (1,000ì› ~ 100,000ì›)
                            if 1000 <= num <= 100000:
                                numeric_prices.append(num)
                    except:
                        continue
                
                if numeric_prices:
                    # ê°€ê²© í†µê³„ ê³„ì‚°
                    min_price = min(numeric_prices)
                    max_price = max(numeric_prices)
                    avg_price = sum(numeric_prices) // len(numeric_prices)
                    
                    # ê°€ê²© ì •ë³´ ì„¤ì •
                    price_info['price_range'] = f"{min_price:,}ì› ~ {max_price:,}ì›"
                    price_info['average_price'] = f"{avg_price:,}ì›"
                    price_info['price_details'] = menu_prices[:10]  # ìµœëŒ€ 10ê°œ
                    
                    logger.info(f"ê°€ê²© ì •ë³´ ì¶”ì¶œ ì„±ê³µ: {len(numeric_prices)}ê°œ ê°€ê²©, í‰ê·  {avg_price:,}ì›")
                else:
                    logger.warning("ìœ íš¨í•œ ê°€ê²© ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
            # 7. ì¶”ê°€ ê°€ê²© ì •ë³´ ê²€ìƒ‰ (Selenium í™œìš©)
            if not price_info['price_details'] and self.driver:
                try:
                    # ë©”ë‰´ íƒ­ì´ë‚˜ ê°€ê²© ì •ë³´ ë²„íŠ¼ í´ë¦­ ì‹œë„
                    menu_buttons = self.driver.find_elements(By.CSS_SELECTOR, 'button, a, div')
                    
                    for button in menu_buttons:
                        button_text = button.text.lower()
                        if any(keyword in button_text for keyword in ['ë©”ë‰´', 'ê°€ê²©', 'menu', 'price', 'ìš´ì˜ì •ë³´']):
                            try:
                                logger.info(f"ë©”ë‰´/ê°€ê²© ì •ë³´ ë²„íŠ¼ í´ë¦­ ì‹œë„: {button_text}")
                                self.driver.execute_script("arguments[0].scrollIntoView();", button)
                                time.sleep(1)
                                button.click()
                                time.sleep(3)
                                
                                # ì—…ë°ì´íŠ¸ëœ í˜ì´ì§€ì—ì„œ ê°€ê²© ì •ë³´ ì¬ì¶”ì¶œ
                                updated_soup = BeautifulSoup(self.driver.page_source, 'html.parser')
                                updated_price_info = self._extract_price_info_from_soup(updated_soup)
                                
                                if updated_price_info['price_details']:
                                    price_info.update(updated_price_info)
                                    logger.info("ë©”ë‰´ í´ë¦­ í›„ ê°€ê²© ì •ë³´ ìˆ˜ì§‘ ì„±ê³µ")
                                    break
                                    
                            except Exception as e:
                                logger.debug(f"ë©”ë‰´ ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨: {e}")
                                continue
                
                except Exception as e:
                    logger.debug(f"ì¶”ê°€ ê°€ê²© ì •ë³´ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            
            logger.info(f"ê°€ê²© ì •ë³´ ì¶”ì¶œ ì™„ë£Œ: {len(price_info['price_details'])}ê°œ ì¼ë°˜ê°€ê²©, {len(price_info['refill_prices'])}ê°œ ë¬´í•œë¦¬í•„ê°€ê²©")
            
        except Exception as e:
            logger.error(f"ê°€ê²© ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())
        
        return price_info
    
    def _extract_price_info_from_soup(self, soup: BeautifulSoup) -> Dict:
        """BeautifulSoup ê°ì²´ì—ì„œ ê°€ê²© ì •ë³´ë§Œ ì¶”ì¶œí•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
        price_info = {
            'price_range': '',
            'average_price': '',
            'price_details': []
        }
        
        try:
            # ê°„ë‹¨í•œ ê°€ê²© ì¶”ì¶œ ë¡œì§ (ì¬ê·€ í˜¸ì¶œ ë°©ì§€)
            price_elements = soup.find_all(['span', 'div', 'td'], string=re.compile(r'\d+.*ì›'))
            
            prices = []
            for elem in price_elements:
                text = elem.get_text(strip=True)
                price_matches = re.findall(r'(\d{1,3}(?:,\d{3})*)\s*ì›', text)
                prices.extend(price_matches)
            
            if prices:
                numeric_prices = []
                for price in prices:
                    try:
                        num = int(price.replace(',', ''))
                        if 1000 <= num <= 100000:
                            numeric_prices.append(num)
                    except:
                        continue
                
                if numeric_prices:
                    min_price = min(numeric_prices)
                    max_price = max(numeric_prices)
                    avg_price = sum(numeric_prices) // len(numeric_prices)
                    
                    price_info['price_range'] = f"{min_price:,}ì› ~ {max_price:,}ì›"
                    price_info['average_price'] = f"{avg_price:,}ì›"
                    price_info['price_details'] = [f"{p}ì›" for p in prices[:10]]
            
        except Exception as e:
            logger.debug(f"í—¬í¼ ê°€ê²© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return price_info

    def _extract_hours_info(self, detail_soup: BeautifulSoup) -> Dict[str, Any]:
        """ì˜ì—…ì‹œê°„, ë¸Œë ˆì´í¬íƒ€ì„, ë¼ìŠ¤íŠ¸ì˜¤ë” ì •ë³´ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
        hours_info = {
            'open_hours': '',
            'holiday': '',
            'break_time': '',
            'last_order': ''
        }
        
        try:
            # 1ë‹¨ê³„: ì˜ì—…ì‹œê°„ì´ í¬í•¨ëœ ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ì°¾ê¸°
            hours_section = None
            list_items = detail_soup.find_all('li')
            
            for li in list_items:
                li_text = li.get_text()
                if any(keyword in li_text for keyword in ['ì˜ì—…ì‹œê°„', 'ë¼ìŠ¤íŠ¸ ì˜¤ë”', 'ë¼ìŠ¤íŠ¸ì˜¤ë”', 'íœ´ë¬´']):
                    hours_section = li
                    logger.info(f"ì˜ì—…ì‹œê°„ ì„¹ì…˜ ë°œê²¬: {li_text[:150]}...")
                    break
            
            if not hours_section:
                logger.warning("ì˜ì—…ì‹œê°„ ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return hours_info
            
            # 2ë‹¨ê³„: Seleniumìœ¼ë¡œ í† ê¸€ ë²„íŠ¼ í´ë¦­í•˜ì—¬ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ (ê°œì„ ëœ ë²„ì „)
            expanded_hours_text = ""
            if self.driver:
                try:
                    # ì˜ì—…ì‹œê°„ í† ê¸€ ë²„íŠ¼ ì°¾ê¸° (ë” ì •í™•í•œ ë°©ë²•)
                    toggle_buttons = self.driver.find_elements(By.CSS_SELECTOR, 'button')
                    
                    for button in toggle_buttons:
                        try:
                            # ë²„íŠ¼ì˜ ë¶€ëª¨ë‚˜ í˜•ì œ ìš”ì†Œì—ì„œ ì˜ì—…ì‹œê°„ ê´€ë ¨ í…ìŠ¤íŠ¸ í™•ì¸
                            parent = button.find_element(By.XPATH, '..')
                            parent_text = parent.text
                            
                            if any(keyword in parent_text for keyword in ['ì˜ì—…ì‹œê°„', 'ë¼ìŠ¤íŠ¸ì˜¤ë”', 'ë¼ìŠ¤íŠ¸ ì˜¤ë”']):
                                # í† ê¸€ ì•„ì´ì½˜ì´ ìˆëŠ” ë²„íŠ¼ì¸ì§€ í™•ì¸
                                imgs = button.find_elements(By.TAG_NAME, 'img')
                                for img in imgs:
                                    alt_text = img.get_attribute('alt') or ''
                                    if 'í† ê¸€' in alt_text or 'toggle' in alt_text.lower():
                                        logger.info(f"ì˜ì—…ì‹œê°„ í† ê¸€ ë²„íŠ¼ í´ë¦­ ì‹œë„")
                                        
                                        # ìŠ¤í¬ë¡¤í•˜ì—¬ ë²„íŠ¼ì´ ë³´ì´ë„ë¡ ì¡°ì •
                                        self.driver.execute_script("arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});", button)
                                        time.sleep(1)
                                        
                                        # í´ë¦­
                                        self.driver.execute_script("arguments[0].click();", button)
                                        
                                        # í† ê¸€ ì• ë‹ˆë©”ì´ì…˜ ë° ë°ì´í„° ë¡œë”© ëŒ€ê¸° (ê°œì„ ëœ ëŒ€ê¸° ì‹œê°„)
                                        logger.info("ì˜ì—…ì‹œê°„ ìƒì„¸ ì •ë³´ ë¡œë”© ëŒ€ê¸° ì¤‘...")
                                        time.sleep(5)  # 3ì´ˆì—ì„œ 5ì´ˆë¡œ ì¦ê°€
                                        
                                        # ì¶”ê°€ ë°ì´í„° ë¡œë”© í™•ì¸
                                        for wait_attempt in range(3):
                                            updated_soup = BeautifulSoup(self.driver.page_source, 'html.parser')
                                            page_text = updated_soup.get_text()
                                            
                                            # ìš”ì¼ë³„ ì •ë³´ê°€ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
                                            weekday_count = sum(1 for day in ['ì›”ìš”ì¼', 'í™”ìš”ì¼', 'ìˆ˜ìš”ì¼', 'ëª©ìš”ì¼', 'ê¸ˆìš”ì¼', 'í† ìš”ì¼', 'ì¼ìš”ì¼'] if day in page_text)
                                            if weekday_count >= 3:  # ìµœì†Œ 3ê°œ ìš”ì¼ ì •ë³´ê°€ ìˆìœ¼ë©´ ë¡œë”© ì™„ë£Œë¡œ íŒë‹¨
                                                logger.info(f"ìš”ì¼ë³„ ì •ë³´ ë¡œë”© ì™„ë£Œ: {weekday_count}ê°œ ìš”ì¼ ê°ì§€")
                                                break
                                            
                                            logger.info(f"ì¶”ê°€ ë¡œë”© ëŒ€ê¸° ì¤‘... (ì‹œë„ {wait_attempt + 1}/3)")
                                            time.sleep(2)
                                        
                                        # í™•ì¥ëœ ì˜ì—…ì‹œê°„ ì •ë³´ ì°¾ê¸° (ê°œì„ ëœ ë°©ë²•)
                                        updated_list_items = updated_soup.find_all('li')
                                        
                                        for updated_li in updated_list_items:
                                            updated_text = updated_li.get_text()
                                            
                                            # ë‚ ì§œë³„ ì˜ì—…ì‹œê°„ íŒ¨í„´ì´ ìˆëŠ”ì§€ í™•ì¸
                                            if re.search(r'\d{1,2}ì›”\s*\d{1,2}ì¼\s*\([ì›”í™”ìˆ˜ëª©ê¸ˆí† ì¼]\)', updated_text):
                                                # ì‹¤ì œ ì˜ì—…ì‹œê°„ ì •ë³´ì¸ì§€ í™•ì¸ (ë¦¬ë·°ë‚˜ ê¸°íƒ€ ë‚´ìš© ì œì™¸)
                                                if not any(word in updated_text for word in ['ë¸”ë¡œê·¸', 'í›„ê¸°', 'ë¦¬ë·°', 'ë°©ë¬¸', 'ì¶”ì²œ', 'ë§›ì§‘']):
                                                    if any(keyword in updated_text for keyword in ['ì˜ì—…ì‹œê°„', 'ë¼ìŠ¤íŠ¸ì˜¤ë”', 'íœ´ë¬´ì¼', 'íœ´ë¬´']):
                                                        expanded_hours_text = updated_text
                                                        logger.info(f"í™•ì¥ëœ ì˜ì—…ì‹œê°„ ì •ë³´ ìˆ˜ì§‘: {expanded_hours_text[:200]}...")
                                                        break
                                        
                                        # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì¶”ê°€ ì‹œë„
                                        if not expanded_hours_text:
                                            page_text = updated_soup.get_text()
                                            
                                            # ê°œì„ ëœ ë‚ ì§œë³„ ì˜ì—…ì‹œê°„ íŒ¨í„´
                                            date_patterns = [
                                                r'(\d{1,2}ì›”\s*\d{1,2}ì¼\s*\([ì›”í™”ìˆ˜ëª©ê¸ˆí† ì¼]\)\s*[^\n]*?ì˜ì—…ì‹œê°„[^\n]*?\d{1,2}:\d{2}[^\n]*)',
                                                r'(\d{1,2}ì›”\s*\d{1,2}ì¼\s*\([ì›”í™”ìˆ˜ëª©ê¸ˆí† ì¼]\)\s*[^\n]*?íœ´ë¬´[^\n]*)',
                                                r'([ì›”í™”ìˆ˜ëª©ê¸ˆí† ì¼]ìš”ì¼[^\n]*?ì˜ì—…ì‹œê°„[^\n]*?\d{1,2}:\d{2}[^\n]*)',
                                                r'([ì›”í™”ìˆ˜ëª©ê¸ˆí† ì¼]ìš”ì¼[^\n]*?íœ´ë¬´[^\n]*)'
                                            ]
                                            
                                            found_dates = []
                                            for pattern in date_patterns:
                                                matches = re.findall(pattern, page_text)
                                                found_dates.extend(matches)
                                            
                                            if found_dates:
                                                expanded_hours_text = ' '.join(found_dates)
                                                logger.info(f"íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì˜ì—…ì‹œê°„ ì •ë³´ ìˆ˜ì§‘: {len(found_dates)}ê°œ í•­ëª©")
                                        
                                        break
                                break
                        except Exception as e:
                            continue
                            
                except Exception as e:
                    logger.warning(f"í† ê¸€ ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨: {e}")
            
            # 3ë‹¨ê³„: ê°œì„ ëœ ì˜ì—…ì‹œê°„ íŒŒì‹± ë¡œì§ ì ìš©
            hours_text = expanded_hours_text if expanded_hours_text else hours_section.get_text(strip=True)
            logger.info(f"íŒŒì‹±í•  ì˜ì—…ì‹œê°„ í…ìŠ¤íŠ¸: {hours_text[:300]}...")
            
            # ê°œì„ ëœ íŒŒì‹± í•¨ìˆ˜ ì‚¬ìš©
            parsed_hours = self._parse_hours_info_improved(hours_text)
            
            # ê²°ê³¼ ë³‘í•©
            hours_info.update(parsed_hours)
            
            logger.info(f"ìµœì¢… ì˜ì—…ì‹œê°„: {hours_info['open_hours']}")
            logger.info(f"íœ´ë¬´ì¼: {hours_info['holiday']}")
            logger.info(f"ë¸Œë ˆì´í¬íƒ€ì„: {hours_info['break_time']}")
            logger.info(f"ë¼ìŠ¤íŠ¸ì˜¤ë”: {hours_info['last_order']}")
            
        except Exception as e:
            logger.error(f"ì˜ì—…ì‹œê°„ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())
        
        return hours_info

    def _parse_hours_info_improved(self, hours_text: str) -> Dict[str, Any]:
        """
        ê°œì„ ëœ ì˜ì—…ì‹œê°„ ì •ë³´ íŒŒì‹±
        
        Args:
            hours_text: ì˜ì—…ì‹œê°„ ê´€ë ¨ í…ìŠ¤íŠ¸
            
        Returns:
            Dict: íŒŒì‹±ëœ ì˜ì—…ì‹œê°„ ì •ë³´
        """
        hours_info = {
            'open_hours': '',
            'holiday': '',
            'break_time': '',
            'last_order': ''
        }
        
        try:
            # 1ë‹¨ê³„: ë¼ìŠ¤íŠ¸ì˜¤ë” ì •ë³´ ì¶”ì¶œ
            last_order_patterns = [
                r'ë¼ìŠ¤íŠ¸\s*ì˜¤ë”\s*[:ï¼š]?\s*(\d{1,2}:\d{2})',
                r'ë¼ìŠ¤íŠ¸ì˜¤ë”\s*[:ï¼š]?\s*(\d{1,2}:\d{2})',
                r'L\.?O\.?\s*[:ï¼š]?\s*(\d{1,2}:\d{2})',
                r'ì£¼ë¬¸\s*ë§ˆê°\s*[:ï¼š]?\s*(\d{1,2}:\d{2})',
                r'ë§ˆì§€ë§‰\s*ì£¼ë¬¸\s*[:ï¼š]?\s*(\d{1,2}:\d{2})'
            ]
            
            for pattern in last_order_patterns:
                matches = re.findall(pattern, hours_text, re.IGNORECASE)
                if matches:
                    hours_info['last_order'] = matches[0]
                    logger.info(f"ë¼ìŠ¤íŠ¸ì˜¤ë” ì¶”ì¶œ: {hours_info['last_order']}")
                    break
            
            # 2ë‹¨ê³„: ë¸Œë ˆì´í¬íƒ€ì„ ì •ë³´ ì¶”ì¶œ
            break_patterns = [
                r'ë¸Œë ˆì´í¬\s*íƒ€ì„?\s*[:ï¼š]?\s*(\d{1,2}:\d{2})\s*[-~]\s*(\d{1,2}:\d{2})',
                r'ë¸Œë ˆì´í¬\s*[:ï¼š]?\s*(\d{1,2}:\d{2})\s*[-~]\s*(\d{1,2}:\d{2})',
                r'íœ´ê²Œì‹œê°„\s*[:ï¼š]?\s*(\d{1,2}:\d{2})\s*[-~]\s*(\d{1,2}:\d{2})',
                r'ì‰¬ëŠ”ì‹œê°„\s*[:ï¼š]?\s*(\d{1,2}:\d{2})\s*[-~]\s*(\d{1,2}:\d{2})',
                r'ì¤‘ê°„íœ´ì‹\s*[:ï¼š]?\s*(\d{1,2}:\d{2})\s*[-~]\s*(\d{1,2}:\d{2})'
            ]
            
            for pattern in break_patterns:
                matches = re.findall(pattern, hours_text, re.IGNORECASE)
                if matches:
                    start_time, end_time = matches[0]
                    hours_info['break_time'] = f"{start_time}-{end_time}"
                    logger.info(f"ë¸Œë ˆì´í¬íƒ€ì„ ì¶”ì¶œ: {hours_info['break_time']}")
                    break
            
            # 3ë‹¨ê³„: ìš”ì¼ë³„ ì˜ì—…ì‹œê°„ ì¶”ì¶œ
            day_hours = {}
            holiday_days = []
            
            # í•œêµ­ì–´ ìš”ì¼ ë§¤í•‘
            day_mapping = {
                'ì›”': 'ì›”', 'í™”': 'í™”', 'ìˆ˜': 'ìˆ˜', 'ëª©': 'ëª©', 'ê¸ˆ': 'ê¸ˆ', 'í† ': 'í† ', 'ì¼': 'ì¼',
                'ì›”ìš”ì¼': 'ì›”', 'í™”ìš”ì¼': 'í™”', 'ìˆ˜ìš”ì¼': 'ìˆ˜', 'ëª©ìš”ì¼': 'ëª©', 
                'ê¸ˆìš”ì¼': 'ê¸ˆ', 'í† ìš”ì¼': 'í† ', 'ì¼ìš”ì¼': 'ì¼'
            }
            
            # ì˜ì—…ì‹œê°„ íŒ¨í„´ë“¤ (ìš°ì„ ìˆœìœ„ ìˆœ)
            hour_patterns = [
                # íŒ¨í„´ 1: "ì˜ì—…ì‹œê°„: 11:00 - 23:00" í˜•íƒœ
                r'ì˜ì—…ì‹œê°„\s*[:ï¼š]?\s*(\d{1,2}:\d{2})\s*[-~]\s*(\d{1,2}:\d{2})',
                
                # íŒ¨í„´ 2: "11:00 - 23:00" ë‹¨ìˆœ í˜•íƒœ
                r'(\d{1,2}:\d{2})\s*[-~]\s*(\d{1,2}:\d{2})',
                
                # íŒ¨í„´ 3: "ì˜¤ì „ 11ì‹œ - ì˜¤í›„ 11ì‹œ" í˜•íƒœ
                r'ì˜¤ì „\s*(\d{1,2})ì‹œ?\s*[-~]\s*ì˜¤í›„\s*(\d{1,2})ì‹œ?',
                
                # íŒ¨í„´ 4: "11ì‹œ - 23ì‹œ" í˜•íƒœ
                r'(\d{1,2})ì‹œ\s*[-~]\s*(\d{1,2})ì‹œ'
            ]
            
            # íœ´ë¬´ íŒ¨í„´ë“¤
            holiday_patterns = [
                r'([ì›”í™”ìˆ˜ëª©ê¸ˆí† ì¼])ìš”ì¼\s*íœ´ë¬´',
                r'([ì›”í™”ìˆ˜ëª©ê¸ˆí† ì¼])\s*[:ï¼š]?\s*íœ´ë¬´',
                r'ë§¤ì£¼\s*([ì›”í™”ìˆ˜ëª©ê¸ˆí† ì¼])ìš”ì¼\s*íœ´ë¬´',
                r'íœ´ë¬´ì¼?\s*[:ï¼š]?\s*([ì›”í™”ìˆ˜ëª©ê¸ˆí† ì¼])ìš”ì¼?'
            ]
            
            # í…ìŠ¤íŠ¸ë¥¼ ì¤„ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ì²˜ë¦¬
            lines = hours_text.split('\n')
            
            current_day = None
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                # ìš”ì¼ ê°ì§€ (ë‚ ì§œ í˜•íƒœì—ì„œ ìš”ì¼ ì¶”ì¶œ)
                date_day_match = re.search(r'\d{1,2}ì›”\s*\d{1,2}ì¼\s*\(([ì›”í™”ìˆ˜ëª©ê¸ˆí† ì¼])\)', line)
                if date_day_match:
                    current_day = date_day_match.group(1)
                else:
                    # ì¼ë°˜ ìš”ì¼ ê°ì§€
                    for day_text, day_short in day_mapping.items():
                        if day_text in line:
                            current_day = day_short
                            break
                
                # íœ´ë¬´ì¼ í™•ì¸
                for pattern in holiday_patterns:
                    matches = re.findall(pattern, line)
                    if matches:
                        for match in matches:
                            day = day_mapping.get(match, match)
                            if day and day not in holiday_days:
                                holiday_days.append(day)
                                logger.info(f"íœ´ë¬´ì¼ ë°œê²¬: {day}ìš”ì¼")
                
                # ì˜ì—…ì‹œê°„ ì¶”ì¶œ
                for pattern in hour_patterns:
                    matches = re.findall(pattern, line)
                    if matches and current_day:
                        start_time, end_time = matches[0]
                        
                        # ì‹œê°„ í˜•ì‹ ì •ê·œí™”
                        if ':' not in start_time:  # "11ì‹œ" í˜•íƒœ
                            start_time = f"{start_time.zfill(2)}:00"
                        if ':' not in end_time:    # "23ì‹œ" í˜•íƒœ
                            end_time = f"{end_time.zfill(2)}:00"
                        
                        # ì˜¤ì „/ì˜¤í›„ ì²˜ë¦¬
                        if 'ì˜¤ì „' in line and 'ì˜¤í›„' in line:
                            end_hour = int(end_time.split(':')[0])
                            if end_hour != 12:
                                end_hour += 12
                            end_time = f"{end_hour:02d}:{end_time.split(':')[1]}"
                        
                        hours_str = f"{start_time}-{end_time}"
                        day_hours[current_day] = hours_str
                        logger.info(f"ì˜ì—…ì‹œê°„ ë°œê²¬: {current_day}ìš”ì¼ {hours_str}")
                        break
            
            # 4ë‹¨ê³„: íŒ¨í„´ ë¶„ì„ìœ¼ë¡œ ëˆ„ë½ëœ ìš”ì¼ ë³´ì™„
            all_days = ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼']
            collected_days = set(day_hours.keys())
            missing_days = [d for d in all_days if d not in collected_days and d not in holiday_days]
            
            logger.info(f"ìˆ˜ì§‘ëœ ìš”ì¼: {list(collected_days)}")
            logger.info(f"íœ´ë¬´ì¼: {holiday_days}")
            logger.info(f"ëˆ„ë½ëœ ìš”ì¼: {missing_days}")
            
            # íŒ¨í„´ ë¶„ì„í•˜ì—¬ ëˆ„ë½ëœ ìš”ì¼ ë³´ì™„
            if len(day_hours) >= 1 and missing_days:
                # ì£¼ì¤‘/ì£¼ë§ íŒ¨í„´ ë¶„ì„
                weekday_hours = []
                weekend_hours = []
                
                for day in ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ']:
                    if day in day_hours:
                        weekday_hours.append(day_hours[day])
                
                for day in ['í† ', 'ì¼']:
                    if day in day_hours:
                        weekend_hours.append(day_hours[day])
                
                # ì£¼ì¤‘ íŒ¨í„´ ì ìš© (2ê°œ ì´ìƒ ë™ì¼í•œ ì‹œê°„ì´ë©´ íŒ¨í„´ìœ¼ë¡œ ì¸ì •)
                if weekday_hours and len(weekday_hours) >= 2:
                    most_common_weekday = max(set(weekday_hours), key=weekday_hours.count)
                    if weekday_hours.count(most_common_weekday) >= 2:
                        for day in ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ']:
                            if day in missing_days:
                                day_hours[day] = most_common_weekday
                                logger.info(f"{day}ìš”ì¼ì— ì£¼ì¤‘ íŒ¨í„´ ì ìš©: {most_common_weekday}")
                
                # ì£¼ë§ íŒ¨í„´ ì ìš©
                if weekend_hours and len(weekend_hours) >= 1:
                    most_common_weekend = max(set(weekend_hours), key=weekend_hours.count)
                    for day in ['í† ', 'ì¼']:
                        if day in missing_days and day not in holiday_days:
                            day_hours[day] = most_common_weekend
                            logger.info(f"{day}ìš”ì¼ì— ì£¼ë§ íŒ¨í„´ ì ìš©: {most_common_weekend}")
                
                # ì „ì²´ ë™ì¼ íŒ¨í„´ ì ìš© (ëª¨ë“  ìš”ì¼ì´ ê°™ì€ ì‹œê°„ì¸ ê²½ìš°)
                if len(day_hours) == 1:
                    common_hours = list(day_hours.values())[0]
                    for day in missing_days:
                        if day not in holiday_days:
                            day_hours[day] = common_hours
                            logger.info(f"{day}ìš”ì¼ì— ì „ì²´ íŒ¨í„´ ì ìš©: {common_hours}")
            
            # 5ë‹¨ê³„: ìµœì¢… ì˜ì—…ì‹œê°„ ë¬¸ìì—´ ìƒì„±
            if day_hours:
                hours_parts = []
                days_order = ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼']
                
                for day in days_order:
                    if day in day_hours:
                        hours_parts.append(f"{day}: {day_hours[day]}")
                    elif day in holiday_days:
                        hours_parts.append(f"{day}: íœ´ë¬´")
                
                # ê¸°ë³¸ ì˜ì—…ì‹œê°„ ì„¤ì •
                hours_info['open_hours'] = ', '.join(hours_parts)
                
                # ë¼ìŠ¤íŠ¸ì˜¤ë”ê°€ ìˆìœ¼ë©´ ë§¨ ë§ˆì§€ë§‰ì— ì¶”ê°€
                if hours_info['last_order']:
                    hours_info['open_hours'] += f" / ë¼ìŠ¤íŠ¸ì˜¤ë”: {hours_info['last_order']}"
            
            # 6ë‹¨ê³„: íœ´ë¬´ì¼ ì„¤ì •
            if holiday_days:
                unique_holidays = list(set(holiday_days))
                if len(unique_holidays) == 1:
                    hours_info['holiday'] = f"ë§¤ì£¼ {unique_holidays[0]}ìš”ì¼ íœ´ë¬´"
                else:
                    hours_info['holiday'] = f"ë§¤ì£¼ {', '.join(unique_holidays)}ìš”ì¼ íœ´ë¬´"
            
        except Exception as e:
            logger.error(f"ì˜ì—…ì‹œê°„ ì •ë³´ íŒŒì‹± ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())
        
        return hours_info

    def _extract_image_info(self, soup: BeautifulSoup) -> Dict:
        """ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ ë° Storage ì—…ë¡œë“œ (ê¸°ì¡´ ìŠ¤í‚¤ë§ˆ í™œìš©)"""
        image_info = {
            'main_image': '',
            'image_urls': []
        }
        
        try:
            logger.info("ğŸ–¼ï¸ ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ ì‹œì‘...")
            
            main_image_found = False
            original_image_url = None
            
            # ë°©ë²• 1: íŠ¹ì • í´ë˜ìŠ¤ë‚˜ IDë¥¼ ê°€ì§„ ëŒ€í‘œ ì´ë¯¸ì§€ ì°¾ê¸°
            main_image_selectors = [
                '.restaurant-image img',
                '.main-image img', 
                '.hero-image img',
                '.restaurant-photo img',
                '.poi-image img',
                '.store-image img',
                '#main-image',
                '.photo-main img',
                'img[alt*="ëŒ€í‘œ"]',
                'img[alt*="ë©”ì¸"]',
                'img[alt*="main"]'
            ]
            
            for selector in main_image_selectors:
                img_elem = soup.select_one(selector)
                if img_elem:
                    src = img_elem.get('src') or img_elem.get('data-src') or img_elem.get('data-lazy')
                    if src and src.startswith('http'):
                        original_image_url = src
                        main_image_found = True
                        logger.info(f"ëŒ€í‘œ ì´ë¯¸ì§€ ë°œê²¬ (ë°©ë²•1): {selector}")
                        break
            
            # ë°©ë²• 2: í˜ì´ì§€ ìƒë‹¨ì˜ ì²« ë²ˆì§¸ í° ì´ë¯¸ì§€ ì°¾ê¸°
            if not main_image_found:
                all_images = soup.find_all('img')
                for img in all_images[:10]:  # ìƒìœ„ 10ê°œ ì´ë¯¸ì§€ë§Œ ì²´í¬
                    src = img.get('src') or img.get('data-src') or img.get('data-lazy')
                    if src and src.startswith('http'):
                        # ì‘ì€ ì•„ì´ì½˜ì´ë‚˜ UI ìš”ì†Œ ì œì™¸
                        if not any(keyword in src.lower() for keyword in [
                            'icon', 'logo', 'btn', 'button', 'arrow', 'close', 
                            'addphoto', 'placeholder', 'default', 'loading'
                        ]):
                            # alt í…ìŠ¤íŠ¸ë‚˜ í´ë˜ìŠ¤ì—ì„œ ëŒ€í‘œ ì´ë¯¸ì§€ íŒíŠ¸ ì°¾ê¸°
                            alt_text = img.get('alt', '').lower()
                            class_name = ' '.join(img.get('class', [])).lower()
                            
                            if (any(keyword in alt_text for keyword in ['ëŒ€í‘œ', 'main', 'ê°€ê²Œ', 'ë§¤ì¥']) or
                                any(keyword in class_name for keyword in ['main', 'hero', 'banner', 'primary'])):
                                original_image_url = src
                                main_image_found = True
                                logger.info(f"ëŒ€í‘œ ì´ë¯¸ì§€ ë°œê²¬ (ë°©ë²•2): alt='{alt_text}', class='{class_name}'")
                                break
            
            # ë°©ë²• 3: JavaScriptë¡œ ë™ì ìœ¼ë¡œ ë¡œë“œëœ ì´ë¯¸ì§€ ì°¾ê¸°
            if not main_image_found and self.driver:
                try:
                    js_script = """
                    var images = document.querySelectorAll('img');
                    var largestImage = null;
                    var maxArea = 0;
                    
                    for (var i = 0; i < Math.min(images.length, 10); i++) {
                        var img = images[i];
                        var rect = img.getBoundingClientRect();
                        var area = rect.width * rect.height;
                        
                        if (rect.top < window.innerHeight / 2 && area > 10000) {
                            var src = img.src || img.getAttribute('data-src') || img.getAttribute('data-lazy');
                            if (src && src.startsWith('http') && 
                                !src.includes('icon') && !src.includes('logo') && 
                                !src.includes('btn') && !src.includes('addphoto')) {
                                if (area > maxArea) {
                                    maxArea = area;
                                    largestImage = src;
                                }
                            }
                        }
                    }
                    return largestImage;
                    """
                    
                    js_main_image = self.driver.execute_script(js_script)
                    if js_main_image:
                        original_image_url = js_main_image
                        main_image_found = True
                        logger.info(f"ëŒ€í‘œ ì´ë¯¸ì§€ ë°œê²¬ (ë°©ë²•3-JS): ê°€ì¥ í° ì´ë¯¸ì§€")
                        
                except Exception as js_error:
                    logger.warning(f"JavaScript ì´ë¯¸ì§€ ê²€ìƒ‰ ì‹¤íŒ¨: {js_error}")
            
            # ë°©ë²• 4: ë°±ì—… - ì²« ë²ˆì§¸ ìœ íš¨í•œ ì´ë¯¸ì§€
            if not main_image_found:
                all_images = soup.find_all('img')
                for img in all_images[:15]:
                    src = img.get('src') or img.get('data-src') or img.get('data-lazy')
                    if src and src.startswith('http'):
                        if not any(keyword in src.lower() for keyword in [
                            'icon', 'logo', 'btn', 'button', 'arrow', 'close', 
                            'addphoto', 'placeholder', 'default', 'loading',
                            'common', 'ui', 'sprite'
                        ]):
                            original_image_url = src
                            main_image_found = True
                            logger.info(f"ëŒ€í‘œ ì´ë¯¸ì§€ ë°œê²¬ (ë°©ë²•4-ë°±ì—…): ì²« ë²ˆì§¸ ìœ íš¨ ì´ë¯¸ì§€")
                            break
            
            # ì´ë¯¸ì§€ ì²˜ë¦¬ ë° Storage ì—…ë¡œë“œ
            if main_image_found and original_image_url:
                logger.info(f"ğŸ“¸ ì›ë³¸ ì´ë¯¸ì§€ URL: {original_image_url[:100]}...")
                
                # ê¸°ë³¸ì ìœ¼ë¡œ ì›ë³¸ URLì„ ì €ì¥
                image_info['main_image'] = original_image_url
                image_info['image_urls'] = [original_image_url]
                
                # ì´ë¯¸ì§€ ë§¤ë‹ˆì €ê°€ í™œì„±í™”ëœ ê²½ìš° Storage ì—…ë¡œë“œ ì‹œë„
                if self.enable_image_download and self.image_manager:
                    try:
                        self.stats['images_processed'] += 1
                        
                        # ê°€ê²Œëª… ì¶”ì¶œ (íŒŒì¼ëª… ìƒì„±ìš©)
                        store_name = soup.find('h1')
                        if store_name:
                            store_name = store_name.get_text(strip=True)
                        else:
                            store_name = "unknown_store"
                        
                        logger.info(f"ğŸ”„ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° Storage ì—…ë¡œë“œ ì‹œì‘: {store_name}")
                        
                        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ â†’ ì²˜ë¦¬ â†’ Storage ì—…ë¡œë“œ
                        result = self.image_manager.process_and_upload_image(
                            original_image_url, 
                            store_name
                        )
                        
                        if result.get('storage_url'):
                            # Storage URLì„ main_imageë¡œ ì„¤ì • (ìš°ì„  ì‚¬ìš©)
                            image_info['main_image'] = result['storage_url']
                            # image_urlsì—ëŠ” ì›ë³¸ê³¼ Storage URL ëª¨ë‘ ì €ì¥
                            image_info['image_urls'] = [original_image_url, result['storage_url']]
                            
                            self.stats['images_uploaded'] += 1
                            logger.info(f"âœ… ì´ë¯¸ì§€ Storage ì—…ë¡œë“œ ì„±ê³µ!")
                            logger.info(f"ğŸ”— Storage URL: {result['storage_url']}")
                        else:
                            logger.warning(f"âš ï¸ Storage ì—…ë¡œë“œ ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                            logger.info("ğŸ“Œ ì›ë³¸ URLì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                            
                    except Exception as upload_error:
                        logger.error(f"ì´ë¯¸ì§€ ì—…ë¡œë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {upload_error}")
                        # ì‹¤íŒ¨í•´ë„ ì›ë³¸ URLì€ ìœ ì§€
                
                logger.info(f"âœ… ì´ë¯¸ì§€ ì •ë³´ ì²˜ë¦¬ ì™„ë£Œ")
                logger.info(f"ğŸ“¸ ìµœì¢… main_image: {image_info['main_image'][:100]}...")
                logger.info(f"ğŸ“¸ image_urls ê°œìˆ˜: {len(image_info['image_urls'])}")
            else:
                logger.warning("âŒ ê°€ê²Œ ëŒ€í‘œ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())
        
        return image_info

    def _extract_review_info(self, soup: BeautifulSoup) -> Dict:
        """ë¦¬ë·° ë° ì„¤ëª… ì •ë³´ ì¶”ì¶œ (ê°•í™”)"""
        review_info = {
            'description': '',
            'review_summary': '',
            'keywords': [],
            'atmosphere': ''
        }
        
        try:
            # ì„¤ëª… í…ìŠ¤íŠ¸ ì¶”ì¶œ
            desc_elements = soup.find_all(['div', 'p'], class_=re.compile(r'desc|description|intro|summary'))
            descriptions = []
            
            for elem in desc_elements:
                text = elem.get_text(strip=True)
                if text and len(text) > 10:
                    descriptions.append(text)
            
            if descriptions:
                review_info['description'] = ' '.join(descriptions[:3])  # ìµœëŒ€ 3ê°œ í•©ì¹˜ê¸°
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ (ë¬´í•œë¦¬í•„ ê´€ë ¨)
            refill_keywords = ['ë¬´í•œë¦¬í•„', 'ë¬´ì œí•œ', 'ì…€í”„ë°”', 'ë¦¬í•„ê°€ëŠ¥', 'ë¬´ë£Œë¦¬í•„']
            food_keywords = ['ê³ ê¸°', 'ì‚¼ê²¹ì‚´', 'ì†Œê³ ê¸°', 'ë¼ì§€ê³ ê¸°', 'ì´ˆë°¥', 'íšŒ', 'í•´ì‚°ë¬¼', 'ì•¼ì±„']
            
            all_text = soup.get_text().lower()
            found_keywords = []
            
            for keyword in refill_keywords + food_keywords:
                if keyword in all_text:
                    found_keywords.append(keyword)
            
            review_info['keywords'] = found_keywords[:10]  # ìµœëŒ€ 10ê°œ
            
            logger.info(f"ë¦¬ë·° ì •ë³´ ì¶”ì¶œ: {len(found_keywords)}ê°œ í‚¤ì›Œë“œ")
            
        except Exception as e:
            logger.error(f"ë¦¬ë·° ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return review_info

    def _extract_contact_info(self, soup: BeautifulSoup) -> Dict:
        """ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ (ê°•í™”)"""
        contact_info = {
            'phone_number': '',
            'website': '',
            'social_media': []
        }
        
        try:
            # ì „í™”ë²ˆí˜¸ íŒ¨í„´ ë§¤ì¹­
            all_text = soup.get_text()
            phone_patterns = [
                r'0\d{1,2}-\d{3,4}-\d{4}',  # 02-1234-5678
                r'0\d{9,10}',               # 0212345678
                r'\d{3}-\d{3,4}-\d{4}'      # 010-1234-5678
            ]
            
            for pattern in phone_patterns:
                matches = re.findall(pattern, all_text)
                if matches:
                    raw_phone = matches[0]
                    # ì „í™”ë²ˆí˜¸ ì •ê·œí™” (0507 â†’ 07 ë¬¸ì œ í•´ê²°)
                    contact_info['phone_number'] = self._normalize_phone_number(raw_phone)
                    break
            
            # ì›¹ì‚¬ì´íŠ¸ ë§í¬ ì°¾ê¸°
            links = soup.find_all('a', href=True)
            for link in links:
                href = link['href']
                if href.startswith('http') and 'diningcode.com' not in href:
                    contact_info['website'] = href
                    break
            
            logger.info(f"ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return contact_info
    
    def _normalize_phone_number(self, phone: str) -> str:
        """ì „í™”ë²ˆí˜¸ ì •ê·œí™” (0507 â†’ 07 ë¬¸ì œ í•´ê²°)"""
        if not phone:
            return phone
        
        # í•˜ì´í”ˆ ì œê±°
        clean_phone = phone.replace('-', '')
        
        # 0507, 0508 ë“±ì˜ ì¸í„°ë„· ì „í™”ë²ˆí˜¸ ì •ê·œí™”
        # 0507-1234-5678 â†’ 0507-1234-5678 (ê·¸ëŒ€ë¡œ ìœ ì§€)
        # ì˜ëª» íŒŒì‹±ëœ 07-1234-5678 â†’ 0507-1234-5678 (ë³µì›)
        
        # íŒ¨í„´ 1: 07-XXXX-XXXX í˜•íƒœ (0507ì—ì„œ 05ê°€ ëˆ„ë½ëœ ê²½ìš°)
        if re.match(r'^07\d{8}$', clean_phone):
            # 07XXXXXXXX â†’ 0507XXXXXXXX
            clean_phone = '0507' + clean_phone[2:]
        elif re.match(r'^08\d{8}$', clean_phone):
            # 08XXXXXXXX â†’ 0508XXXXXXXX  
            clean_phone = '0508' + clean_phone[2:]
        
        # í•˜ì´í”ˆ ì¶”ê°€í•˜ì—¬ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        if len(clean_phone) == 11:
            if clean_phone.startswith('010'):
                # 010-XXXX-XXXX
                return f"{clean_phone[:3]}-{clean_phone[3:7]}-{clean_phone[7:]}"
            elif clean_phone.startswith(('0507', '0508', '070')):
                # 0507-XXXX-XXXX, 0508-XXXX-XXXX, 070-XXXX-XXXX
                return f"{clean_phone[:4]}-{clean_phone[4:8]}-{clean_phone[8:]}"
        elif len(clean_phone) == 10:
            if clean_phone.startswith('02'):
                # 02-XXXX-XXXX
                return f"{clean_phone[:2]}-{clean_phone[2:6]}-{clean_phone[6:]}"
            else:
                # ê¸°íƒ€ ì§€ì—­ë²ˆí˜¸ 0XX-XXX-XXXX
                return f"{clean_phone[:3]}-{clean_phone[3:6]}-{clean_phone[6:]}"
        elif len(clean_phone) == 9 and clean_phone.startswith('02'):
            # 02-XXX-XXXX
            return f"{clean_phone[:2]}-{clean_phone[2:5]}-{clean_phone[5:]}"
        
        # ê¸°ë³¸ì ìœ¼ë¡œ ì›ë˜ ë²ˆí˜¸ ë°˜í™˜
        return phone

    def _extract_refill_info(self, soup: BeautifulSoup) -> Dict:
        """ë¬´í•œë¦¬í•„ ê´€ë ¨ ì •ë³´ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „ - ë¦¬ë·° ì œì™¸)"""
        refill_info = {
            'refill_items': [],
            'refill_type': '',
            'refill_conditions': '',
            'is_confirmed_refill': False,
            'refill_menu_summary': []  # ë©”ë‰´ ì •ë³´ì™€ í†µí•©ëœ ìš”ì•½
        }
        
        try:
            logger.info("ğŸ”„ ë¬´í•œë¦¬í•„ ì •ë³´ ì¶”ì¶œ ì‹œì‘...")
            
            # 1. ë©”ë‰´/ìš´ì˜ ì •ë³´ ì„¹ì…˜ë§Œ ì¶”ì¶œ (ë¦¬ë·° ì„¹ì…˜ ì œì™¸)
            operation_text = self._extract_operation_section_text(soup)
            
            # 2. ë¬´í•œë¦¬í•„ í™•ì¸
            refill_keywords = ['ë¬´í•œë¦¬í•„', 'ë¬´ì œí•œ', 'ì…€í”„ë°”', 'ë·”í˜']
            for keyword in refill_keywords:
                if keyword in operation_text:
                    refill_info['is_confirmed_refill'] = True
                    break
            
            # 3. ë¬´í•œë¦¬í•„ ì•„ì´í…œ ì¶”ì¶œ (ë” ì •í™•í•œ íŒ¨í„´)
            refill_items = self._extract_clean_refill_items(operation_text)
            refill_info['refill_items'] = refill_items
            
            # 4. ë¦¬í•„ íƒ€ì… ì¶”ì •
            if 'ê³ ê¸°' in operation_text or 'ì‚¼ê²¹ì‚´' in operation_text or 'ì†Œê³ ê¸°' in operation_text:
                refill_info['refill_type'] = 'ê³ ê¸°ë¬´í•œë¦¬í•„'
            elif 'ì´ˆë°¥' in operation_text or 'íšŒ' in operation_text:
                refill_info['refill_type'] = 'ì´ˆë°¥ë·”í˜'
            elif 'ë·”í˜' in operation_text:
                refill_info['refill_type'] = 'ë·”í˜'
            elif 'ì…€í”„ë°”' in operation_text:
                refill_info['refill_type'] = 'ì…€í”„ë°”'
            else:
                refill_info['refill_type'] = 'ë¬´í•œë¦¬í•„'
            
            # 5. ë¬´í•œë¦¬í•„ ì¡°ê±´ ì¶”ì¶œ
            conditions = self._extract_refill_conditions(operation_text)
            refill_info['refill_conditions'] = '; '.join(conditions[:5])
            
            # 5. ë©”ë‰´ ì •ë³´ì™€ ê°€ê²© ì •ë³´ë¥¼ í†µí•©í•œ ìš”ì•½ ìƒì„±
            # (ë©”ë‰´ ì¶”ì¶œì—ì„œ ì–»ì€ refill_menu_items í™œìš©)
            menu_info = self._extract_menu_info(soup)
            price_info = self._extract_price_info(soup)
            
            # ë¬´í•œë¦¬í•„ ë©”ë‰´ ìš”ì•½ ìƒì„±
            refill_summary = []
            
            # ê°€ê²© ì •ë³´ê°€ ìˆëŠ” ê²½ìš°
            if price_info.get('refill_prices'):
                for price_item in price_info['refill_prices']:
                    refill_summary.append({
                        'name': price_item['name'],
                        'price': price_item['price'],
                        'type': 'price_menu'
                    })
            
            # ë©”ë‰´ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°
            if menu_info.get('refill_menu_items'):
                for menu_item in menu_info['refill_menu_items']:
                    # ì¤‘ë³µ ì²´í¬ (ì´ë¦„ì´ ê°™ì€ ê²½ìš° ì œì™¸)
                    if not any(item['name'] == menu_item['name'] for item in refill_summary):
                        refill_summary.append({
                            'name': menu_item['name'],
                            'price': menu_item.get('price', ''),
                            'type': 'menu_item'
                        })
            
            # ì¼ë°˜ ë¬´í•œë¦¬í•„ ì•„ì´í…œ ì¶”ê°€
            for item in refill_info['refill_items']:
                if not any(summary_item['name'] == item for summary_item in refill_summary):
                    refill_summary.append({
                        'name': item,
                        'price': '',
                        'type': 'general_item'
                    })
            
            refill_info['refill_menu_summary'] = refill_summary[:10]  # ìµœëŒ€ 10ê°œ
            
            logger.info(f"ë¬´í•œë¦¬í•„ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ: {len(refill_info['refill_items'])}ê°œ ì•„ì´í…œ, {len(refill_summary)}ê°œ ìš”ì•½")
            
        except Exception as e:
            logger.error(f"ë¬´í•œë¦¬í•„ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return refill_info
    
    def _extract_coordinate_info(self, soup: BeautifulSoup) -> Dict:
        """ì¢Œí‘œ ì •ë³´ ì¶”ì¶œ (ë‹¤ì´ë‹ì½”ë“œ ìƒì„¸ í˜ì´ì§€ì—ì„œ) - ê°œì„ ëœ ë²„ì „"""
        coordinate_info = {
            'position_lat': None,
            'position_lng': None,
            'address': None
        }
        
        try:
            # 1. Seleniumì„ í†µí•œ JavaScript ë³€ìˆ˜ ì¶”ì¶œ (ê°€ì¥ í™•ì‹¤í•œ ë°©ë²•)
            if self.driver:
                try:
                    # JavaScript ì‹¤í–‰ìœ¼ë¡œ ì¢Œí‘œ ì •ë³´ ì¶”ì¶œ
                    lat_script = r"""
                    var lat = null;
                    try {
                        // ì „ì—­ ë³€ìˆ˜ì—ì„œ ì°¾ê¸°
                        if (typeof lat !== 'undefined' && lat) lat = lat;
                        else if (typeof latitude !== 'undefined' && latitude) lat = latitude;
                        else if (typeof poi_lat !== 'undefined' && poi_lat) lat = poi_lat;
                        else if (typeof mapLat !== 'undefined' && mapLat) lat = mapLat;
                        else if (typeof window.lat !== 'undefined' && window.lat) lat = window.lat;
                        else if (typeof window.latitude !== 'undefined' && window.latitude) lat = window.latitude;
                        
                        // ê°ì²´ ì•ˆì—ì„œ ì°¾ê¸°
                        if (!lat && typeof window.poi !== 'undefined' && window.poi && window.poi.lat) lat = window.poi.lat;
                        if (!lat && typeof window.store !== 'undefined' && window.store && window.store.lat) lat = window.store.lat;
                        if (!lat && typeof window.restaurant !== 'undefined' && window.restaurant && window.restaurant.lat) lat = window.restaurant.lat;
                        
                        // í˜ì´ì§€ ì†ŒìŠ¤ì—ì„œ ì •ê·œì‹ìœ¼ë¡œ ì°¾ê¸°
                        if (!lat) {
                            var pageSource = document.documentElement.outerHTML;
                            var latMatch = pageSource.match(/(?:latitude|lat)["']?\s*[:=]\s*([0-9]+\.?[0-9]*)/i);
                            if (latMatch) lat = parseFloat(latMatch[1]);
                        }
                    } catch(e) {
                        console.log('ìœ„ë„ ì¶”ì¶œ ì˜¤ë¥˜:', e);
                    }
                    return lat;
                    """
                    
                    lng_script = r"""
                    var lng = null;
                    try {
                        // ì „ì—­ ë³€ìˆ˜ì—ì„œ ì°¾ê¸°
                        if (typeof lng !== 'undefined' && lng) lng = lng;
                        else if (typeof longitude !== 'undefined' && longitude) lng = longitude;
                        else if (typeof poi_lng !== 'undefined' && poi_lng) lng = poi_lng;
                        else if (typeof mapLng !== 'undefined' && mapLng) lng = mapLng;
                        else if (typeof window.lng !== 'undefined' && window.lng) lng = window.lng;
                        else if (typeof window.longitude !== 'undefined' && window.longitude) lng = window.longitude;
                        
                        // ê°ì²´ ì•ˆì—ì„œ ì°¾ê¸°
                        if (!lng && typeof window.poi !== 'undefined' && window.poi && window.poi.lng) lng = window.poi.lng;
                        if (!lng && typeof window.store !== 'undefined' && window.store && window.store.lng) lng = window.store.lng;
                        if (!lng && typeof window.restaurant !== 'undefined' && window.restaurant && window.restaurant.lng) lng = window.restaurant.lng;
                        
                        // í˜ì´ì§€ ì†ŒìŠ¤ì—ì„œ ì •ê·œì‹ìœ¼ë¡œ ì°¾ê¸°
                        if (!lng) {
                            var pageSource = document.documentElement.outerHTML;
                            var lngMatch = pageSource.match(/(?:longitude|lng)["']?\s*[:=]\s*([0-9]+\.?[0-9]*)/i);
                            if (lngMatch) lng = parseFloat(lngMatch[1]);
                        }
                    } catch(e) {
                        console.log('ê²½ë„ ì¶”ì¶œ ì˜¤ë¥˜:', e);
                    }
                    return lng;
                    """
                    
                    lat = self.driver.execute_script(lat_script)
                    lng = self.driver.execute_script(lng_script)
                    
                    if lat and lng:
                        coordinate_info['position_lat'] = float(lat)
                        coordinate_info['position_lng'] = float(lng)
                        logger.info(f"JavaScriptì—ì„œ ì¢Œí‘œ ì¶”ì¶œ ì„±ê³µ: ({lat}, {lng})")
                    else:
                        logger.warning("JavaScriptì—ì„œ ì¢Œí‘œ ì¶”ì¶œ ì‹¤íŒ¨")
                        
                except Exception as js_error:
                    logger.error(f"JavaScript ì¢Œí‘œ ì¶”ì¶œ ì˜¤ë¥˜: {js_error}")
            
            # 2. JavaScript ì‹¤í–‰ì´ ì‹¤íŒ¨í•œ ê²½ìš° HTML íŒŒì‹±ìœ¼ë¡œ ëŒ€ì²´
            if not coordinate_info['position_lat']:
                scripts = soup.find_all('script')
                for script in scripts:
                    if script.string:
                        script_content = script.string
                        
                        # ë‹¤ì–‘í•œ íŒ¨í„´ìœ¼ë¡œ ì¢Œí‘œ ê²€ìƒ‰
                        patterns = [
                            (r'latitude["\']?\s*[:=]\s*([0-9]+\.?[0-9]*)', r'longitude["\']?\s*[:=]\s*([0-9]+\.?[0-9]*)'),
                            (r'lat["\']?\s*[:=]\s*([0-9]+\.?[0-9]*)', r'lng["\']?\s*[:=]\s*([0-9]+\.?[0-9]*)'),
                            (r'poi_lat["\']?\s*[:=]\s*([0-9]+\.?[0-9]*)', r'poi_lng["\']?\s*[:=]\s*([0-9]+\.?[0-9]*)'),
                            (r'mapLat["\']?\s*[:=]\s*([0-9]+\.?[0-9]*)', r'mapLng["\']?\s*[:=]\s*([0-9]+\.?[0-9]*)')
                        ]
                        
                        for lat_pattern, lng_pattern in patterns:
                            lat_match = re.search(lat_pattern, script_content, re.IGNORECASE)
                            lng_match = re.search(lng_pattern, script_content, re.IGNORECASE)
                            
                            if lat_match and lng_match:
                                coordinate_info['position_lat'] = float(lat_match.group(1))
                                coordinate_info['position_lng'] = float(lng_match.group(1))
                                logger.info(f"HTML íŒŒì‹±ìœ¼ë¡œ ì¢Œí‘œ ì¶”ì¶œ: ({coordinate_info['position_lat']}, {coordinate_info['position_lng']})")
                                break
                        
                        if coordinate_info['position_lat']:
                            break
            
            # 3. ì£¼ì†Œ ì •ë³´ ì¶”ì¶œ (ì§€ì˜¤ì½”ë”©ìš©)
            address_selectors = [
                '.address',
                '.location',
                '[class*="addr"]',
                '[class*="address"]',
                '.info-address',
                '.restaurant-address',
                '.store-address',
                '.poi-address'
            ]
            
            for selector in address_selectors:
                address_elem = soup.select_one(selector)
                if address_elem:
                    address_text = address_elem.get_text(strip=True)
                    if address_text and len(address_text) > 5:
                        coordinate_info['address'] = address_text
                        logger.info(f"ì£¼ì†Œ ì •ë³´ ì¶”ì¶œ: {address_text}")
                        break
            
            # 4. ì¢Œí‘œ ìœ íš¨ì„± ê²€ì¦ (í•œêµ­ ì˜ì—­ ë‚´)
            if coordinate_info['position_lat'] and coordinate_info['position_lng']:
                lat = coordinate_info['position_lat']
                lng = coordinate_info['position_lng']
                
                # í•œêµ­ ì˜ì—­ ì²´í¬ (ëŒ€ëµì ì¸ ë²”ìœ„)
                if not (33.0 <= lat <= 38.5 and 124.0 <= lng <= 132.0):
                    logger.warning(f"ì¢Œí‘œê°€ í•œêµ­ ì˜ì—­ì„ ë²—ì–´ë‚¨: ({lat}, {lng})")
                    coordinate_info['position_lat'] = None
                    coordinate_info['position_lng'] = None
                else:
                    logger.info(f"ì¢Œí‘œ ìœ íš¨ì„± ê²€ì¦ í†µê³¼: ({lat}, {lng})")
            
            # 5. ì¢Œí‘œê°€ ì—†ìœ¼ë©´ ì£¼ì†Œë¡œ ì§€ì˜¤ì½”ë”© í•„ìš” í‘œì‹œ
            if not coordinate_info['position_lat'] and coordinate_info['address']:
                logger.info(f"ì£¼ì†Œ ê¸°ë°˜ ì§€ì˜¤ì½”ë”© í•„ìš”: {coordinate_info['address']}")
                
        except Exception as e:
            logger.error(f"ì¢Œí‘œ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return coordinate_info
    
    def _extract_address_info(self, soup: BeautifulSoup) -> Dict:
        """ì£¼ì†Œ ì •ë³´ ì¶”ì¶œ (ë‹¤ì´ë‹ì½”ë“œ êµ¬ì¡°ì— ë§ê²Œ ê°œì„ )"""
        address_info = {
            'address': '',
            'basic_address': '',
            'road_address': ''
        }
        
        try:
            # ë‹¤ì´ë‹ì½”ë“œì˜ ì£¼ì†ŒëŠ” ì£¼ë¡œ ë§í¬ í˜•íƒœë¡œ ë˜ì–´ ìˆìŒ
            # ì˜ˆ: <a href="/list.dc?query=ì„œìš¸íŠ¹ë³„ì‹œ">ì„œìš¸íŠ¹ë³„ì‹œ</a> <a href="/list.dc?query=ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬">ê°•ë‚¨êµ¬</a>
            
            # 1. ì£¼ì†Œ ë§í¬ë“¤ì„ ì°¾ê¸°
            address_parts = []
            
            # list.dc?query= íŒ¨í„´ì„ ê°€ì§„ ë§í¬ë“¤ ì°¾ê¸°
            address_links = soup.find_all('a', href=re.compile(r'/list\.dc\?query='))
            
            # ì£¼ì†Œ ë¶€ë¶„ë§Œ ì¶”ì¶œ
            for link in address_links:
                text = link.get_text(strip=True)
                
                # ì£¼ì†Œ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆëŠ” ê²½ìš°ë§Œ
                if any(keyword in text for keyword in ['ì„œìš¸', 'íŠ¹ë³„ì‹œ', 'ê´‘ì—­ì‹œ', 'ì‹œ', 'êµ¬', 'ë™', 'ë¡œ', 'ê¸¸', 'ë²ˆì§€']):
                    # ë§›ì§‘, ê²€ìƒ‰í•˜ê¸° ë“±ì˜ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš° ì œì™¸
                    if not any(exclude in text for exclude in ['ë§›ì§‘', 'ê²€ìƒ‰', 'ìŒì‹', 'ë­í‚¹', 'ì¶”ì²œ']):
                        address_parts.append(text)
            
            # 2. ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ìˆœì„œëŒ€ë¡œ ì£¼ì†Œ ì¡°í•©
            if address_parts:
                # ì¤‘ë³µ ì œê±° (ìˆœì„œ ìœ ì§€)
                seen = set()
                unique_parts = []
                for part in address_parts:
                    if part not in seen and len(part) > 1:
                        seen.add(part)
                        unique_parts.append(part)
                
                # ì£¼ì†Œ ë¶€ë¶„ë“¤ì„ í•©ì³ì„œ ì™„ì „í•œ ì£¼ì†Œ ë§Œë“¤ê¸°
                # ì„œìš¸íŠ¹ë³„ì‹œ, ê°•ë‚¨êµ¬, í…Œí—¤ë€ë¡œ -> ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬ í…Œí—¤ë€ë¡œ
                full_address = ""
                
                # ì‹œ/ë„ ì°¾ê¸°
                for part in unique_parts:
                    if any(keyword in part for keyword in ['ì„œìš¸íŠ¹ë³„ì‹œ', 'ì„œìš¸ì‹œ', 'ì„œìš¸', 'ê²½ê¸°ë„', 'ì¸ì²œê´‘ì—­ì‹œ']):
                        full_address = part
                        break
                
                # êµ¬ ì°¾ê¸°
                for part in unique_parts:
                    if 'êµ¬' in part and part not in full_address:
                        if full_address:
                            full_address += " " + part
                        else:
                            full_address = part
                
                # ë™/ë¡œ/ê¸¸ ì°¾ê¸°
                for part in unique_parts:
                    if any(keyword in part for keyword in ['ë™', 'ë¡œ', 'ê¸¸']) and part not in full_address:
                        if full_address:
                            full_address += " " + part
                        else:
                            full_address = part
                
                # ë²ˆì§€/ë²ˆí˜¸ ì°¾ê¸°
                for part in unique_parts:
                    if re.search(r'\d+', part) and part not in full_address:
                        if full_address:
                            full_address += " " + part
                
                address_info['address'] = full_address.strip()
            
            # 3. ì£¼ì†Œê°€ ì—†ê±°ë‚˜ ë¶ˆì™„ì „í•œ ê²½ìš° ë°±ì—… ë°©ë²• ì‹œë„
            if not address_info['address'] or len(address_info['address']) < 10:
                # ì£¼ì†Œ ê´€ë ¨ ìš”ì†Œ ì§ì ‘ ì°¾ê¸°
                address_selectors = [
                    '.BasicInfo__Address',
                    '.Info__Address', 
                    '.address',
                    '.location',
                    '[class*="addr"]',
                    '[class*="address"]'
                ]
                
                for selector in address_selectors:
                    elem = soup.select_one(selector)
                    if elem:
                        text = elem.get_text(strip=True)
                        # HTML íƒœê·¸ ì œê±°
                        text = re.sub(r'<[^>]+>', '', text)
                        # ì£¼ì†Œ íŒ¨í„´ í™•ì¸
                        if text and any(keyword in text for keyword in ['ì„œìš¸', 'êµ¬', 'ë™', 'ë¡œ', 'ê¸¸']):
                            # ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
                            text = re.sub(r'(ë§›ì§‘|ê²€ìƒ‰í•˜ê¸°|ìŒì‹|ë­í‚¹|ì¶”ì²œ).*', '', text)
                            text = text.strip()
                            if len(text) > len(address_info.get('address', '')):
                                address_info['address'] = text
                                break
            
            # 4. ì£¼ì†Œ ìµœì¢… ì •ë¦¬
            if address_info['address']:
                # ì—°ì†ëœ ê³µë°± ì œê±°
                address_info['address'] = ' '.join(address_info['address'].split())
                
                # HTML ì—”í‹°í‹° ì œê±°
                address_info['address'] = address_info['address'].replace('&nbsp;', ' ')
                address_info['address'] = address_info['address'].replace('&amp;', '&')
                
                # ë„ë¡œëª… ì£¼ì†Œì¸ì§€ í™•ì¸
                if any(keyword in address_info['address'] for keyword in ['ë¡œ', 'ê¸¸']):
                    address_info['road_address'] = address_info['address']
                else:
                    address_info['basic_address'] = address_info['address']
            
            logger.info(f"ìµœì¢… ì£¼ì†Œ: {address_info['address']}")
            
        except Exception as e:
            logger.error(f"ì£¼ì†Œ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return address_info
    
    def _extract_refill_prices_from_operation_info(self, soup: BeautifulSoup) -> List[Dict]:
        """ìš´ì˜ ì •ë³´ì—ì„œ ë¬´í•œë¦¬í•„ ê°€ê²© ì •ë³´ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
        price_items = []
        
        try:
            logger.info("ğŸ” ìš´ì˜ ì •ë³´ì—ì„œ ë¬´í•œë¦¬í•„ ê°€ê²© ì¶”ì¶œ ì‹œì‘...")
            
            # 1. ë©”ë‰´ì •ë³´ ì„¹ì…˜ ì§ì ‘ ì°¾ê¸°
            menu_section = None
            
            # ë©”ë‰´ì •ë³´ í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ìš”ì†Œ ì°¾ê¸°
            menu_headers = soup.find_all(text=re.compile(r'ë©”ë‰´\s*ì •ë³´', re.IGNORECASE))
            for header in menu_headers:
                parent = header.parent
                if parent:
                    # ë©”ë‰´ì •ë³´ ì„¹ì…˜ì˜ ë¶€ëª¨ ìš”ì†Œ ì°¾ê¸°
                    menu_section = parent.find_parent(['div', 'section', 'article'])
                    if menu_section:
                        break
            
            # 2. ë©”ë‰´ì •ë³´ ì„¹ì…˜ì´ ì—†ìœ¼ë©´ ì „ì²´ í˜ì´ì§€ì—ì„œ ê²€ìƒ‰
            if not menu_section:
                menu_section = soup
            
            # 3. ê°€ê²© ì •ë³´ ì¶”ì¶œ
            all_text = menu_section.get_text()
            
            # 4. ê°€ê²© ì •ë³´ íŒŒì‹±
            price_items = self._extract_refill_prices_from_text(all_text)
            
            # 5. ì¶”ê°€ë¡œ HTML êµ¬ì¡°ì—ì„œ ì§ì ‘ ì¶”ì¶œ ì‹œë„
            if len(price_items) == 0:
                # ê°€ê²© ìš”ì†Œë¥¼ í¬í•¨í•˜ëŠ” ìš”ì†Œë“¤ ì°¾ê¸°
                price_elements = menu_section.find_all(['div', 'span', 'p'], 
                                                     text=re.compile(r'\d{1,3}(?:,\d{3})*\s*ì›'))
                
                for element in price_elements:
                    price_text = element.get_text()
                    price_match = re.search(r'(\d{1,3}(?:,\d{3})*)\s*ì›', price_text)
                    
                    if price_match:
                        price = price_match.group(1)
                        
                        # ì´ì „ í˜•ì œ ìš”ì†Œì—ì„œ ë©”ë‰´ëª… ì°¾ê¸°
                        prev_sibling = element.find_previous_sibling()
                        if prev_sibling:
                            menu_text = prev_sibling.get_text()
                            if 'ë¬´í•œë¦¬í•„' in menu_text or 'ì¶”ì²œ' in menu_text:
                                menu_name = re.sub(r'^[a-z]\s+', '', menu_text.strip())
                                
                                if len(menu_name) >= 3:
                                    price_num = int(price.replace(',', ''))
                                    if 5000 <= price_num <= 50000:
                                        price_items.append({
                                            'name': menu_name,
                                            'price': f"{price}ì›",
                                            'price_numeric': price_num
                                        })
            
            # 6. ì •ë ¬ ë° ì¤‘ë³µ ì œê±°
            if price_items:
                # ê°€ê²© ìˆœ ì •ë ¬
                price_items.sort(key=lambda x: x['price_numeric'])
                
                # ì¤‘ë³µ ì œê±° (ì´ë¦„ ê¸°ì¤€)
                unique_items = []
                seen_names = set()
                for item in price_items:
                    if item['name'] not in seen_names:
                        unique_items.append(item)
                        seen_names.add(item['name'])
                price_items = unique_items
            
            logger.info(f"í…ìŠ¤íŠ¸ì—ì„œ {len(price_items)}ê°œ ë¬´í•œë¦¬í•„ ê°€ê²© ì¶”ì¶œ")
            
            # ì¶”ì¶œëœ ê°€ê²© ì •ë³´ ë¡œê¹…
            for item in price_items:
                logger.debug(f"ê°€ê²©: {item['name']} - {item['price']}")
            
        except Exception as e:
            logger.error(f"ìš´ì˜ ì •ë³´ì—ì„œ ë¬´í•œë¦¬í•„ ê°€ê²© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return price_items
    
    def _extract_refill_prices_from_text(self, text: str) -> List[Dict]:
        """í…ìŠ¤íŠ¸ì—ì„œ ë¬´í•œë¦¬í•„ ê°€ê²© íŒ¨í„´ ì¶”ì¶œ"""
        price_items = []
        
        try:
            # ì‹¤ì œ ë‹¤ì´ë‹ì½”ë“œ í˜ì´ì§€ì˜ ë©”ë‰´ ì •ë³´ íŒ¨í„´ì— ë§ëŠ” ì •ê·œì‹
            patterns = [
                # íŒ¨í„´ 1: ë‹¤ì´ë‹ì½”ë“œ ë©”ë‰´ ì •ë³´ í˜•ì‹ (ì˜ˆ: "a ë¼ì§€ëª¨ë“¬ ë¬´í•œë¦¬í•„ ì¶”ì²œ\n17,900ì›")
                r'([a-z]\s+[ê°€-í£\s\w()]+?(?:ë¬´í•œë¦¬í•„|ë·”í˜|ì…€í”„ë°”)[ê°€-í£\s\w()]*?(?:ì¶”ì²œ|í• ì¸)?)\s*\n?\s*(\d{1,3}(?:,\d{3})*)\s*ì›',
                
                # íŒ¨í„´ 2: ì‹œê°„ëŒ€ë³„ ê°€ê²© (ì˜ˆ: "í‰ì¼ 15:00 ì´ì „ ì…ì¥ê¸°ì¤€ ì¶”ì²œ 17,900ì›")
                r'([ê°€-í£\s\w()]+?(?:í‰ì¼|ì£¼ë§|ê³µíœ´ì¼|ì´ì „|ì´í›„|ì €ë…|ì ì‹¬|ì˜¤ì „|ì˜¤í›„)[ê°€-í£\s\w()]*?(?:ì¶”ì²œ|í• ì¸|ë¬´í•œë¦¬í•„|ì…ì¥ê¸°ì¤€))\s*(\d{1,3}(?:,\d{3})*)\s*ì›',
                
                # íŒ¨í„´ 3: ì—°ë ¹ëŒ€ë³„ ê°€ê²© (ì˜ˆ: "ë¬´í•œë¦¬í•„ ì´ˆë“±í• ì¸(4~6í•™ë…„) ì¶”ì²œ 16,000ì›")
                r'([ê°€-í£\s\w()]+?(?:ì´ˆë“±|ë¯¸ì·¨í•™|ì„±ì¸|ì–´ë¦°ì´|í•™ìƒ)[ê°€-í£\s\w()]*?(?:ì¶”ì²œ|í• ì¸|ë¬´í•œë¦¬í•„))\s*(\d{1,3}(?:,\d{3})*)\s*ì›',
                
                # íŒ¨í„´ 4: ê¸°ë³¸ ë¬´í•œë¦¬í•„ ê°€ê²©
                r'([ê°€-í£\s\w()]+?(?:ë¬´í•œë¦¬í•„|ë·”í˜|ì…€í”„ë°”)[ê°€-í£\s\w()]*?(?:ì¶”ì²œ|í• ì¸)?)\s*(\d{1,3}(?:,\d{3})*)\s*ì›',
                
                # íŒ¨í„´ 5: ë©”ë‰´ëª… + ê°€ê²© (ì½œë¡  êµ¬ë¶„)
                r'([ê°€-í£\s\w()]+?(?:ì¶”ì²œ|í• ì¸|ë¬´í•œë¦¬í•„|ì…ì¥ê¸°ì¤€))\s*[:ï¼š]\s*(\d{1,3}(?:,\d{3})*)\s*ì›',
                
                # íŒ¨í„´ 6: ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„ëœ ê°€ê²©
                r'([ê°€-í£\s\w()]+?(?:ì¶”ì²œ|í• ì¸|ë¬´í•œë¦¬í•„|ì…ì¥ê¸°ì¤€))\s*\n\s*(\d{1,3}(?:,\d{3})*)\s*ì›'
            ]
            
            for pattern in patterns:
                matches = re.findall(pattern, text, re.MULTILINE | re.IGNORECASE)
                
                for match in matches:
                    menu_name = match[0].strip()
                    price = match[1].strip()
                    
                    # ë©”ë‰´ëª… ì •ë¦¬ (ì•ì˜ ì•ŒíŒŒë²³ ì œê±°)
                    menu_name = re.sub(r'^[a-z]\s+', '', menu_name)
                    
                    # ìœ íš¨ì„± ê²€ì‚¬
                    if (len(menu_name) >= 3 and len(menu_name) <= 50 and 
                        price.replace(',', '').isdigit()):
                        
                        # ë¦¬ë·°ë‚˜ ê¸°íƒ€ ì •ë³´ ì œì™¸
                        if any(exclude in menu_name for exclude in ['ë¦¬ë·°', 'í›„ê¸°', 'í‰ì ', 'ë³„ì ', 'ë°©ë¬¸', 'ë¸”ë¡œê·¸']):
                            continue
                        
                        # ê°€ê²© ë²”ìœ„ ê²€ì¦ (5,000ì› ~ 50,000ì›)
                        price_num = int(price.replace(',', ''))
                        if 5000 <= price_num <= 50000:
                            price_items.append({
                                'name': menu_name,
                                'price': f"{price}ì›",
                                'price_numeric': price_num
                            })
            
            # ì¶”ê°€ íŒ¨í„´: í…Œì´ë¸” í˜•íƒœì˜ ê°€ê²© ì •ë³´
            table_patterns = [
                # íƒ­ìœ¼ë¡œ êµ¬ë¶„ëœ í˜•íƒœ
                r'([ê°€-í£\s\w()]+?(?:ì¶”ì²œ|í• ì¸|ë¬´í•œë¦¬í•„|ì…ì¥ê¸°ì¤€))\s+(\d{1,3}(?:,\d{3})*)\s*ì›',
                
                # ëŒ€ì‹œë¡œ êµ¬ë¶„ëœ í˜•íƒœ
                r'([ê°€-í£\s\w()]+?(?:ì¶”ì²œ|í• ì¸|ë¬´í•œë¦¬í•„|ì…ì¥ê¸°ì¤€))\s*-\s*(\d{1,3}(?:,\d{3})*)\s*ì›'
            ]
            
            for pattern in table_patterns:
                matches = re.findall(pattern, text, re.MULTILINE)
                
                for match in matches:
                    menu_name = match[0].strip()
                    price = match[1].strip()
                    
                    # ë©”ë‰´ëª… ì •ë¦¬
                    menu_name = re.sub(r'^[a-z]\s+', '', menu_name)
                    
                    if (len(menu_name) >= 3 and len(menu_name) <= 50 and 
                        price.replace(',', '').isdigit()):
                        
                        # ì¤‘ë³µ ì²´í¬
                        if not any(item['name'] == menu_name for item in price_items):
                            price_num = int(price.replace(',', ''))
                            if 5000 <= price_num <= 50000:
                                price_items.append({
                                    'name': menu_name,
                                    'price': f"{price}ì›",
                                    'price_numeric': price_num
                                })
            
            # ì •ë ¬ (ê°€ê²© ìˆœ)
            price_items.sort(key=lambda x: x['price_numeric'])
            
            logger.info(f"í…ìŠ¤íŠ¸ì—ì„œ {len(price_items)}ê°œ ë¬´í•œë¦¬í•„ ê°€ê²© ì¶”ì¶œ")
            
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ì—ì„œ ë¬´í•œë¦¬í•„ ê°€ê²© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return price_items

    def get_stats(self) -> Dict:
        """í¬ë¡¤ë§ ë° ì´ë¯¸ì§€ ì²˜ë¦¬ í†µê³„ ë°˜í™˜"""
        return self.stats.copy()
    
    def print_stats(self):
        """í¬ë¡¤ë§ í†µê³„ ì¶œë ¥"""
        print("\nğŸ“Š í¬ë¡¤ë§ í†µê³„:")
        print(f"   ì „ì²´ ìš”ì²­: {self.stats['total_requests']}")
        print(f"   ì„±ê³µ ìš”ì²­: {self.stats['successful_requests']}")
        print(f"   ì‹¤íŒ¨ ìš”ì²­: {self.stats['failed_requests']}")
        print(f"   ì´ë¯¸ì§€ ì²˜ë¦¬: {self.stats['images_processed']}")
        print(f"   Storage ì—…ë¡œë“œ: {self.stats['images_uploaded']}")
        
        if self.stats['images_processed'] > 0:
            upload_rate = (self.stats['images_uploaded'] / self.stats['images_processed']) * 100
            print(f"   ì—…ë¡œë“œ ì„±ê³µë¥ : {upload_rate:.1f}%")
    
    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ë° í†µê³„ ì¶œë ¥"""
        if self.driver:
            try:
                self.driver.quit()
                logger.info("WebDriver ì¢…ë£Œ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"WebDriver ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # ì´ë¯¸ì§€ ë§¤ë‹ˆì € í†µê³„ ì¶œë ¥
        if self.image_manager:
            try:
                storage_stats = self.image_manager.get_storage_stats()
                print("\nğŸ“ˆ ì´ë¯¸ì§€ ì €ì¥ì†Œ í†µê³„:")
                for key, value in storage_stats.items():
                    print(f"   {key}: {value}")
            except Exception as e:
                logger.warning(f"ì €ì¥ì†Œ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        # í¬ë¡¤ë§ í†µê³„ ì¶œë ¥
        self.print_stats()

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í•¨ìˆ˜
def test_crawling():
    """ê¸°ë³¸ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ (ì§€ì—­ëª… í¬í•¨ í‚¤ì›Œë“œ ì‚¬ìš©)"""
    crawler = DiningCodeCrawler()
    
    try:
        # 1. ëª©ë¡ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ (ì§€ì—­ëª… í¬í•¨ í‚¤ì›Œë“œ ì‚¬ìš©)
        region_name = config.REGIONS[config.TEST_REGION]["name"]
        test_keyword = f"{region_name} ë¬´í•œë¦¬í•„"  # ì§€ì—­ëª… í¬í•¨
        logger.info(f"í…ŒìŠ¤íŠ¸ í‚¤ì›Œë“œ: {test_keyword}")
        
        stores = crawler.get_store_list(test_keyword, config.TEST_RECT)
        logger.info(f"ì´ {len(stores)}ê°œ ê°€ê²Œ ë°œê²¬")
        
        if stores:
            # 2. ì²« ë²ˆì§¸ ê°€ê²Œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸
            first_store = stores[0]
            logger.info(f"í…ŒìŠ¤íŠ¸ ëŒ€ìƒ ê°€ê²Œ: {first_store.get('name')}")
            logger.info(f"ê°€ê²Œ ID: {first_store.get('diningcode_place_id')}")
            
            detailed_store = crawler.get_store_detail(first_store)
            
            logger.info("=== í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===")
            for key, value in detailed_store.items():
                logger.info(f"{key}: {value}")
                
            # CSVë¡œ ì €ì¥
            df = pd.DataFrame([detailed_store])
            df.to_csv('data/test_crawling_result.csv', index=False, encoding='utf-8-sig')
            logger.info("í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ data/test_crawling_result.csvì— ì €ì¥")
            
        else:
            logger.warning(f"'{test_keyword}' í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ëœ ê°€ê²Œê°€ ì—†ìŠµë‹ˆë‹¤.")
            logger.info("ë‹¤ë¥¸ í‚¤ì›Œë“œë¥¼ ì‹œë„í•´ë³´ê² ìŠµë‹ˆë‹¤...")
            
            # ë°±ì—… í‚¤ì›Œë“œë¡œ ì¬ì‹œë„
            backup_keyword = "ê°•ë‚¨ ê³ ê¸°ë¬´í•œë¦¬í•„"
            logger.info(f"ë°±ì—… í‚¤ì›Œë“œ: {backup_keyword}")
            stores = crawler.get_store_list(backup_keyword, config.TEST_RECT)
            
            if stores:
                first_store = stores[0]
                detailed_store = crawler.get_store_detail(first_store)
                logger.info(f"ë°±ì—… í‚¤ì›Œë“œë¡œ {len(stores)}ê°œ ê°€ê²Œ ë°œê²¬")
            else:
                logger.warning("ë°±ì—… í‚¤ì›Œë“œë¡œë„ ê°€ê²Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        logger.error(f"í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
    finally:
        crawler.close()

if __name__ == "__main__":
    test_crawling()