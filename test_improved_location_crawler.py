"""
ê°œì„ ëœ ìœ„ì¹˜ì •ë³´ ìˆ˜ì§‘ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸
JavaScript ë³€ìˆ˜ ì¶”ì¶œ ë°©ì‹ì„ ì ìš©í•œ í¬ë¡¤ëŸ¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.core.crawler import DiningCodeCrawler
from src.core.data_enhancement import DataEnhancer
import logging
import json
import pandas as pd
from datetime import datetime
import time

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('test_improved_location.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

def test_improved_location_crawler():
    """ê°œì„ ëœ ìœ„ì¹˜ì •ë³´ ìˆ˜ì§‘ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸"""
    
    logger.info("=" * 80)
    logger.info("ê°œì„ ëœ ìœ„ì¹˜ì •ë³´ ìˆ˜ì§‘ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    logger.info("=" * 80)
    
    start_time = time.time()
    crawler = DiningCodeCrawler()
    
    try:
        # 1ë‹¨ê³„: ê°€ê²Œ ëª©ë¡ ìˆ˜ì§‘
        logger.info("\n[1ë‹¨ê³„] ê°€ê²Œ ëª©ë¡ ìˆ˜ì§‘")
        keyword = "ê°•ë‚¨ ë¬´í•œë¦¬í•„"
        rect = "37.4979,127.027,37.5178,127.047"
        
        stores = crawler.get_store_list(keyword, rect)
        logger.info(f"ì´ {len(stores)}ê°œ ê°€ê²Œ ë°œê²¬")
        
        if not stores:
            logger.error("ê°€ê²Œ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # 2ë‹¨ê³„: ëª¨ë“  ê°€ê²Œì˜ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ (ìœ„ì¹˜ì •ë³´ í¬í•¨)
        logger.info(f"\n[2ë‹¨ê³„] ëª¨ë“  ê°€ê²Œì˜ ìƒì„¸ ì •ë³´ ë° ìœ„ì¹˜ì •ë³´ ìˆ˜ì§‘")
        detailed_stores = []
        success_count = 0
        location_success_count = 0
        
        for i, store in enumerate(stores[:15], 1):  # ì²˜ìŒ 15ê°œë§Œ í…ŒìŠ¤íŠ¸
            logger.info(f"\n--- {i}/{min(15, len(stores))} ê°€ê²Œ ì²˜ë¦¬ ì¤‘ ---")
            logger.info(f"ê°€ê²Œëª…: {store.get('name', 'Unknown')}")
            
            try:
                # ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
                detailed_store = crawler.get_store_detail(store)
                
                if detailed_store:
                    detailed_stores.append(detailed_store)
                    success_count += 1
                    
                    # ìœ„ì¹˜ì •ë³´ í™•ì¸
                    if detailed_store.get('position_lat') and detailed_store.get('position_lng'):
                        location_success_count += 1
                        logger.info(f"âœ… ìœ„ì¹˜ì •ë³´ ìˆ˜ì§‘ ì„±ê³µ: ({detailed_store['position_lat']}, {detailed_store['position_lng']})")
                    else:
                        logger.warning("âŒ ìœ„ì¹˜ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨")
                        
                    # ì§„í–‰ë¥  í‘œì‹œ
                    logger.info(f"ì§„í–‰ë¥ : {i}/{min(15, len(stores))} ({i/min(15, len(stores))*100:.1f}%)")
                    
                else:
                    logger.warning(f"ê°€ê²Œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {store.get('name')}")
                    
            except Exception as e:
                logger.error(f"ê°€ê²Œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
            
            # ê°„ê²© ì¡°ì •
            time.sleep(1)
        
        # 3ë‹¨ê³„: ê²°ê³¼ ë¶„ì„
        logger.info("\n[3ë‹¨ê³„] ê²°ê³¼ ë¶„ì„")
        total_processed = min(15, len(stores))
        
        logger.info(f"ì „ì²´ ì²˜ë¦¬ ê°€ê²Œ ìˆ˜: {total_processed}")
        logger.info(f"ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì„±ê³µ: {success_count}ê°œ ({success_count/total_processed*100:.1f}%)")
        logger.info(f"ìœ„ì¹˜ì •ë³´ ìˆ˜ì§‘ ì„±ê³µ: {location_success_count}ê°œ ({location_success_count/total_processed*100:.1f}%)")
        
        # 4ë‹¨ê³„: ë°ì´í„° í–¥ìƒ (ì§€ì˜¤ì½”ë”©)
        if detailed_stores:
            logger.info("\n[4ë‹¨ê³„] ë°ì´í„° í–¥ìƒ (ì§€ì˜¤ì½”ë”©)")
            enhancer = DataEnhancer()
            
            # ìœ„ì¹˜ì •ë³´ê°€ ì—†ëŠ” ê°€ê²Œë“¤ì— ëŒ€í•´ ì§€ì˜¤ì½”ë”© ì‹œë„
            no_location_stores = [store for store in detailed_stores 
                                if not store.get('position_lat') and store.get('address')]
            
            if no_location_stores:
                logger.info(f"{len(no_location_stores)}ê°œ ê°€ê²Œì— ëŒ€í•´ ì§€ì˜¤ì½”ë”© ì‹œë„")
                enhanced_stores = enhancer.enhance_store_data(detailed_stores)
                
                # ì§€ì˜¤ì½”ë”© ì„±ê³µë¥  ê³„ì‚°
                geocoding_success = sum(1 for store in enhanced_stores 
                                      if store.get('position_lat') and store.get('position_lng'))
                final_location_success = geocoding_success
                
                logger.info(f"ì§€ì˜¤ì½”ë”© í›„ ìµœì¢… ìœ„ì¹˜ì •ë³´ ë³´ìœ : {final_location_success}ê°œ ({final_location_success/total_processed*100:.1f}%)")
                detailed_stores = enhanced_stores
            else:
                logger.info("ì§€ì˜¤ì½”ë”©ì´ í•„ìš”í•œ ê°€ê²Œê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # 5ë‹¨ê³„: ê²°ê³¼ ì €ì¥
        logger.info("\n[5ë‹¨ê³„] ê²°ê³¼ ì €ì¥")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # CSV ì €ì¥
        if detailed_stores:
            df = pd.DataFrame(detailed_stores)
            csv_filename = f"improved_location_test_{timestamp}.csv"
            df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
            logger.info(f"CSV íŒŒì¼ ì €ì¥: {csv_filename}")
            
            # JSON ì €ì¥
            json_filename = f"improved_location_test_{timestamp}.json"
            with open(json_filename, 'w', encoding='utf-8') as f:
                json.dump(detailed_stores, f, ensure_ascii=False, indent=2)
            logger.info(f"JSON íŒŒì¼ ì €ì¥: {json_filename}")
        
        # 6ë‹¨ê³„: ìµœì¢… ê²°ê³¼ ìš”ì•½
        end_time = time.time()
        total_time = end_time - start_time
        
        logger.info("\n" + "=" * 80)
        logger.info("ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        logger.info("=" * 80)
        logger.info(f"ì´ ì†Œìš” ì‹œê°„: {total_time:.1f}ì´ˆ")
        logger.info(f"ì²˜ë¦¬ëœ ê°€ê²Œ ìˆ˜: {len(detailed_stores)}ê°œ")
        logger.info(f"ìœ„ì¹˜ì •ë³´ ë³´ìœ  ê°€ê²Œ: {sum(1 for s in detailed_stores if s.get('position_lat'))}ê°œ")
        logger.info(f"ìœ„ì¹˜ì •ë³´ ì„±ê³µë¥ : {sum(1 for s in detailed_stores if s.get('position_lat'))/len(detailed_stores)*100:.1f}%" if detailed_stores else "0%")
        
        # ìœ„ì¹˜ì •ë³´ê°€ ìˆëŠ” ê°€ê²Œë“¤ì˜ ì¢Œí‘œ ì¶œë ¥
        location_stores = [s for s in detailed_stores if s.get('position_lat')]
        if location_stores:
            logger.info("\nğŸ“ ìœ„ì¹˜ì •ë³´ê°€ ìˆ˜ì§‘ëœ ê°€ê²Œë“¤:")
            for store in location_stores:
                logger.info(f"- {store.get('name', 'Unknown')}: ({store['position_lat']}, {store['position_lng']})")
        
        return len(detailed_stores) > 0 and sum(1 for s in detailed_stores if s.get('position_lat')) > 0
        
    except Exception as e:
        logger.error(f"í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False
    finally:
        crawler.close()

if __name__ == "__main__":
    success = test_improved_location_crawler()
    if success:
        print("\nâœ… ê°œì„ ëœ ìœ„ì¹˜ì •ë³´ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
    else:
        print("\nâŒ ê°œì„ ëœ ìœ„ì¹˜ì •ë³´ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨!") 